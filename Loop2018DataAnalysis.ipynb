{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Map lives at: https://docs.google.com/spreadsheets/d/1CgqTjdKizat-g7K7-AAuVIazQFKJ3WAAPHR-Qpa49lU/edit#gid=761153638"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ust coming to this quickly:\n",
    "\n",
    " \n",
    "\n",
    "We’ll also need columns for:\n",
    "\n",
    "    Final result\n",
    "        This will need to be “calculated”, probably just by giving reagent results priority over FTIR. However we had some cases this year where volunteers overstated the identification abilities of reagents, so we will need to manually review these before a final version.\n",
    "    Final result category\n",
    "        Every possible input for this (should be pretty limited, really) needs to be put into a lookup table to assign it a category\n",
    "    Soldas category\n",
    "        This will need a separate but very similar lookup table to the final result category\n",
    "    Unique ID\n",
    "        This should ideally use a similar format to last year. (SGP2017-0005)\n",
    "    SPSS Date           \n",
    "        As I recall, SPSS cannot handle standard date formats and needs to be given the US mm/dd/yy format.\n",
    "    Service user UID\n",
    "        This needs to be determined from whether a given intervention is attached to a “what was your first sample number at this event” value. These will need some level of manual review. I improved the data structure before Boomtown but others will probably need some context to sort out unfortunately.\n",
    "\n",
    " \n",
    "\n",
    "Really we are not working with a simple database but actually a 2d database where we have service users which could be linked to one or more sample numbers, and so if we are analysing social stats then we should be cautious about assuming every person only has one sample and vice versa when analysing lab data we should be cautious not to assume that every sample is a different person if it affects the outcome of a conclusion. I did a bit of reading about databases and how to set this up but given that I’m coming from using spreadsheets and the actual software side of a database vs a spreadsheet is new to me even if the concept isn’t, I haven’t gotten to a place where I could move to using a database program that’s designed to handle this sort of situation yet.\n",
    "\n",
    " \n",
    "\n",
    "Regarding orphans, I’ve just been through the Parklife data and found 17 FTIR results which had not been entered (so they were catalogued but appeared to have no FTIR). If this isn’t done at the end of the day then it needs to be done as part of the processing unfortunately, it would be a real shame to just lose these. We did it at the end of the day at boomtown and SW4.\n",
    "\n",
    " \n",
    "\n",
    "I think that’s all the processing considerations that come to mind for right now.\n",
    "\n",
    "Guy\n",
    "\n",
    "I have been through each of the events to add a “smart”\n",
    "\n",
    "    final outcome\n",
    "    second component detected\n",
    "\n",
    " \n",
    "\n",
    "column to the FTIR sheet in stats documents as this is how the other members of TEDI report (it would be a bit odd if we were never reporting a second component with our results, but sometimes the subtraction result that is given is garbage for pills (which later get reagent tested and turn out to most likely be just MDMA).\n",
    "\n",
    " \n",
    "\n",
    "I’ve also added these columns to the “catalog” worksheet tab in the statistics file of each event as I’ve been using that whenever I want to look at any lab data. I guess it won’t affect you.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Thanks Jens.\n",
    "\n",
    "Actually I’d even go as far as deleting that column (Substance(s) detected) altogether. It’s redundant if we have the other FTIR columns.\n",
    "\n",
    " \n",
    "\n",
    "These two entries are coming through because there are two possible fields to record the substance name (a limitation of google forms when using a dropdown list)\n",
    "\n",
    "other drug\n",
    "\n",
    "other drug or reasonable suggestion\n",
    "\n",
    "Any cells with this in them should be overwritten with data from the appropriate column for free text entry.\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "Anyway, I’ve been through the list and done all the important ones and most of the user entered stuff. I’ve ignored anything with a comma in it.\n",
    "\n",
    " \n",
    "\n",
    "Guy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module imports\n",
    "import copy\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def fix_sample_number(x):\n",
    "    \"\"\"Make sure all samples numbers are of form: AXXX (where A is one of A, F, W and X is a digit)\"\"\"\n",
    "    if isinstance(x, float) and np.isnan(x):\n",
    "        return x # leave NaN's alone\n",
    "    if (isinstance(x, str) or isinstance(x, unicode)) and len(x) == 0:\n",
    "        return np.nan\n",
    "    try:\n",
    "        sn = 'F{:04d}'.format(int(x))\n",
    "    except ValueError:\n",
    "        # Assume string so make sure it's of the right format\n",
    "        sn = str(x).strip().upper()\n",
    "    len_sn = len(sn)\n",
    "    if not ((len_sn == 5 and sn[0] in ['A', 'F', 'W', 'B']) or (len_sn == 6 and sn[0] == 'D')):\n",
    "        print(\"!!! Bad ID \\'%s\\'\" % sn)\n",
    "    return sn\n",
    "\n",
    "def now():\n",
    "    return datetime.datetime.now().strftime(\"%d/%m/%y %H:%M:%S\")\n",
    "\n",
    "def enumerate_duplicates(row):\n",
    "    \"\"\"Append a counter to duplicate labels\"\"\"\n",
    "    SEPARATOR = '.'\n",
    "    duplicates = {}\n",
    "    updated_row = []\n",
    "    for r in row:\n",
    "        count = duplicates.get(r, 0)\n",
    "        if count > 0:\n",
    "            label = \"{}{}{}\".format(r, SEPARATOR, count)\n",
    "        else:\n",
    "            label = r\n",
    "        updated_row.append(label)\n",
    "        duplicates[r] = count + 1\n",
    "    return updated_row\n",
    "\n",
    "# Need to define in main or we can't pickle the data objects\n",
    "class DataFrames(object):\n",
    "    def __init__(self):\n",
    "        self.catalog = None\n",
    "        self.ftir = None\n",
    "        self.reagent = None\n",
    "        self.mla = None\n",
    "        self.hr = None\n",
    "        self.combined = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = 'raise'\n",
    "\n",
    "def gsheets_service():\n",
    "    from googleapiclient.discovery import build\n",
    "    from httplib2 import Http\n",
    "    from oauth2client import file, client, tools\n",
    "    # If modifying these scopes, delete the file token.json.\n",
    "    #Ensure that the creds file is always taken from the current working folder\n",
    "        #This allows two people on different PCs to merge changes more easily.\n",
    "    CREDS_FILE = os.path.join(os.path.realpath('./'),'JensDataExportJupyter_client_secret.json')\n",
    "    SCOPES = 'https://www.googleapis.com/auth/spreadsheets.readonly'\n",
    "    store = file.Storage('token.json')\n",
    "    creds = store.get()\n",
    "    if not creds or creds.invalid:\n",
    "        import argparse\n",
    "        flags = argparse.ArgumentParser(parents=[tools.argparser]).parse_args([])\n",
    "        flow = client.flow_from_clientsecrets(CREDS_FILE, SCOPES)\n",
    "        creds = tools.run_flow(flow, store, flags)\n",
    "    service = build('sheets', 'v4', http=creds.authorize(Http()))\n",
    "    return service\n",
    "\n",
    "def get_df(spreadsheet_id, ss_range, mla=False):\n",
    "    # Call the Sheets API\n",
    "    result = GSHEETS_SERVICE.spreadsheets().values().get(spreadsheetId=spreadsheet_id,\n",
    "                                                         range=ss_range).execute()\n",
    "    values = result.get('values', [])\n",
    "    if not values:\n",
    "        print('*** No data found ***')\n",
    "        return None\n",
    "\n",
    "    # mla has irrelevant stuff in columns 1 and 3 and sample numbers in first column\n",
    "    if mla:\n",
    "        values.pop(0)\n",
    "        values.pop(1)\n",
    "        def not_blank(row):\n",
    "            return len(row[0]) > 0       \n",
    "    else:\n",
    "        def not_blank(row):\n",
    "            return sum(map(len, row[:6])) > 0\n",
    "\n",
    "    rows = list(filter(not_blank, values))\n",
    "    if not rows:\n",
    "        print('*** No data found after pruning rows! ***')\n",
    "        return None\n",
    "    \n",
    "    columns = enumerate_duplicates(rows[0])\n",
    "    ncols = len(rows[0])\n",
    "    row_max = max(map(len, rows[1:]))\n",
    "    width = min(ncols, row_max)\n",
    "    return pd.DataFrame(rows[1:], columns=columns[:width])\n",
    "\n",
    "def canonicalise_df(df, source=None):\n",
    "    \"\"\"Initial cleaning of all dataframes\"\"\"\n",
    "    #from pandas._libs.tslib import OutOfBoundsDatetime\n",
    "    if source:\n",
    "        print(\"Canonicalising %s\" % source)\n",
    "    # Standardise names\n",
    "    df.rename(columns=RENAME_COLUMN_MAP, inplace=True)\n",
    "    \n",
    "    def fix_timestamp(x):\n",
    "        return pd.to_datetime(str(x), format='%d/%m/%Y %H:%M:%S')\n",
    "    if 'Timestamp' in df.columns:\n",
    "        df.loc[:, 'Timestamp'] = df['Timestamp'].map(fix_timestamp)\n",
    "    df.loc[:, 'SampleNumber'] = df['SampleNumber'].apply(fix_sample_number)\n",
    "    df.dropna(subset=['SampleNumber'], inplace=True)\n",
    "    #df.sort_values(['Sample Number'], ascending=True, inplace=True)\n",
    "    # Make sure we don't have any blank columns\n",
    "    if set(df.columns.values).intersection(set([np.nan, ''])):\n",
    "        raise RuntimeError(\"Blank column names in Dataframe\")\n",
    "    return df\n",
    "\n",
    "def get_data(spreadsheet_id):\n",
    "\n",
    "    catalog_range = 'Catalog!A:R'\n",
    "    ftir_range = 'FTIR!A:X'\n",
    "    reagent_range = 'Reagent!A:W'\n",
    "    mla_range = 'MLA!A:R'\n",
    "    hr_range = 'Interventions!A:BJ'\n",
    "    \n",
    "    df_catalog = get_df(spreadsheet_id, catalog_range)\n",
    "    df_catalog = canonicalise_df(df_catalog, source='catalog')\n",
    "    df_ftir = get_df(spreadsheet_id, ftir_range)\n",
    "    df_ftir = canonicalise_df(df_ftir, source='ftir')\n",
    "    df_reagent = get_df(spreadsheet_id, reagent_range)\n",
    "    df_reagent = canonicalise_df(df_reagent, source='reagent')\n",
    "    df_mla = get_df(spreadsheet_id, mla_range, mla=True)\n",
    "    df_mla = canonicalise_df(df_mla, source='mla')\n",
    "    try:\n",
    "        df_hr = get_df(spreadsheet_id, hr_range)\n",
    "    except ValueError:\n",
    "        df_hr = None\n",
    "    if df_hr is not None:\n",
    "        pass\n",
    "        df_hr = canonicalise_df(df_hr, source='hr')\n",
    "\n",
    "    df = DataFrames()\n",
    "    df.catalog = df_catalog\n",
    "    df.ftir = df_ftir\n",
    "    df.reagent = df_reagent\n",
    "    df.mla = df_mla\n",
    "    df.hr = df_hr\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_rename_columns_map():\n",
    "    sheet_id = '1CgqTjdKizat-g7K7-AAuVIazQFKJ3WAAPHR-Qpa49lU'\n",
    "    ss_range = 'ColumnMap!A:B'\n",
    "    result = GSHEETS_SERVICE.spreadsheets().values().get(spreadsheetId=sheet_id,\n",
    "                                                         range=ss_range).execute()\n",
    "    values = result.get('values', [])\n",
    "    assert values[0] == ['OriginalColumn', 'CanonicalColumn'], values[0]\n",
    "    return { cm[0] : cm[1] for cm in values[1:] if len(cm) >= 2 }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script running from: /opt/random\n",
      "PROCESSING BOOMTOWN\n",
      "Canonicalising catalog\n",
      "!!! Bad ID 'TF0579'\n",
      "!!! Bad ID 'TF1665'\n",
      "!!! Bad ID 'TF1660'\n",
      "Canonicalising ftir\n",
      "!!! Bad ID 'TF1665'\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "Canonicalising hr\n",
      "!!! Bad ID 'FXXX'\n",
      "!!! Bad ID 'TF0653'\n",
      "!!! Bad ID 'TF1172'\n",
      "!!! Bad ID 'TF1762'\n",
      "PROCESSING BOARDMASTERS\n",
      "Canonicalising catalog\n",
      "Canonicalising ftir\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "Canonicalising hr\n",
      "PROCESSING MADE\n",
      "Canonicalising catalog\n",
      "Canonicalising ftir\n",
      "!!! Bad ID 'XF0005'\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "Canonicalising hr\n",
      "PROCESSING SW4\n",
      "Canonicalising catalog\n",
      "Canonicalising ftir\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "PROCESSING LOST VILLAGE\n",
      "Canonicalising catalog\n",
      "Canonicalising ftir\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "PROCESSING BESTIVAL\n",
      "Canonicalising catalog\n",
      "Canonicalising ftir\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "Canonicalising hr\n",
      "!!! Bad ID 'P1000'\n",
      "!!! Bad ID 'F20005'\n",
      "!!! Bad ID 'G9998'\n",
      "PROCESSING YNOT\n",
      "Canonicalising catalog\n",
      "Canonicalising ftir\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "Canonicalising hr\n",
      "PROCESSING TRUCKFEST\n",
      "Canonicalising catalog\n",
      "Canonicalising ftir\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "PROCESSING LSTD\n",
      "Canonicalising catalog\n",
      "!!! Bad ID 'S0050'\n",
      "!!! Bad ID 'M0120'\n",
      "!!! Bad ID 'M0141'\n",
      "!!! Bad ID 'M0204'\n",
      "!!! Bad ID 'S0186'\n",
      "!!! Bad ID 'S0202'\n",
      "Canonicalising ftir\n",
      "!!! Bad ID 'S0050'\n",
      "!!! Bad ID 'M0141'\n",
      "!!! Bad ID 'M0120'\n",
      "!!! Bad ID 'S0186'\n",
      "!!! Bad ID 'M0204'\n",
      "!!! Bad ID 'S0202'\n",
      "Canonicalising reagent\n",
      "!!! Bad ID 'S0050'\n",
      "Canonicalising mla\n",
      "!!! Bad ID 'M0120'\n",
      "Canonicalising hr\n",
      "PROCESSING KENDAL CALLING\n",
      "Canonicalising catalog\n",
      "!!! Bad ID 'M0011'\n",
      "Canonicalising ftir\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "Canonicalising hr\n",
      "PROCESSING PARKLIFE\n",
      "Canonicalising catalog\n",
      "!!! Bad ID 'M2248'\n",
      "Canonicalising ftir\n",
      "!!! Bad ID 'M2248'\n",
      "Canonicalising reagent\n",
      "!!! Bad ID 'NOT A1451'\n",
      "Canonicalising mla\n"
     ]
    }
   ],
   "source": [
    "#S how the folder where the code file is being run from        \n",
    "print(\"Script running from: %s\" % os.path.realpath(os.getcwd()))\n",
    "\n",
    "# The ID and range of a sample spreadsheet.\n",
    "BOOMTOWN2018_SPREADSHEET_ID = '1RiA-FwG_954Ger2VPsOSA3JLh-7sEoTYr40eVS0mp24'\n",
    "MADE2018_SPREADSHEET_ID = '1daXdyL6uL8qnMsEsP0RLZE9nDzt6J7Zr1ygQdguvi-E'\n",
    "BOARDMASTERS2018_SPREADSHEET_ID = '1U1lhUWLazDBN-wb2eZM8YV674f46npVfQK3XUVZjPow'\n",
    "SW42018_SPREADSHEET_ID = '1agpMmJ9XukeWXS5_mwrDSKeshUaFtYwOzsPiR1DKsPU'\n",
    "LOSTVILLAGE2018_SPREADSHEET_ID = '1OL0gyXrpZnJ8e7yR7eF6S2OaBYBiPDoVp5xGpdK4wlA'\n",
    "BESTIVAL2018_SPREADSHEET_ID = '184qudGcw4PB0SMtOo0ZBDtckeGaH0RCLUXbA-u3BiHE'\n",
    "YNOT2018_SPREADSHEET_ID = '1D01cj-Mra06TuoG_MsKuLq9OdtvKzrvRdiE255po_ag'\n",
    "TRUCKFEST2018_SPREADSHEET_ID = '1sGG9WJxKyD2CGUjzJAXul3g9hVnRz6HbTiqKV5cUAyA'\n",
    "LSTD2018_SPREADSHEET_ID = '1R8YqDnrhvuVMwPFShwaaAUIyCXQMeozA230OXsFsDQM'\n",
    "KENDALCALLING2018_SPREADSHEET_ID = '16-PfwBOaUxwod3X75LGk1VAjBblkNsTJpCsX825aghI'\n",
    "PARKLIFE2018_SPREADSHEET_ID = '1oO5sHcUhUn_7M1Hap73sOZHNEfWFMcDkQuWDRFf4d-w'\n",
    "\n",
    "\n",
    "data = {}\n",
    "GSHEETS_SERVICE = gsheets_service()\n",
    "RENAME_COLUMN_MAP = get_rename_columns_map()\n",
    "print(\"PROCESSING BOOMTOWN\")\n",
    "data['boomtown'] = get_data(BOOMTOWN2018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING BOARDMASTERS\")\n",
    "data['boardmasters'] = get_data(BOARDMASTERS2018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING MADE\")\n",
    "data['made'] = get_data(MADE2018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING SW4\")\n",
    "data['sw4'] = get_data(SW42018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING LOST VILLAGE\")\n",
    "data['lostvillage'] = get_data(LOSTVILLAGE2018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING BESTIVAL\")\n",
    "data['bestival'] = get_data(BESTIVAL2018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING YNOT\")\n",
    "data['ynot'] = get_data(YNOT2018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING TRUCKFEST\")\n",
    "data['truckfest'] = get_data(TRUCKFEST2018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING LSTD\")\n",
    "data['lstd'] = get_data(LSTD2018_SPREADSHEET_ID)\n",
    "print( \"PROCESSING KENDAL CALLING\")\n",
    "data['kc'] = get_data(KENDALCALLING2018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING PARKLIFE\")\n",
    "data['parklife'] = get_data(PARKLIFE2018_SPREADSHEET_ID)\n",
    "\n",
    "with open('foo_multi.pkl','wb') as w:\n",
    "    pickle.dump(data, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('foo_multi.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Need to define in main or we can't pickle the data objects\n",
    "class Duplicates(object):\n",
    "    def __init__(self, dfs):\n",
    "        self.dfs = dfs\n",
    "        dtypes = ['catalog', 'ftir', 'reagent', 'mla', 'hr']\n",
    "        for t in dtypes:\n",
    "            setattr(self, t, None)\n",
    "        for t in dtypes:\n",
    "            self.find_duplicates(t)\n",
    "        \n",
    "    def find_duplicates(self, dtype):\n",
    "        dataframe = getattr(self.dfs, dtype)\n",
    "        if dataframe is None:\n",
    "            return\n",
    "        duplicates = dataframe['SampleNumber'].duplicated()\n",
    "        if duplicates.any():\n",
    "            duplicates = list(dataframe.loc[duplicates, 'SampleNumber'].values)\n",
    "            print(\"### %d duplicated %s SampleNumbers %s ###\" % (len(duplicates), dtype, duplicates))\n",
    "#             dataframe[datafra,e['SampleNumber'].duplicated(keep=False)].to_csv('{}_duplicates.csv'.format(dtype))\n",
    "        else:\n",
    "            duplicates = None\n",
    "        setattr(self, dtype, duplicates)\n",
    "        \n",
    "    def has_hr_duplicates(self):\n",
    "        if self.hr:\n",
    "            outs = 'Please fix HR duplicates'\n",
    "            print(outs)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class Orphans(object):\n",
    "    def __init__(self, dataframes):\n",
    "        self.catalog = None\n",
    "        self.ftir = None\n",
    "        self.reagent = None\n",
    "        self.mla = None\n",
    "        self.hr = None\n",
    "        self.ftir_missing = None\n",
    "        \n",
    "        self.find_unique(dataframes)\n",
    "        \n",
    "    def find_unique(self, dataframes):\n",
    "        # Check there are no SampleNumbers in any of the other spreadsheets that aren't in the cataolog sheet\n",
    "        catalog_unique = set(dataframes.catalog['SampleNumber'].unique())\n",
    "        \n",
    "        ftir_unique = set(dataframes.ftir['SampleNumber'].unique())\n",
    "        self.ftir = ftir_unique.difference(catalog_unique)\n",
    "\n",
    "        reagent_unique = set(dataframes.reagent['SampleNumber'].unique())\n",
    "        self.reagent = reagent_unique.difference(catalog_unique)\n",
    "\n",
    "        self.hr = None\n",
    "        hr_unique = None\n",
    "        if dataframes.hr is not None:\n",
    "            hr_unique = set(dataframes.hr['SampleNumber'].unique())\n",
    "            # HR need to be both in catalog and ftir\n",
    "            self.hr = hr_unique.difference(ftir_unique.union(catalog_unique))\n",
    "\n",
    "        mla_unique = set(dataframes.mla['SampleNumber'].unique()).difference(catalog_unique)\n",
    "        self.mla = mla_unique.difference(catalog_unique)\n",
    "\n",
    "        # Check for any that are only in the catalog\n",
    "        unique = [u for u in [ftir_unique, reagent_unique, hr_unique, mla_unique] if u is not None]\n",
    "        outside_catalog = set.union(*unique)\n",
    "        self.catalog = catalog_unique.difference(outside_catalog)\n",
    "\n",
    "        # Check for any that aren't in FTIR and don't have anything in reagent test\n",
    "        self.ftir_missing = catalog_unique.difference(ftir_unique).difference(reagent_unique).difference(self.catalog)\n",
    "\n",
    "            \n",
    "    def print_orphans(self):\n",
    "        if self.catalog:\n",
    "            print(\"Orphaned Catalog SampleNumbers: %s\" % sorted(self.catalog))\n",
    "        if self.ftir:\n",
    "            print(\"Orphaned FTIR SampleNumbers: %s\" % sorted(self.ftir))\n",
    "        if self.mla:\n",
    "            print(\"Orphaned MLA SampleNumbers: %s\" % sorted(self.mla))\n",
    "        if self.hr:\n",
    "            print(\"Orphaned HR SampleNumbers: %s\" % sorted(self.hr))\n",
    "        if self.ftir_missing:\n",
    "            print(\"Samples not in FTIR or Reagent: %s\" % sorted(self.ftir_missing))          \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Summary Cell\n",
    "catalog_orphan = 0\n",
    "ftir_orphan = 0\n",
    "hr_orphan = 0\n",
    "catalog_duplicates = 0\n",
    "ftir_duplicates = 0\n",
    "hr_duplicates = 0\n",
    "hr = 0\n",
    "for festival, fdfs in data.items():\n",
    "    print(\"\\n{} Festival: {}{} \".format(\"=\"* 15, festival, \"=\"*15))\n",
    "    orphans = Orphans(fdfs)\n",
    "    orphans.print_orphans()\n",
    "    catalog_orphan += len(orphans.catalog)\n",
    "    ftir_orphan += len(orphans.ftir)\n",
    "    if orphans.hr is not None:\n",
    "        hr_orphan += len(orphans.hr)\n",
    "    catalog_duplicates += sum(list(map(lambda x: x.startswith('D'), fdfs.catalog['SampleNumber'].values)))\n",
    "    ftir_duplicates += sum(list(map(lambda x: x.startswith('D'), fdfs.ftir['SampleNumber'].values)))\n",
    "    if fdfs.hr is not None:\n",
    "        hr_duplicates += sum(list(map(lambda x: x.startswith('D'), fdfs.hr['SampleNumber'].values)))\n",
    "    if fdfs.hr is not None:\n",
    "        hr += len(fdfs.hr)\n",
    "\n",
    "print(\"=\" * 30)\n",
    "print(\"SUMMARY\")\n",
    "print(\"Catalog duplicates \", catalog_duplicates)\n",
    "print(\"FTIR duplicates \", ftir_duplicates)\n",
    "print(\"HR duplicates \", hr_duplicates)\n",
    "print(\"Catalog orphan \", catalog_orphan)\n",
    "print(\"FTIR orphan \", ftir_orphan)\n",
    "print(\"HR orphan \", hr_orphan)\n",
    "\n",
    "print(\"Total number HR interventions \", hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_ftir_drug_columns(df):\n",
    "    # Make sure all columns are the same\n",
    "    # cols = np.array(['Substance detected', 'Hit Confidence', 'Compound detected', 'Hit Confidence.1', \n",
    "    #  'Is anything detected after subtraction analysis?', 'Compound detected (Subtraction)', 'Hit Confidence.2',\n",
    "    #  'Substance detected.1', 'Hit Confidence.3'])\n",
    "\n",
    "    # Copy over 'Other' substances into the main column\n",
    "    target_label = 'Substance detected'\n",
    "    source_label = 'Compound detected'\n",
    "    to_drop = [source_label, 'Hit Confidence.1']\n",
    "    other_mask = ~df[target_label].str.startswith('Other').fillna(value=False)\n",
    "    df[target_label].where(other_mask, df[source_label], inplace=True) # Copy values from source_label column over\n",
    "    df.drop(to_drop, axis=1, inplace=True) # Remove now redundant columns\n",
    "    df.rename(columns={target_label : 'Primary_hit', 'Hit Confidence' : 'Primary_confidence'}, inplace=True) # Rename Columns\n",
    "\n",
    "\n",
    "    # Column names appear to be reversed - compound now is substance!!\n",
    "    target_label = 'Compound detected (Subtraction)'\n",
    "    source_label = 'Substance detected.1'\n",
    "    to_drop = [source_label, 'Hit Confidence.3', 'Is anything detected after subtraction analysis?']\n",
    "    other_mask = ~df[target_label].str.startswith('Other').fillna(value=False)\n",
    "    df[target_label].where(other_mask, df[source_label], inplace=True) # Copy values from source_label column over\n",
    "    df.drop(to_drop, axis=1, inplace=True) # Remove now redundant columns\n",
    "    df.rename(columns={target_label : 'Secondary_hit', 'Hit Confidence.2' : 'Secondary_confidence'}, inplace=True) # Rename Columns\n",
    "\n",
    "    \n",
    "# This cell cleans the \"sample form\" field \n",
    "def clean_sample_form(df):\n",
    "    sample_form_d = { 'pill' : ['Ecstasy Tablet',\n",
    "                                'ecstasy pill',\n",
    "                                'ecstacy pill',\n",
    "                                'Non-pharmaceutical tablet (ecstasy etc)',\n",
    "                                'other recreational pill',\n",
    "                                 'Whole pill',\n",
    "                                'Other pill',\n",
    "                                'Pharmaceutical'],\n",
    "                      'partial pill' : ['Partial ecstasy pill',\n",
    "                                        'Partial 2C-B pill',\n",
    "                                        'Crushed tablet'],\n",
    "                      'powder' : ['powder/capsule/bomb',\n",
    "                                  'Powder/capsule/bomb/crystal',\n",
    "                                  'Powder or crushed pill',\n",
    "                                  'Crystal, Capsule or Powder'],\n",
    "                      'liquid' : ['*Cannabinoid liquid',\n",
    "                                   '*Viscous liquid',\n",
    "                                  'Dissolved in Propylene Glycol',\n",
    "                                  'Oil'],\n",
    "                       'tab' : ['blotter', 'LSD Tab']\n",
    "                      }\n",
    "\n",
    "\n",
    "    # Firstly convert all columns to lower case and remove any spaces\n",
    "    def lower(value):\n",
    "        if type(value) is str:\n",
    "            value = value.strip().lower()\n",
    "        return value\n",
    "\n",
    "    column = 'SampleForm'\n",
    "    df[column] = df[column].map(lower, na_action='ignore')\n",
    "    \n",
    "    replace_d = {}\n",
    "    replace_d[column] = {}\n",
    "    for drug, names in sample_form_d.items():\n",
    "        for name in names:\n",
    "            replace_d[column][name.lower()] = drug\n",
    "    \n",
    "    # Replace values\n",
    "    df.replace(replace_d, inplace=True)\n",
    "    return df    \n",
    "\n",
    "\n",
    "# This cell cleans the \"sample form\" field \n",
    "def rename_values(df, columns, replace_map):\n",
    "    # Firstly convert all columns to lower case and remove any spaces\n",
    "    def clean(value):\n",
    "        if type(value) is str:\n",
    "            value = value.strip().lower()\n",
    "        return value\n",
    "\n",
    "    for column in columns:\n",
    "        if column in df.columns:\n",
    "            df[column] = df[column].map(clean, na_action='ignore')\n",
    "\n",
    "    replace_d = {column : replace_map for column in columns}\n",
    "    df.replace(replace_d, inplace=True)\n",
    "    return df\n",
    "\n",
    "def get_drugs_maps():\n",
    "    # Get the drugs map\n",
    "    if 'GSHEETS_SERVICE' not in locals():\n",
    "        GSHEETS_SERVICE = gsheets_service()\n",
    "    sheet_id = '1CgqTjdKizat-g7K7-AAuVIazQFKJ3WAAPHR-Qpa49lU'\n",
    "    ss_range = 'UserDrugsMap!A:B'\n",
    "    result = GSHEETS_SERVICE.spreadsheets().values().get(spreadsheetId=sheet_id,\n",
    "                                                         range=ss_range).execute()\n",
    "    values = result.get('values', [])\n",
    "    assert values[0] == ['Drug name', 'Translation']\n",
    "    user_drugs_map = { dt[0].lower() : dt[1].lower() for dt in values[1:] if len(dt) == 2 }\n",
    "    user_drugs_map = { k : v for k, v in user_drugs_map.items() if k != v } # remove duplicates\n",
    "\n",
    "    ss_range = 'TesterDrugsMap!A:B'\n",
    "    result = GSHEETS_SERVICE.spreadsheets().values().get(spreadsheetId=sheet_id,\n",
    "                                                         range=ss_range).execute()\n",
    "    values = result.get('values', [])\n",
    "    tester_drugs_map = { dt[0].lower() : dt[1].lower() for dt in values[1:] if len(dt) == 2 }\n",
    "    tester_drugs_map = { k : v for k, v in tester_drugs_map.items() if k != v } # remove duplicates\n",
    "    return user_drugs_map, tester_drugs_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FESTIVAL  boomtown\n",
      "FESTIVAL  boardmasters\n",
      "FESTIVAL  made\n",
      "FESTIVAL  sw4\n",
      "FESTIVAL  lostvillage\n",
      "FESTIVAL  bestival\n",
      "FESTIVAL  ynot\n",
      "FESTIVAL  truckfest\n",
      "FESTIVAL  lstd\n",
      "FESTIVAL  kc\n",
      "FESTIVAL  parklife\n",
      "Finished cleaning dataframes at 21/11/18 22:17:30\n"
     ]
    }
   ],
   "source": [
    "# Clean the cells\n",
    "import pickle\n",
    "with open('foo_multi.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "user_drugs_map, tester_drugs_map = get_drugs_maps()\n",
    "user_drug_columns = [\"Had_other_drugs\", 'SoldAs', 'UserSuspicion']\n",
    "tester_drug_columns = ['FTIR_detected', 'FTIR_detected_subtraction', 'FTIR_final_result']\n",
    "\n",
    "for festival, dfs in data.items():\n",
    "    print(\"FESTIVAL \",festival)\n",
    "    \n",
    "    # Clean up ftir sheet\n",
    "    merge_ftir_drug_columns(dfs.ftir)\n",
    "    \n",
    "    dfs.catalog = clean_sample_form(dfs.catalog)\n",
    "    dfs.ftir = clean_sample_form(dfs.ftir)\n",
    "    dfs.mla = clean_sample_form(dfs.mla)\n",
    "    \n",
    "    dfs.catalog = rename_values(dfs.catalog, user_drug_columns, user_drugs_map)\n",
    "    dfs.catalog = rename_values(dfs.catalog, tester_drug_columns, tester_drugs_map)\n",
    "    dfs.ftir = rename_values(dfs.ftir, user_drug_columns, user_drugs_map)\n",
    "    dfs.ftir = rename_values(dfs.ftir, tester_drug_columns, tester_drugs_map)    \n",
    "    dfs.mla = rename_values(dfs.mla, user_drug_columns, user_drugs_map)\n",
    "    dfs.mla = rename_values(dfs.mla, tester_drug_columns, tester_drugs_map)\n",
    "    if dfs.hr is not None:\n",
    "        dfs.hr = rename_values(dfs.hr, user_drug_columns, user_drugs_map)\n",
    "        dfs.hr = rename_values(dfs.hr, tester_drug_columns, tester_drugs_map)\n",
    "    \n",
    "print(\"Finished cleaning dataframes at %s\" % now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FESTIVAL  boomtown\n",
      "FESTIVAL  boardmasters\n",
      "FESTIVAL  made\n",
      "FESTIVAL  sw4\n",
      "FESTIVAL  lostvillage\n",
      "FESTIVAL  bestival\n",
      "FESTIVAL  ynot\n",
      "FESTIVAL  truckfest\n",
      "FESTIVAL  lstd\n",
      "FESTIVAL  kc\n",
      "FESTIVAL  parklife\n",
      "Finished merging first round at 21/11/18 22:17:42\n"
     ]
    }
   ],
   "source": [
    "# Merge of all data\n",
    "# import pickle\n",
    "# with open('foo_multi.pkl', 'rb') as f:\n",
    "#     data = pickle.load(f)\n",
    "    \n",
    "# def has_multiple(df):\n",
    "#     return sum([1 if c[-2:] in ['.1', '.2', '.3'] else 0 for c in df.columns.values ]) > 0\n",
    "\n",
    "for festival, dfs in data.items():\n",
    "    print(\"FESTIVAL \",festival)\n",
    "\n",
    "    # Rename columns to identify source dataframe\n",
    "    dfs.catalog.columns = ['catalog_'+ name if name != 'SampleNumber' else name for name in dfs.catalog.columns]\n",
    "    dfs.ftir.columns = ['ftir_'+ name if name != 'SampleNumber' else name for name in dfs.ftir.columns]\n",
    "    dfs.mla.columns = ['mla_'+ name if name != 'SampleNumber' else name for name in dfs.mla.columns]\n",
    "    if dfs.hr is not None:\n",
    "        dfs.hr.columns = ['hr_'+ name if name != 'SampleNumber' else name for name in dfs.hr.columns]\n",
    "        \n",
    "\n",
    "    # Remove all but the last of any duplicate SampleNumber\n",
    "    # want a list of all but the last duplicates\n",
    "    mask = ~dfs.catalog['SampleNumber'].duplicated(keep=False) | ~dfs.catalog['SampleNumber'].duplicated(keep='last')\n",
    "    dfs.catalog = dfs.catalog[mask]\n",
    "    mask = ~dfs.ftir['SampleNumber'].duplicated(keep=False) | ~dfs.ftir['SampleNumber'].duplicated(keep='last')\n",
    "    dfs.ftir = dfs.ftir[mask]\n",
    "    mask = ~dfs.mla['SampleNumber'].duplicated(keep=False) | ~dfs.mla['SampleNumber'].duplicated(keep='last')\n",
    "    dfs.mla = dfs.mla[mask]\n",
    "    if dfs.hr is not None:\n",
    "        mask = ~dfs.hr['SampleNumber'].duplicated(keep=False) | ~dfs.hr['SampleNumber'].duplicated(keep='last')\n",
    "        dfs.hr = dfs.hr[mask]\n",
    "\n",
    "    # First outer join on catalog/ftir to make sure we collect all possible information - this will result in\n",
    "    # some rows where there was no catalog data, only ftir data, but this is ok as when we merge with hr we will\n",
    "    # throw away any row that doesn't have a corresponding sample number in HR. This was even if catalog data is\n",
    "    # missing, we still get the FTIR data, which may be enough for our purposes\n",
    "    df_all = pd.merge(dfs.catalog, dfs.ftir, how='outer', on=['SampleNumber'])\n",
    "    # Add in mla data - only for when there are existing sample numbers\n",
    "    df_all = pd.merge(df_all, dfs.mla, how='left', on=['SampleNumber'])\n",
    "    if dfs.hr is not None:\n",
    "        # inner join -> merge only where there are matching sample numbers\n",
    "        df_all = pd.merge(df_all, dfs.hr, how='inner', on=['SampleNumber'])\n",
    "    dfs.combined = df_all\n",
    "\n",
    "    \n",
    "print(\"Finished merging first round at %s\" % now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SampleNumber\n",
      "catalog_Actual filename\n",
      "catalog_AlreadyTried\n",
      "catalog_Duplicate finder\n",
      "catalog_Final actual file name\n",
      "catalog_Formulaic File Name\n",
      "catalog_Has FTIR?\n",
      "catalog_How many are present?\n",
      "catalog_Is a breakline present?\n",
      "catalog_Might have photo\n",
      "catalog_Name of file\n",
      "catalog_Photo filename\n",
      "catalog_Photo filename (valueonly)\n",
      "catalog_Pill_mass_mg\n",
      "catalog_SampleForm\n",
      "catalog_SampleSource\n",
      "catalog_SoldAs\n",
      "catalog_Suggested Filename\n",
      "catalog_Tester\n",
      "catalog_Timestamp\n",
      "catalog_UID\n",
      "catalog_Unusual appearance\n",
      "catalog_UserSuspicion\n",
      "catalog_What colour is the pill?\n",
      "catalog_What is the logo?\n",
      "catalog_What is the quality of the press\n",
      "catalog_What is the shape of the pill?\n",
      "catalog_What tone does the colour have\n",
      "catalog_Which device was a photo taken with? Who does it belong to?\n",
      "ftir_AlreadyTried\n",
      "ftir_Analysis required\n",
      "ftir_Brief Note\n",
      "ftir_Brief Note.1\n",
      "ftir_Matches_soldas\n",
      "ftir_NOTES\n",
      "ftir_Next action(s)\n",
      "ftir_Note for harm reduction worker\n",
      "ftir_Powder_strength\n",
      "ftir_Primary_confidence\n",
      "ftir_Primary_hit\n",
      "ftir_Result going to service user?\n",
      "ftir_SampleForm\n",
      "ftir_Secondary_confidence\n",
      "ftir_Secondary_hit\n",
      "ftir_Send to HR team\n",
      "ftir_SoldAs\n",
      "ftir_Substance(s) detected\n",
      "ftir_Tester\n",
      "ftir_Timestamp\n",
      "ftir_UserSuspicion\n",
      "ftir_Was catalogued\n",
      "hr_Age\n",
      "hr_AlreadyTried\n",
      "hr_Bad_experience_with_batch\n",
      "hr_DRINK 1 - Type\n",
      "hr_DRINK 1: Quantity\n",
      "hr_DRINK 1: Vessel\n",
      "hr_DRINK 2 - Type\n",
      "hr_DRINK 2: Quantity\n",
      "hr_DRINK 2: Vessel\n",
      "hr_DRINK 3 - Type\n",
      "hr_DRINK 3: Quantity\n",
      "hr_DRINK 3: Vessel\n",
      "hr_DRINK 4 - Type\n",
      "hr_DRINK 4: Quantity\n",
      "hr_DRINK 4: Vessel\n",
      "hr_Ethnicity\n",
      "hr_Feeling_concerns\n",
      "hr_Gender\n",
      "hr_HR_worker\n",
      "hr_Had_2cb\n",
      "hr_Had_alcohol_today\n",
      "hr_Had_amphetamine\n",
      "hr_Had_cannabis\n",
      "hr_Had_cocaine\n",
      "hr_Had_codeine\n",
      "hr_Had_ecstascy_pill\n",
      "hr_Had_heroin_or_opioids\n",
      "hr_Had_ketamine\n",
      "hr_Had_lsd\n",
      "hr_Had_magic_mushrooms\n",
      "hr_Had_mdma_crystal\n",
      "hr_Had_mephedrone\n",
      "hr_Had_no2\n",
      "hr_Had_other_drugs\n",
      "hr_Had_other_illegal_drugs\n",
      "hr_Had_spice\n",
      "hr_Had_tramadol\n",
      "hr_Had_unknown_power\n",
      "hr_Had_valium_or_benzos\n",
      "hr_Had_xanax\n",
      "hr_Have you ever accessed a health service for your alcohol or drug use?\n",
      "hr_In general terms, who did you get the substance from?\n",
      "hr_Interview_abandonded\n",
      "hr_Number of people present\n",
      "hr_Over The Counter Medication\n",
      "hr_Plan to do?\n",
      "hr_Prescribed Medication\n",
      "hr_Previous_sample\n",
      "hr_Require further advice\n",
      "hr_SoldAs\n",
      "hr_Timestamp\n",
      "hr_Trust_supplier\n",
      "hr_Used_service\n",
      "hr_Was the sample bought, given or found?\n",
      "hr_What other actions will you do? (tick all that apply)\n",
      "hr_What was your first sample number at this event? Did you take a photo or keep the ticket?\n",
      "hr_When did you first use this batch?\n",
      "hr_When was the last time you used this service?\n",
      "hr_Where did you obtain this substance from?\n",
      "hr_Why did you bring this substance to be tested?\n",
      "mla_Colour\n",
      "mla_Dried weight w/ paper\n",
      "mla_Dried_mass_mg\n",
      "mla_FTIR\n",
      "mla_Fraction washed\n",
      "mla_Index in FOH sheet\n",
      "mla_Index in FTIR sheet\n",
      "mla_Logo\n",
      "mla_MDMA / tablet (mg)\n",
      "mla_MDMA_in_wash_mass\n",
      "mla_Mass lost (mg)\n",
      "mla_Notes\n",
      "mla_Paper (mg)\n",
      "mla_Paper + wash mass (mg)\n",
      "mla_Percent_MDMA\n",
      "mla_SampleForm\n",
      "mla_Tester\n",
      "mla_Wash mass\n",
      "mla_Whole pill (mg)\n"
     ]
    }
   ],
   "source": [
    "# columns = set()\n",
    "# for festival, dfs in data.items():\n",
    "#     columns.update(dfs.combined.columns.values)\n",
    "# for c in sorted(columns):\n",
    "#     print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-5392b211dae6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0muser_drug_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_user_drug_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0muser_drug_columns\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0m_tester_drug_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'FTIR_detected'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FTIR_detected_subtraction'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FTIR_final_result'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Get list of already known drugs\n",
    "# all_known = set()\n",
    "# all_known.update(*drugs_map.values())\n",
    "\n",
    "# for c \n",
    "# _user_drug_columns = [\"Had_other_drugs\", 'SoldAs', 'UserSuspicion']\n",
    "\n",
    "# _tester_drug_columns = ['FTIR_detected', 'FTIR_detected_subtraction', 'FTIR_final_result']\n",
    "# tester_drug_columns = []\n",
    "# for cname in _tester_drug_columns:\n",
    "#     tester_drug_columns += [c for c in df.columns.values if cname in c]\n",
    "    \n",
    "\n",
    "# for festival, dfs in data.items():\n",
    "#     dfs.combined = clean_sample_form(dfs.combined)\n",
    "    \n",
    "#     user_drug_columns = []\n",
    "# for cname in _user_drug_columns:\n",
    "#     user_drug_columns += [c for c in df.columns.values if cname in c]\n",
    "\n",
    "    \n",
    "\n",
    "# prefixes = ['catalog_', 'ftir_', 'mla_', 'hr_']\n",
    "# all_drug_columns = []\n",
    "# for d in drug_columns:\n",
    "#     for p in prefixes:\n",
    "#         all_drug_columns.append(p+d)\n",
    "\n",
    "# drug_names = set()\n",
    "# for festival, dfs in data.items():\n",
    "#     df = dfs.combined\n",
    "#     for cname in user_drug_columns:\n",
    "#         if cname in df.columns.values:\n",
    "#             drug_names.update(df[cname].str.lower().unique())\n",
    "# if np.nan in drug_names:\n",
    "#     drug_names.remove(np.nan)\n",
    "# if None in drug_names:\n",
    "#     drug_names.remove(None)\n",
    "# drug_names = set([s.strip() for s in drug_names])\n",
    "\n",
    "# for d in sorted(drug_names):\n",
    "#     print(\"'%s'\" % d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SampleNumber',\n",
       " 'catalog_SampleSource',\n",
       " 'catalog_SampleForm',\n",
       " 'ftir_SampleForm',\n",
       " 'mla_SampleForm',\n",
       " 'hr_What was your first sample number at this event? Did you take a photo or keep the ticket?',\n",
       " 'hr_Was the sample bought, given or found?']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we overwrite the values - if necessary we could create separate columns\n",
    "# Create dict for replace function is form {column : {value_to_replace, replacement_value}}\n",
    "replace_d = {}\n",
    "drug_columns = ['sold/acquired/advertised as', 'Client suspicion', 'Final Result', 'SubmittedSubstanceAs', 'other_specify']\n",
    "\n",
    "# Firstly convert all columns to lower case and remove any spaces\n",
    "def clean(value):\n",
    "    if type(value) is str:\n",
    "        value = value.strip().lower()\n",
    "    return value\n",
    "\n",
    "for column in drug_columns:\n",
    "    df_final[column] = df_final[column].map(clean, na_action='ignore')\n",
    "\n",
    "for column in drug_columns:\n",
    "    replace_d[column] = {}\n",
    "    for drug, names in drugs_map.items():\n",
    "        for name in names:\n",
    "            replace_d[column][name] = drug\n",
    "\n",
    "# Replace values\n",
    "df_final.replace(replace_d, inplace=True)\n",
    "            \n",
    "# NO_ANALYSIS as is treated separtely as only applies to Final Result - also can't include with other dict\n",
    "# or the replacement values and keys overlap\n",
    "NO_ANALYSIS = 'analysis_inconclusive'\n",
    "no_analysis = ['compound not in library', 'inconclusive', 'insufficient quantity for testing', \n",
    "               'insufficient sample', 'insufficient sample', 'lost', 'no active component identified', \n",
    "               'no match', 'no match', 'none', 'nothing detected', 'result missing', 'unable to test', 'unknown']\n",
    "\n",
    "# Fix 'Final Result' for NO_ANALYSIS\n",
    "column = 'Final Result'\n",
    "replace_d = {column: {}}\n",
    "for name in no_analysis:\n",
    "    replace_d[column][name] = NO_ANALYSIS\n",
    "\n",
    "# Replace values\n",
    "df_final.replace(replace_d, inplace=True)\n",
    "\n",
    "# Additional grouping requested by Fiona\n",
    "column = 'sold/acquired/advertised as'\n",
    "replace_d = {column: {'found' : 'unknown',\n",
    "                      \"don't know\" : 'unknown',\n",
    "                      'not sure' : 'unknown',\n",
    "                     }}\n",
    "df_final.replace(replace_d, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 43 duplicated ftir SampleNumbers ['F0071', 'F0247', 'F0206', 'F0367', 'F0019', 'F0546', 'F0446', 'F0005', 'F0659', 'F1137', 'F0983', 'F0938', 'F0869', 'F0838', 'F0981', 'F0865', 'F0668', 'F0816', 'F0878', 'F0885', 'F0833', 'F0815', 'F1196', 'F1253', 'F1313', 'F1393', 'F1215', 'F1392', 'F1640', 'F1172', 'F1606', 'F1433', 'F1431', 'F1609', 'F1660', 'F1623', 'F1792', 'F0912', 'F1876', 'F1830', 'F1262', 'F1439', 'F1904'] ###\n",
      "### 1 duplicated hr SampleNumbers ['F9999'] ###\n"
     ]
    }
   ],
   "source": [
    "# The code above runs across all festival data. The code below is for looking at the data for an individual\n",
    "# festival (and so really should be in a function), but for the time being we just set the variables we\n",
    "# require here\n",
    "dfs = data['boomtown']\n",
    "duplicates = Duplicates(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate FTIR SampleNumber B0103 (line: 28) DIFFERENT Catalog sample (line: 36)\n",
      "['Found or otherwise not known', '', 'powder'] 2018-08-25 15:56:29\n",
      "['Found or otherwise not known', 'No', 'powder'] 2018-08-25 15:02:33\n",
      "Duplicate FTIR SampleNumber B0103 (line: 29) DIFFERENT Catalog sample (line: 36)\n",
      "['Found or otherwise not known', '', 'pill'] 2018-08-25 15:58:35\n",
      "['Found or otherwise not known', 'No', 'powder'] 2018-08-25 15:02:33\n",
      "Duplicate FTIR SampleNumber B0131 (line: 50) MATCHES Catalog sample (line: 55)\n",
      "Duplicate FTIR SampleNumber B0131 (line: 51) MATCHES Catalog sample (line: 55)\n",
      "Duplicate FTIR SampleNumber B0221 (line: 136) DIFFERENT Catalog sample (line: 132)\n",
      "['Found or otherwise not known', 'No', 'powder'] 2018-08-26 15:42:33\n",
      "['', '', 'powder'] 2018-08-26 15:30:58\n",
      "Duplicate FTIR SampleNumber B0221 (line: 137) DIFFERENT Catalog sample (line: 132)\n",
      "['Found or otherwise not known', 'No', 'powder'] 2018-08-26 15:58:20\n",
      "['', '', 'powder'] 2018-08-26 15:30:58\n",
      "Duplicate FTIR SampleNumber B0229 (line: 145) DIFFERENT Catalog sample (line: 140)\n",
      "['Found or otherwise not known', 'No', 'powder'] 2018-08-26 15:56:23\n",
      "['', '', 'powder'] 2018-08-26 15:48:00\n",
      "Duplicate FTIR SampleNumber B0229 (line: 146) DIFFERENT Catalog sample (line: 140)\n",
      "['Found or otherwise not known', 'No', 'pill'] 2018-08-26 16:31:40\n",
      "['', '', 'powder'] 2018-08-26 15:48:00\n"
     ]
    }
   ],
   "source": [
    "def find_duplicate_matches(duplicates, df1, df2, df1_name='DataFrame1', df2_name='DataFrame2'):\n",
    "    hr = False\n",
    "    if df1_name.lower()[:2] == 'hr':\n",
    "        hr = True\n",
    "    duplicate_matches = {}\n",
    "    min_stage_delay = 60 * 1\n",
    "    max_stage_delay = 60 * 60\n",
    "    for sample_number in duplicates:\n",
    "        duplicate_matches[sample_number] = {}\n",
    "        for df1_idx, df1_row in df1.loc[df1['SampleNumber'] == sample_number].iterrows():\n",
    "            for df2_idx, df2_row in df2.loc[df2['SampleNumber'] == sample_number].iterrows():\n",
    "                df1_data = df1_row.loc[['SoldAs', 'AlreadyTried']].values.tolist()\n",
    "                if not hr:\n",
    "                    df1_data.append(df1_row.SampleForm)\n",
    "                df1_time = df1_row.Timestamp\n",
    "                df2_data = df2_row.loc[['SoldAs', 'AlreadyTried']].values.tolist()\n",
    "                if not hr:\n",
    "                    df2_data.append(df2_row.SampleForm)\n",
    "                df2_time = df2_row.Timestamp\n",
    "                if df2_time >= df1_time:\n",
    "                    delta_t = (df2_time - df1_time).seconds\n",
    "                else:\n",
    "                    delta_t = (df1_time - df2_time).seconds\n",
    "                if df1_data == df2_data and min_stage_delay < delta_t <= max_stage_delay:\n",
    "                    print(\"Duplicate %s SampleNumber %s (line: %d) MATCHES %s sample (line: %d)\" % \\\n",
    "                          (df1_name, sample_number, df1_idx + 1, df2_name, df2_idx + 1))\n",
    "                    duplicate_matches[sample_number][df1_idx] = True\n",
    "                else:\n",
    "                    print(\"Duplicate %s SampleNumber %s (line: %d) DIFFERENT %s sample (line: %d)\\n%s %s\\n%s %s\" % \\\n",
    "                          (df1_name, sample_number, df1_idx + 1, df2_name, df2_idx + 1,\n",
    "                           df1_data, df1_time,\n",
    "                           df2_data, df2_time))\n",
    "                    duplicate_matches[sample_number][df1_idx] = False\n",
    "    return duplicate_matches\n",
    "\n",
    "def match_orphans_to_duplicates(df1_orphans, duplicate_matches, df1, df2):\n",
    "    for orphan_sample_number in df1_orphans:\n",
    "        df1_data = df1.loc[df1['SampleNumber'] == orphan_sample_number, ['SampleForm', 'SoldAs', 'AlreadyTried', 'Timestamp']]\n",
    "        df1_data = df1_data.values.tolist()[0]\n",
    "        df1_time = df1_data.pop()\n",
    "        for sample_number, indexd in duplicate_matches.items():\n",
    "            for k, v in indexd.items():\n",
    "                if not v:\n",
    "                    df2_data = dfs.catalog.iloc[k][['SampleForm', 'SoldAs', 'AlreadyTried', 'Timestamp']].values.tolist()\n",
    "                    df2_time = df2_data.pop()\n",
    "                    if df2_time >= df1_time:\n",
    "                        delta_t = (df2_time - df1_time).seconds\n",
    "                    else:\n",
    "                        delta_t = (df1_time - df2_time).seconds\n",
    "                    if df2_data == df1_data and min_stage_delay < delta_t <= max_stage_delay:\n",
    "                        print(\"Orphan {} could be match for duplicate {} (line: {})\\n{} {}\\n{} {}\".format(orphan_sample_number,\n",
    "                                                                                                  sample_number, k+1,\n",
    "                                                                                                  df2_data, df2_time,\n",
    "                                                                                                  df1_data, df1_time))\n",
    "\n",
    "\n",
    "#duplicate_matches = find_duplicate_matches(duplicates.catalog, dfs.catalog, dfs.ftir, df1_name='Catalog', df2_name='FTIR')\n",
    "duplicate_matches = find_duplicate_matches(duplicates.ftir, dfs.ftir, dfs.catalog, df1_name='FTIR', df2_name='Catalog')\n",
    "#duplicate_matches = find_duplicate_matches(duplicates.hr, dfs.hr, dfs.catalog, df1_name='HR', df2_name='Catalog')\n",
    "\n",
    "#match_orphans_to_duplicates(ftir_orphan, duplicate_matches, dfs.ftir, dfs.catalog)\n",
    "# match_orphans_to_duplicates(catalog_orphan, duplicate_matches, dfs.catalog, dfs.ftir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad SampleNumber FXXX\n",
      "Bad SampleNumber FXXX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmht/miniconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:20: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "/Users/jmht/miniconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:23: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n"
     ]
    }
   ],
   "source": [
    "# Check orphans against the FTIR sheet using just their numbers\n",
    "def match_orphans_with_sample_integer(orphans, orphan_df, ref_df):\n",
    "#     min_stage_delay = 60 * 1\n",
    "#     max_stage_delay = 60 * 60\n",
    "    def to_int(sn):\n",
    "        if type(sn) is str:\n",
    "            try:\n",
    "                sn = int(sn[-4:])\n",
    "            except ValueError:\n",
    "                print(\"Bad SampleNumber %s\" % sn)\n",
    "                sn = np.nan\n",
    "        return sn\n",
    "    orphan_df['SampleInteger'] = orphan_df['SampleNumber'].apply(to_int)\n",
    "    ref_df['SampleInteger'] = ref_df['SampleNumber'].apply(to_int)\n",
    "    orphan_ints = map(to_int, orphans)\n",
    "    skipform = True\n",
    "    for orphan_sample_number, oint in zip(orphans, orphan_ints):\n",
    "        for orphan_idx, orphan_row in orphan_df.loc[orphan_df['SampleNumber'] == orphan_sample_number].iterrows():\n",
    "            for ref_idx, ref_row in ref_df.loc[ref_df['SampleInteger'] == oint].iterrows():\n",
    "                orphan_data = orphan_row.loc[['SampleForm', 'SoldAs', 'AlreadyTried']].values.tolist()\n",
    "                orphan_time = orphan_row.Timestamp\n",
    "                ref_sample_number = ref_row.SampleNumber\n",
    "                ref_data = ref_row.loc[['SampleForm', 'SoldAs', 'AlreadyTried']].values.tolist()\n",
    "                ref_time = ref_row.Timestamp\n",
    "                delta_t = (ref_time - orphan_time).seconds\n",
    "#                 if orphan_data == ref_data and min_stage_delay < delta_t <= max_stage_delay:\n",
    "                if skipform:\n",
    "                    orphan_data.pop(0)\n",
    "                    ref_data.pop(0)\n",
    "                if orphan_data == ref_data:\n",
    "                    print(\"HR orphan %s (line: %d) could be match for FTIR SampleNumber %s (line: %d)\\n%s %s\\n%s %s\" % \\\n",
    "                          (orphan_sample_number, orphan_idx + 1, ref_sample_number, ref_idx + 1, orphan_data, orphan_time, ref_data, ref_time))\n",
    "\n",
    "# match_orphans_with_sample_integer(catalog_orphan, dfs.catalog, dfs.ftir)\n",
    "match_orphans_with_sample_integer(hr_orphan, dfs.hr, dfs.catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmht/miniconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:9: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# Check orphans against other orphans just using data\n",
    "def match_orphans_vs_orphans(orphan1_list, orphan1_df, orphan2_list, orphan2_df, hr=False):\n",
    "    min_stage_delay = 60 * 1\n",
    "    max_stage_delay = 60 * 60\n",
    "    for orphan1 in orphan1_list:\n",
    "        orphan1_row = orphan1_df.loc[orphan1_df['SampleNumber'] == orphan1].iloc[0]\n",
    "        for orphan2 in orphan2_list:\n",
    "            orphan2_row = orphan2_df.loc[orphan2_df['SampleNumber'] == orphan2].iloc[0]\n",
    "            orphan1_data = orphan1_row.loc[['SampleForm', 'SoldAs', 'AlreadyTried']].values.tolist()\n",
    "            orphan1_time = orphan1_row.Timestamp\n",
    "            orphan2_data = orphan2_row.loc[['SampleForm', 'SoldAs', 'AlreadyTried']].values.tolist()\n",
    "            orphan2_time = orphan2_row.Timestamp\n",
    "            delta_t = (orphan2_time - orphan1_time).seconds\n",
    "            if orphan1_data == orphan2_data and min_stage_delay <= delta_t <= max_stage_delay:\n",
    "                print(\"orphan1 %s could be match for orphan2 %s\\n%s %s\\n%s %s\" % \\\n",
    "                      (orphan1, orphan2, orphan1_data, orphan1_time, orphan1_data, orphan1_time))\n",
    "\n",
    "# match_orphans_with_sample_integer(catalog_orphan, dfs.catalog, dfs.ftir)\n",
    "match_orphans_vs_orphans(catalog_orphan, dfs.catalog, ftir_orphan, dfs.ftir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfs.catalog[dfs.catalog['SampleNumber'].isin(catalog.duplicates)]\n",
    "#pd.DataFrame({'SampleNumber' : duplicates.ftir}).to_csv('ftir_sn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean up catalog\n",
    "# Drop all unwanted columns\n",
    "\n",
    "#  or 'Your initials'\n",
    "l = set(['Your initials',\n",
    "         'Your name and first initial',\n",
    "         'Which device was a photo taken with? Who does it belong to?',\n",
    "         'Is a breakline present?',\n",
    "         'Unusual appearance'\n",
    "        ])\n",
    "\n",
    "to_drop = set(dfs.catalog.columns).intersection(l)\n",
    "dfs.catalog.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "d = {\n",
    "    'Timestamp' : 'Catalog timestamp',\n",
    "    'Sample Advertised/Acquired/Sold As': 'Catalog_SoldAs',\n",
    "    'Sample Form' : 'Catalog_Form',\n",
    "    'Has the Service User or a close friend tried this batch?': 'Catalog_Tried',\n",
    "    'What is the mass? (mg)': 'FullPillMass',\n",
    "    'What is the shape of the pill?': 'PillShape',\n",
    "    'What is the logo?': 'PillLogo',\n",
    "    'What colour is the pill?': 'PillColour'\n",
    "}\n",
    "dfs.catalog.rename(columns=d, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('COLS ', Index([                                                           u'Timestamp',\n",
      "                                                              u'Sample Number',\n",
      "                                                                     u'Tester',\n",
      "                                                                    u'Sold As',\n",
      "                                                                u'Sample Form',\n",
      "                                                              u'Already Tried',\n",
      "                                                             u'User Suspicion',\n",
      "                                                         u'Substance detected',\n",
      "                                                             u'Hit Confidence',\n",
      "                                                          u'Compound detected',\n",
      "                                                           u'Hit Confidence.1',\n",
      "                                                                 u'Brief Note',\n",
      "                           u'Is anything detected after subtraction analysis?',\n",
      "                                            u'Compound detected (Subtraction)',\n",
      "                                                           u'Hit Confidence.2',\n",
      "                                                       u'Substance detected.1',\n",
      "                                                           u'Hit Confidence.3',\n",
      "                                                               u'Brief Note.1',\n",
      "                                                             u'Next action(s)',\n",
      "                                                      u'Substance(s) detected',\n",
      "                                           u'\"Strength\" of powdered substance',\n",
      "       u'Does the substance detected match the substance that was advertised?',\n",
      "                                             u'Note for harm reduction worker',\n",
      "                                                            u'Send to HR team'],\n",
      "      dtype='object'))\n",
      "('SS ', 1691    N-Ethylpentylone\n",
      "1692    N-Ethylpentylone\n",
      "1694    N-Ethylpentylone\n",
      "1696             Cocaine\n",
      "0                   MDMA\n",
      "Name: Substance detected, dtype: object)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_ftir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f76c2c2a13ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Compound detected (Subtraction)'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'Other'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Compound detected (Subtraction)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_ftir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Substance detected.1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Copy values from 'Compound detected'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hit Confidence.2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hit Confidence.3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Substance detected.1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Hit Confidence.3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Brief Note.1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_ftir' is not defined"
     ]
    }
   ],
   "source": [
    "# For FTIR columns need to merge the data from the 'Compound detected', 'Hit Confidence.1' columns into the\n",
    "# 'Substance detected', 'Hit Confidence' column where the substance detected was 'other'\n",
    "print(\"COLS \",dfs.ftir.columns)\n",
    "print(\"SS \",dfs.ftir['Substance detected'][:5])\n",
    "mask = dfs.ftir['Substance detected'] != 'Other'\n",
    "dfs.ftir['Substance detected'].where(mask, dfs.ftir['Compound detected'], inplace=True) # Copy values from 'Compound detected'\n",
    "dfs.ftir['Hit Confidence'].where(mask, dfs.ftir['Hit Confidence.1'], inplace=True)\n",
    "dfs.ftir.drop(['Compound detected', 'Hit Confidence.1', 'Brief Note'], axis=1, inplace=True)\n",
    "\n",
    "mask = dfs.ftir['Compound detected (Subtraction)'] != 'Other'\n",
    "dfs.ftir['Compound detected (Subtraction)'].where(mask, df_ftir['Substance detected.1'], inplace=True) # Copy values from 'Compound detected'\n",
    "dfs.ftir['Hit Confidence.2'].where(mask, dfs.ftir['Hit Confidence.3'], inplace=True)\n",
    "dfs.ftir.drop(['Substance detected.1', 'Hit Confidence.3', 'Brief Note.1'], axis=1, inplace=True)\n",
    "\n",
    "# Drop all unwanted columns\n",
    "l = ['Your name and surname initial',\n",
    "     'User Suspicion',\n",
    "     'Is anything detected after subtraction analysis?',\n",
    "     'Analysis required', \n",
    "     'Next action(s)',\n",
    "     'Send to HR team'\n",
    "    ]\n",
    "#'Note for harm reduction worker'\n",
    "to_drop = set(dfs.ftir.columns).intersection(l)\n",
    "dfs.ftir.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Rename shared columns so that we can check for any errors and remove any columns not of interest to the master df\n",
    "d = {\n",
    "    'Timestamp' : 'FTIR timestamp',\n",
    "    'Sample Sold As': 'FTIR Sold As',\n",
    "    'Sample Form' : 'FTIR form',\n",
    "    'Has the Service User or a close friend tried this batch?': 'FTIR tried',\n",
    "    'Substance(s) detected' : 'FTIR final result',\n",
    "    'Substance detected' : 'FTIR result1',\n",
    "    'Hit Confidence' :  'FTIR hit1',\n",
    "    'Is anything detected after subtraction analysis?' : 'FTIR subtraction positive',\n",
    "    'Compound detected (Subtraction)' :  'FTIR result2',\n",
    "    'Hit Confidence.2' :  'FTIR hit2',\n",
    "    '\"Strength\" of powdered substance' : 'FTIR Powder Strength',\n",
    "    'Does the substance detected match the substance that was advertised?' : 'FTIR Matches Sold As',\n",
    "}\n",
    "dfs.ftir.rename(columns=d, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean up HR form\n",
    "\n",
    "# Drop all unwanted columns\n",
    "l = ['HR worker name:']\n",
    "dfs.hr.drop(l, axis=1, inplace=True)\n",
    "\n",
    "# Rename shared columns so that we can check for any errors and remove any columns not of interest to the master df\n",
    "d = {\n",
    "    'Timestamp' : 'HR timestamp',\n",
    "    'You submitted a substance for analysis. What were you told it was when you got it?': 'HR Sold as',\n",
    "    'Had you already tried this substance before getting it tested?': 'HR tried',\n",
    "    'What was your first sample number at this event? Did you take a photo or keep the ticket?': 'Previous Sample Number'\n",
    "}\n",
    "dfs.hr.rename(columns=d, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fix column orders\n",
    "prefix = ['Sample Number',\n",
    "          'Catalog timestamp', 'FTIR timestamp', 'HR timestamp',\n",
    "          'Catalog Sold As', 'FTIR Sold As','HR Sold as', \n",
    "          'Catalog form', 'FTIR form',\n",
    "          'Catalog tried', 'FTIR tried', 'HR tried']\n",
    "columns = [c for c in df_all.columns if c not in prefix]\n",
    "columns = prefix + columns\n",
    "df_all = df_all[columns]\n",
    "df_all.to_csv('foo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
