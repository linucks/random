{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Module imports\n",
    "import copy\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def fix_sample_number(x):\n",
    "    \"\"\"Make sure all samples numbers are of form: AXXX (where A is one of A, F, W and X is a digit)\"\"\"\n",
    "    if isinstance(x, float) and np.isnan(x):\n",
    "        return x # leave NaN's alone\n",
    "    if (isinstance(x, str) or isinstance(x, unicode)) and len(x) == 0:\n",
    "        return np.nan\n",
    "    try:\n",
    "        sn = 'F{:04d}'.format(int(x))\n",
    "    except ValueError:\n",
    "        # Assume string so make sure it's of the right format\n",
    "        sn = str(x).strip().capitalize()\n",
    "    if len(sn) != 5 or sn[0] not in ['A', 'F', 'W']:\n",
    "        print(\"!!! Bad ID \\'%s\\'\" % sn)\n",
    "    return sn\n",
    "\n",
    "def now():\n",
    "    return datetime.datetime.now().strftime(\"%d/%m/%y %H:%M:%S\")\n",
    "\n",
    "def enumerate_duplicates(row):\n",
    "    \"\"\"Append a counter to duplicate labels\"\"\"\n",
    "    SEPARATOR = '.'\n",
    "    duplicates = {}\n",
    "    updated_row = []\n",
    "    for r in row:\n",
    "        count = duplicates.get(r, 0)\n",
    "        if count > 0:\n",
    "            label = \"{}{}{}\".format(r, SEPARATOR, count)\n",
    "        else:\n",
    "            label = r\n",
    "        updated_row.append(label)\n",
    "        duplicates[r] = count + 1\n",
    "    return updated_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = 'raise'\n",
    "\n",
    "# Need to define in main or we can't pickle the data objects\n",
    "class DataFrames(object):\n",
    "    def __init__(self):\n",
    "        catalog = None\n",
    "        ftir = None\n",
    "        reagent = None\n",
    "        mla = None\n",
    "        hr = None\n",
    "\n",
    "def gsheets_service():\n",
    "    from googleapiclient.discovery import build\n",
    "    from httplib2 import Http\n",
    "    from oauth2client import file, client, tools\n",
    "    # If modifying these scopes, delete the file token.json.\n",
    "    CREDS_FILE = '/opt/random/MADE/JensDataExportJupyter_client_secret.json'\n",
    "    SCOPES = 'https://www.googleapis.com/auth/spreadsheets.readonly'\n",
    "    store = file.Storage('token.json')\n",
    "    creds = store.get()\n",
    "    if not creds or creds.invalid:\n",
    "        import argparse\n",
    "        flags = argparse.ArgumentParser(parents=[tools.argparser]).parse_args([])\n",
    "        flow = client.flow_from_clientsecrets(CREDS_FILE, SCOPES)\n",
    "        creds = tools.run_flow(flow, store, flags)\n",
    "    service = build('sheets', 'v4', http=creds.authorize(Http()))\n",
    "    return service\n",
    "\n",
    "def get_df(service, SPREADSHEET_ID, SS_RANGE, mla=False):\n",
    "    # Call the Sheets API\n",
    "    result = service.spreadsheets().values().get(spreadsheetId=SPREADSHEET_ID,\n",
    "                                                range=SS_RANGE).execute()\n",
    "    values = result.get('values', [])\n",
    "    if not values:\n",
    "        print('*** No data found ***')\n",
    "        return None\n",
    "\n",
    "    # mla has irrelevant stuff in columns 1 and 3 and sample numbers in first column\n",
    "    if mla:\n",
    "        values.pop(0)\n",
    "        values.pop(1)\n",
    "        def not_blank(row):\n",
    "            return len(row[0]) > 0       \n",
    "    else:\n",
    "        def not_blank(row):\n",
    "            return sum(map(len, row[:6])) > 0\n",
    "\n",
    "    rows = filter(not_blank, values)\n",
    "    if not rows:\n",
    "        print('*** No data found after pruning rows! ***')\n",
    "        return None\n",
    "    \n",
    "    columns = enumerate_duplicates(rows[0])\n",
    "    ncols = len(rows[0])\n",
    "    row_max = max(map(len, rows[1:]))\n",
    "    width = min(ncols, row_max)\n",
    "    return pd.DataFrame(rows[1:], columns=columns[:width])\n",
    "\n",
    "def canonicalise_df(df, source=None):\n",
    "    \"\"\"Initial cleaning of all dataframes\"\"\"\n",
    "    #from pandas._libs.tslib import OutOfBoundsDatetime\n",
    "    if source:\n",
    "        print(\"Canonicalising %s\" % source)\n",
    "    # Standardise names\n",
    "    d = {\n",
    "        'Sample Code':'SampleNumber',\n",
    "        'Sample Number:':'SampleNumber',\n",
    "        'Sample Number':'SampleNumber',\n",
    "        'Sample number':'SampleNumber',\n",
    "        'Sample Num':'SampleNumber',\n",
    "        'Sample Number i.e F0XXX' : 'SampleNumber',\n",
    "        \n",
    "        'Sample Advertised/Acquired/Sold As' : 'SoldAs',\n",
    "        'Sample Sold As' : 'SoldAs',\n",
    "        'You submitted a substance for analysis. What were you told it was when you got it?':  'SoldAs',\n",
    "        \n",
    "        \n",
    "        'Sample Source' :'SampleSource',\n",
    "\n",
    "        'User Suspicion' :'UserSuspicion',\n",
    "\n",
    "        'Sample Form' :'SampleForm',\n",
    "\n",
    "        'Has the Service User or a close friend tried this batch?' : 'AlreadyTried',\n",
    "        'Had you already tried this substance before getting it tested?' : 'AlreadyTried',\n",
    "\n",
    "        'Your initials' : 'Tester',\n",
    "        'Your name and first initial' : 'Tester',\n",
    "        'Your name and surname initial' : 'Tester'\n",
    "    }\n",
    "    df.rename(columns=d, inplace=True)\n",
    "    \n",
    "    def fix_timestamp(x):\n",
    "        return pd.to_datetime(str(x), format='%d/%m/%Y %H:%M:%S')\n",
    "    if 'Timestamp' in df.columns:\n",
    "        df.loc[:, 'Timestamp'] = df['Timestamp'].map(fix_timestamp)\n",
    "    df.loc[:, 'SampleNumber'] = df['SampleNumber'].apply(fix_sample_number)\n",
    "    df.dropna(subset=['SampleNumber'])\n",
    "    #df.sort_values(['Sample Number'], ascending=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "def get_data(service, SPREADSHEET_ID):\n",
    "\n",
    "    CATALOG_RANGE = 'Catalog!A:R'\n",
    "    FTIR_RANGE = 'FTIR!A:X'\n",
    "    REAGENT_RANGE = 'Reagent!A:W'\n",
    "    MLA_RANGE = 'MLA!A:R'\n",
    "    HR_RANGE = 'Interventions!A:BJ'\n",
    "\n",
    "    df_catalog = get_df(service, SPREADSHEET_ID, CATALOG_RANGE)\n",
    "    df_catalog = canonicalise_df(df_catalog, source='catalog')\n",
    "    df_ftir = get_df(service, SPREADSHEET_ID, FTIR_RANGE)\n",
    "    df_ftir = canonicalise_df(df_ftir, source='ftir')\n",
    "    df_reagent = get_df(service, SPREADSHEET_ID, REAGENT_RANGE)\n",
    "    df_reagent = canonicalise_df(df_reagent, source='reagent')\n",
    "    df_mla = get_df(service, SPREADSHEET_ID, MLA_RANGE, mla=True)\n",
    "    df_mla = canonicalise_df(df_mla, source='mla')\n",
    "    try:\n",
    "        df_hr = get_df(service, SPREADSHEET_ID, HR_RANGE)\n",
    "    except ValueError:\n",
    "        df_hr = None\n",
    "    if df_hr is not None:\n",
    "        df_hr = canonicalise_df(df_hr, source='hr')\n",
    "\n",
    "    df = DataFrames()\n",
    "    df.catalog = df_catalog\n",
    "    df.ftir = df_ftir\n",
    "    df.reagent = df_reagent\n",
    "    df.mla = df_mla\n",
    "    df.hr = df_hr\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING BOOMTOWN\n",
      "Canonicalising catalog\n",
      "!!! Bad ID 'R0876'\n",
      "Canonicalising ftir\n",
      "!!! Bad ID 'F00129'\n",
      "!!! Bad ID 'Z1000'\n",
      "!!! Bad ID 'B0076'\n",
      "Canonicalising reagent\n",
      "!!! Bad ID 'Z1000'\n",
      "!!! Bad ID 'Yellow > green'\n",
      "!!! Bad ID 'F1819 (or 1827?)'\n",
      "Canonicalising mla\n",
      "Canonicalising hr\n",
      "!!! Bad ID 'G0037'\n",
      "!!! Bad ID 'G0242'\n",
      "!!! Bad ID 'G0153'\n",
      "!!! Bad ID 'G0024'\n",
      "!!! Bad ID 'G0652'\n",
      "!!! Bad ID 'G9999'\n",
      "!!! Bad ID 'G0877'\n",
      "!!! Bad ID 'G0878'\n",
      "!!! Bad ID 'G0811'\n",
      "!!! Bad ID 'G1441'\n",
      "!!! Bad ID 'G1216'\n",
      "!!! Bad ID 'G1228'\n",
      "!!! Bad ID 'G1229'\n",
      "!!! Bad ID 'G1398'\n",
      "!!! Bad ID 'G1284'\n",
      "!!! Bad ID 'G1572'\n",
      "!!! Bad ID 'G0875'\n",
      "!!! Bad ID 'G1833'\n",
      "!!! Bad ID 'G1703'\n",
      "!!! Bad ID 'G1860'\n",
      "!!! Bad ID 'G1859'\n",
      "!!! Bad ID 'G1699'\n",
      "!!! Bad ID 'G1686'\n",
      "!!! Bad ID 'G1312'\n",
      "!!! Bad ID 'G0420'\n",
      "!!! Bad ID 'G0983'\n",
      "!!! Bad ID 'G0981'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The ID and range of a sample spreadsheet.\n",
    "BOOMTOWN2018_SPREADSHEET_ID = '1RiA-FwG_954Ger2VPsOSA3JLh-7sEoTYr40eVS0mp24'\n",
    "MADE2018_SPREADSHEET_ID = '1daXdyL6uL8qnMsEsP0RLZE9nDzt6J7Zr1ygQdguvi-E'\n",
    "BOARDMASTERS2018_SPREADSHEET_ID = '1U1lhUWLazDBN-wb2eZM8YV674f46npVfQK3XUVZjPow'\n",
    "SW42018_SPREADSHEET_ID = '1agpMmJ9XukeWXS5_mwrDSKeshUaFtYwOzsPiR1DKsPU'\n",
    "LOSTVILLAGE2018_SPREADSHEET_ID = '1OL0gyXrpZnJ8e7yR7eF6S2OaBYBiPDoVp5xGpdK4wlA'\n",
    "BESTIVAL2018_SPREADSHEET_ID = '184qudGcw4PB0SMtOo0ZBDtckeGaH0RCLUXbA-u3BiHE'\n",
    "YNOT2018_SPREADSHEET_ID = '1D01cj-Mra06TuoG_MsKuLq9OdtvKzrvRdiE255po_ag'\n",
    "TRUCKFEST2018_SPREADSHEET_ID = '1sGG9WJxKyD2CGUjzJAXul3g9hVnRz6HbTiqKV5cUAyA'\n",
    "LSTD2018_SPREADSHEET_ID = '1R8YqDnrhvuVMwPFShwaaAUIyCXQMeozA230OXsFsDQM'\n",
    "KENDALCALLING2018_SPREADSHEET_ID = '16-PfwBOaUxwod3X75LGk1VAjBblkNsTJpCsX825aghI'\n",
    "PARKLIFE2018_SPREADSHEET_ID = '1oO5sHcUhUn_7M1Hap73sOZHNEfWFMcDkQuWDRFf4d-w'\n",
    "\n",
    "\n",
    "data = {}\n",
    "service = gsheets_service()\n",
    "print \"PROCESSING BOOMTOWN\"\n",
    "data['boomtown'] = get_data(service, BOOMTOWN2018_SPREADSHEET_ID)\n",
    "# print \"PROCESSING BOARDMASTERS\"\n",
    "# data['boardmasters'] = get_data(service, BOARDMASTERS2018_SPREADSHEET_ID)\n",
    "# print \"PROCESSING MADE\"\n",
    "# data['made'] = get_data(service, MADE2018_SPREADSHEET_ID)\n",
    "# print \"PROCESSING SW4\"\n",
    "# data['sw4'] = get_data(service, SW42018_SPREADSHEET_ID)\n",
    "# print \"PROCESSING LOST VILLAGE\"\n",
    "# data['lostvillage'] = get_data(service, LOSTVILLAGE2018_SPREADSHEET_ID)\n",
    "# print \"PROCESSING BESTIVAL\"\n",
    "# data['bestival'] = get_data(service, BESTIVAL2018_SPREADSHEET_ID)\n",
    "# print \"PROCESSING YNOT\"\n",
    "# data['ynot'] = get_data(service, YNOT2018_SPREADSHEET_ID)\n",
    "# print \"PROCESSING TRUCKFEST\"\n",
    "# data['truckfest'] = get_data(service, TRUCKFEST2018_SPREADSHEET_ID)\n",
    "# print \"PROCESSING LSTD\"\n",
    "# data['lstd'] = get_data(service, LSTD2018_SPREADSHEET_ID)\n",
    "# print \"PROCESSING KENDAL CALLING\"\n",
    "# data['kc'] = get_data(service, KENDALCALLING2018_SPREADSHEET_ID)\n",
    "# print \"PROCESSING PARKLIFE\"\n",
    "# data['parklife'] = get_data(service, PARKLIFE2018_SPREADSHEET_ID)\n",
    "\n",
    "import pickle\n",
    "with open('foo_multi.pkl','w') as w:\n",
    "    pickle.dump(data, w)\n",
    "    \n",
    "dfs = data['boomtown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 44 duplicated catalog SampleNumbers ['F0109', 'F0076', 'F0024', 'F0158', 'F0255', 'F0275', 'F0344', 'F0373', 'F0446', 'F0690', 'F0579', 'F0833', 'F0939', 'F0870', 'F0934', 'F0930', 'F0873', 'F0857', 'F0945', 'F0858', 'F0853', 'F0943', 'F0944', 'F0855', 'F0848', 'F0695', 'F0862', 'F0831', 'F0852', 'F0889', 'F0846', 'F0904', 'F0922', 'F1122', 'F0976', 'F1170', 'F0012', 'F1207', 'F1227', 'F1585', 'F1665', 'F1660', 'F1856', 'F1873'] ###\n",
      "### 47 duplicated FTIR SampleNumbers ['F0017', 'F0247', 'F0266', 'F0376', 'F0158', 'F0019', 'F0019', 'F0546', 'F0446', 'F0005', 'F0599', 'F0659', 'F0688', 'F0748', 'F1137', 'F0983', 'F0938', 'F0869', 'F0838', 'F0982', 'F0865', 'F0816', 'F0878', 'F0885', 'F0833', 'F0815', 'F1196', 'F1122', 'F1292', 'F1253', 'F1313', 'F1393', 'F1665', 'F1215', 'F1392', 'F1640', 'F1172', 'F1606', 'F1433', 'F1431', 'F1609', 'F1660', 'F1623', 'F1792', 'F1846', 'F0912', 'F1830'] ###\n",
      "### 7 duplicated reagent SampleNumbers ['F0446', 'F0565', 'F0874', 'F0932', 'F0930', 'F1561', 'F1435'] ###\n",
      "### 56 duplicated HR SampleNumbers ['F0058', 'F0138', 'F0392', 'F0645', 'F0653', 'F0886', 'F0699', 'F0128', 'F0144', 'F0807', 'F0150', 'F0011', 'F0170', 'F1188', 'F0193', 'F0162', 'F0770', 'F0172', 'F0236', 'F0136', 'F0267', 'F0106', 'F0277', 'F1371', 'F0116', 'F0104', 'F0104', 'F0126', 'F0809', 'F0577', 'F1133', 'F0580', 'F1208', 'F0306', 'F0210', 'F1246', 'F0145', 'F0635', 'F0474', 'F0833', 'F0561', 'F1557', 'F0562', 'F0602', 'F0640', 'F0609', 'F1316', 'F1418', 'F1418', 'F0545', 'F0713', 'F0129', 'F1426', 'F1762', 'F0689', 'F1856'] ###\n",
      "Please fix duplicated values\n"
     ]
    }
   ],
   "source": [
    "# with open('foo_multi.pkl') as f:\n",
    "#     data = pickle.load(f)\n",
    "# dfs = data['boomtown']\n",
    "\n",
    "# Check for duplicates\n",
    "catalog_duplicates = dfs.catalog['SampleNumber'].duplicated()\n",
    "if catalog_duplicates.any():\n",
    "    catalog_duplicates = list(dfs.catalog.loc[catalog_duplicates, 'SampleNumber'].values)\n",
    "    print(\"### %d duplicated catalog SampleNumbers %s ###\" % (len(catalog_duplicates), catalog_duplicates))\n",
    "    dfs.catalog[dfs.catalog['SampleNumber'].duplicated(keep=False)].to_csv('catalog_duplicates.csv')\n",
    "else:\n",
    "    catalog_duplicates = None\n",
    "    \n",
    "ftir_duplicates = dfs.ftir['SampleNumber'].duplicated()\n",
    "if ftir_duplicates.any():\n",
    "    ftir_duplicates = list(dfs.ftir.loc[dfs.ftir['SampleNumber'].duplicated(), 'SampleNumber'].values)\n",
    "    print(\"### %d duplicated FTIR SampleNumbers %s ###\" % (len(ftir_duplicates), ftir_duplicates))\n",
    "    dfs.ftir[dfs.ftir['SampleNumber'].duplicated(keep=False)].to_csv('ftir_duplicates.csv')\n",
    "else:\n",
    "    ftir_duplicates = None\n",
    "\n",
    "reagent_duplicates = dfs.reagent['SampleNumber'].duplicated()\n",
    "if reagent_duplicates.any():\n",
    "    reagent_duplicates = list(dfs.reagent.loc[dfs.reagent['SampleNumber'].duplicated(), 'SampleNumber'].values)\n",
    "    print(\"### %d duplicated reagent SampleNumbers %s ###\" % (len(reagent_duplicates), reagent_duplicates))    \n",
    "    dfs.reagent[dfs.reagent['SampleNumber'].duplicated(keep=False)].to_csv('reagent_duplicates.csv', encoding = 'utf-8')\n",
    "else:\n",
    "    reagent_duplicates = None\n",
    "\n",
    "hr_duplicates = None\n",
    "if dfs.hr is not None:\n",
    "    hr_duplicates = dfs.hr['SampleNumber'].duplicated()\n",
    "    if hr_duplicates.any():\n",
    "        hr_duplicates = list(dfs.hr.loc[dfs.hr['SampleNumber'].duplicated(), 'SampleNumber'].values)\n",
    "        print(\"### %d duplicated HR SampleNumbers %s ###\" % (len(hr_duplicates), hr_duplicates))\n",
    "        dfs.hr[dfs.hr['SampleNumber'].duplicated(keep=False)].to_csv('hr_duplicates.csv', encoding = 'utf-8')\n",
    "    else:\n",
    "        hr_duplicates = None\n",
    "\n",
    "mla_duplicates = dfs.mla['SampleNumber'].duplicated()\n",
    "if mla_duplicates.any():\n",
    "    mla_duplicates = list(dfs.mla.loc[dfs.mla['SampleNumber'].duplicated(), 'SampleNumber'].values)\n",
    "    print(\"### %d duplicated MLA SampleNumbers %s ###\" % (len(mla_duplicates), mla_duplicates))\n",
    "    dfs.mla[dfs.mla['SampleNumber'].duplicated(keep=False)].to_csv('mla_duplicates.csv')\n",
    "else:\n",
    "    mla_duplicates = None\n",
    "    \n",
    "if catalog_duplicates or \\\n",
    "    ftir_duplicates or \\\n",
    "    reagent_duplicates or \\\n",
    "    hr_duplicates or \\\n",
    "    mla_duplicates:\n",
    "    outs = 'Please fix duplicated values'\n",
    "    print(outs)\n",
    "#     raise RuntimeError(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orphaned FTIR SampleNumbers: ['A1439', 'A1904', 'A1910', 'B0076', 'F00129', 'F0054', 'F0057', 'F0225', 'F0272', 'F0285', 'F0343', 'F0378', 'F0571', 'F0876', 'F0883', 'F0967', 'F0983', 'F1179', 'F1197', 'F1201', 'F1277', 'F1413', 'F1518', 'F1878', 'F1880', 'Z1000']\n",
      "Orphaned Reagent Test SampleNumbers: ['F0225', 'F0526', 'F0983', 'F1437', 'F1819 (or 1827?)', 'Yellow > green', 'Z1000']\n",
      "Orphaned HR SampleNumbers: [nan, 'F0054', 'F0154', 'F0225', 'F0272', 'F0285', 'F0378', 'F0612', 'F0616', 'F0876', 'F0883', 'F0967', 'F1076', 'F1179', 'F1201', 'F1880', 'F2901', 'F9271', 'F9999', 'G0024', 'G0037', 'G0153', 'G0242', 'G0420', 'G0652', 'G0811', 'G0875', 'G0877', 'G0878', 'G0981', 'G0983', 'G1216', 'G1228', 'G1229', 'G1284', 'G1312', 'G1398', 'G1441', 'G1572', 'G1686', 'G1699', 'G1703', 'G1833', 'G1859', 'G1860', 'G9999']\n",
      "Orphaned catalog SampleNumbers: ['F0516', 'F1012', 'F1104', 'F1168', 'F1243', 'F1488', 'F1910', 'R0876']\n",
      "Samples not in FTIR or Reagent: ['F0368', 'F0369', 'F0871', 'F0879', 'F1582', 'F1746', 'F1819']\n",
      "### Please fix orphaned/catalog only samples ###\n"
     ]
    }
   ],
   "source": [
    "# Check there are no SampleNumbers in any of the other spreadsheets that aren't in the cataolog sheet\n",
    "catalog_unique = set(dfs.catalog['SampleNumber'].unique())\n",
    "\n",
    "ftir_unique = set(dfs.ftir['SampleNumber'].unique())\n",
    "ftir_orphan = ftir_unique.difference(catalog_unique)\n",
    "if ftir_orphan:\n",
    "    print(\"Orphaned FTIR SampleNumbers: %s\" % sorted(ftir_orphan))\n",
    "\n",
    "reagent_unique = set(dfs.reagent['SampleNumber'].unique())\n",
    "reagent_orphan = reagent_unique.difference(catalog_unique)\n",
    "if reagent_orphan:\n",
    "    print(\"Orphaned Reagent Test SampleNumbers: %s\" % sorted(reagent_orphan))\n",
    "\n",
    "hr_orphan = None\n",
    "if dfs.hr is not None:\n",
    "    hr_unique = set(dfs.hr['SampleNumber'].unique())\n",
    "    hr_orphan = hr_unique.difference(catalog_unique)\n",
    "    if hr_orphan:\n",
    "        print(\"Orphaned HR SampleNumbers: %s\" % sorted(hr_orphan))\n",
    "    \n",
    "mla_unique = set(dfs.mla['SampleNumber'].unique()).difference(catalog_unique)\n",
    "mla_orphan = mla_unique.difference(catalog_unique)\n",
    "if mla_orphan:\n",
    "    print(\"Orphaned MLA SampleNumbers: %s\" % sorted(mla_orphan))\n",
    "    \n",
    "# Check for any that are only in the catalog\n",
    "outside_catalog = set.union(ftir_unique, reagent_unique, hr_unique, mla_unique)\n",
    "catalog_orphan = catalog_unique.difference(outside_catalog)\n",
    "if catalog_orphan:\n",
    "    print(\"Orphaned catalog SampleNumbers: %s\" % sorted(catalog_orphan))\n",
    "    \n",
    "# Check for any that aren't in FTIR and don't have anything in reagent test\n",
    "ftir_missing = catalog_unique.difference(ftir_unique).difference(reagent_unique).difference(catalog_orphan)\n",
    "if len(ftir_missing):\n",
    "    print(\"Samples not in FTIR or Reagent: %s\" % sorted(ftir_missing))\n",
    "\n",
    "all_unique = copy.copy(ftir_unique)\n",
    "all_unique.update(reagent_unique, hr_unique, mla_unique)\n",
    "if (all_unique or catalog_only):\n",
    "    outs = \"### Please fix orphaned/catalog only samples ###\"\n",
    "    print(outs)\n",
    "    #raise RuntimeError(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-465-e5f84cdabcf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mduplicate_matches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mduplicate_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_duplicate_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcatalog_duplicates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf1_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Catalog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf2_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'FTIR'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;31m#duplicate_matches = find_duplicate_matches(hr_duplicates, dfs.hr, dfs.catalog, df1_name='HR', df2_name='Catalog')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-465-e5f84cdabcf5>\u001b[0m in \u001b[0;36mfind_duplicate_matches\u001b[0;34m(duplicates, df1, df2, df1_name, df2_name)\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mdf1_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1_row\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SoldAs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AlreadyTried'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                     \u001b[0mdf1_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1_row\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSampleForm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0mdf1_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1_row\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimestamp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mdf2_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2_row\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SoldAs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AlreadyTried'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "def find_duplicate_matches(duplicates, df1, df2, df1_name='DataFrame1', df2_name='DataFrame2'):\n",
    "    hr = False\n",
    "    if df1_name.lower()[:2] == 'hr':\n",
    "        hr = True\n",
    "    duplicate_matches = {}\n",
    "    min_stage_delay = 60 * 1\n",
    "    max_stage_delay = 60 * 60\n",
    "    for sample_number in duplicates:\n",
    "        duplicate_matches[sample_number] = {}\n",
    "        for df1_idx, df1_row in df1.loc[df1['SampleNumber'] == sample_number].iterrows():\n",
    "            for df2_idx, df2_row in df2.loc[df2['SampleNumber'] == sample_number].iterrows():\n",
    "                df1_data = df1_row.loc[['SoldAs', 'AlreadyTried']].values\n",
    "                if not hr:\n",
    "                    df1_data.append(df1_row.SampleForm.values)\n",
    "                df1_time = df1_row.Timestamp\n",
    "                df2_data = df2_row.loc[['SoldAs', 'AlreadyTried']].values\n",
    "                if not hr:\n",
    "                    df2_data.append(df2_row.SampleForm.values)\n",
    "                df2_time = df2_row.Timestamp\n",
    "                delta_t = (df2_time - df1_time).seconds\n",
    "                if np.all(df1_data == df2_data) and min_stage_delay < delta_t <= max_stage_delay:\n",
    "                    print(\"%s SampleNumber %s duplicate (line: %d) MATCHES %s sample (line: %d)\" % \\\n",
    "                          (df1_name, sample_number, df1_idx + 1, df2_name, df2_idx + 1))\n",
    "                    duplicate_matches[sample_number][df1_idx] = True\n",
    "                else:\n",
    "                    print(\"%s SampleNumber %s duplicate (line: %d) DIFFERENT %s sample (line: %d)\\n%s %s\\n%s %s\" % \\\n",
    "                          (df1_name, sample_number, df1_idx + 1, df2_name, df2_idx + 1,\n",
    "                           df1_data, df1_time,\n",
    "                           df2_data, df2_time))\n",
    "                    duplicate_matches[sample_number][df1_idx] = False\n",
    "    return duplicate_matches\n",
    "\n",
    "duplicate_matches = find_duplicate_matches(catalog_duplicates, dfs.catalog, dfs.ftir, df1_name='Catalog', df2_name='FTIR')\n",
    "#duplicate_matches = find_duplicate_matches(hr_duplicates, dfs.hr, dfs.catalog, df1_name='HR', df2_name='Catalog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_orphans_to_duplicates(df1_orphans, duplicate_matches, df1, df2):\n",
    "    for orphan_sample_number in df1_orphans:\n",
    "        df1_data = df1.loc[df1['SampleNumber'] == orphan_sample_number, ['SampleForm', 'SoldAs', 'AlreadyTried', 'Timestamp']]\n",
    "        df1_data = df1_data.values.tolist()[0]\n",
    "        df1_time = df1_data.pop()\n",
    "        for sample_number, indexd in duplicate_matches.items():\n",
    "            for k, v in indexd.items():\n",
    "                if not v:\n",
    "                    df2_data = dfs.catalog.iloc[k][['SampleForm', 'SoldAs', 'AlreadyTried', 'Timestamp']].values.tolist()\n",
    "                    df2_time = df2_data.pop()\n",
    "                    delta_t = (df1_time - df2_time).seconds\n",
    "                    if df2_data == df1_data and min_stage_delay < delta_t <= max_stage_delay:\n",
    "                        print \"Orphan {} could be match for duplicate {} (line: {})\\n{} {}\\n{} {}\".format(orphan_sample_number,\n",
    "                                                                                                  sample_number, k+1,\n",
    "                                                                                                  df2_data, df2_time,\n",
    "                                                                                                  df1_data, df1_time)\n",
    "\n",
    "match_orphans_to_duplicates(ftir_orphan, duplicate_matches, dfs.ftir, dfs.catalog)\n",
    "# match_orphans_to_duplicates(catalog_orphan, duplicate_matches, dfs.catalog, dfs.ftir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check orphans against the FTIR sheet using just their numbers\n",
    "\n",
    "def match_orphans_with_sample_integer(orphans, orphan_df, ref_df):\n",
    "#     min_stage_delay = 60 * 1\n",
    "#     max_stage_delay = 60 * 60\n",
    "    def to_int(sn):\n",
    "        if type(sn) in [str, unicode]:\n",
    "            try:\n",
    "                sn = int(sn[1:])\n",
    "            except ValueError:\n",
    "                print \"Bad SampleNumber %s\" % sn\n",
    "        return sn\n",
    "    orphan_df['SampleInteger'] = orphan_df['SampleNumber'].apply(to_int)\n",
    "    ref_df['SampleInteger'] = ref_df['SampleNumber'].apply(to_int)\n",
    "    orphan_ints = map(to_int, orphans)\n",
    "    \n",
    "    for orphan_sample_number, oint in zip(orphans, orphan_ints):\n",
    "        for i, orphan_row in enumerate(orphan_df.loc[orphan_df['SampleNumber'] == orphan_sample_number].itertuples()):\n",
    "            for j, ref_row in enumerate(ref_df.loc[ref_df['SampleInteger'] == oint].itertuples()):\n",
    "                i += 1\n",
    "                j += 1\n",
    "                orphan_data = [orphan_row.SampleForm, orphan_row.SoldAs, orphan_row.AlreadyTried]\n",
    "                orphan_time = orphan_row.Timestamp\n",
    "                orphan_idx = orphan_row.Index + 1\n",
    "                ref_sample_number = ref_row.SampleNumber\n",
    "                ref_data = [ref_row.SampleForm, ref_row.SoldAs, ref_row.AlreadyTried]\n",
    "                ref_time = ref_row.Timestamp\n",
    "                ref_idx = ref_row.Index + 1            \n",
    "                delta_t = (ref_time - orphan_time).seconds\n",
    "#                 if orphan_data == ref_data and min_stage_delay < delta_t <= max_stage_delay:\n",
    "                if orphan_data == ref_data:\n",
    "                    print(\"FTIR orphan %s (line: %d) could be match for Catalog SampleNumber %s (line: %d)\\n%s %s\\n%s %s\" % \\\n",
    "                          (orphan_sample_number, orphan_idx, ref_sample_number, ref_idx, orphan_data, orphan_time, ref_data, ref_time))\n",
    "\n",
    "# match_orphans_with_sample_integer(catalog_orphan, dfs.catalog, dfs.ftir)\n",
    "match_orphans_with_sample_integer(hr_orphan, dfs.hr, dfs.ftir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean up catalog\n",
    "# Drop all unwanted columns\n",
    "\n",
    "#  or 'Your initials'\n",
    "l = set(['Your initials',\n",
    "         'Your name and first initial',\n",
    "         'Which device was a photo taken with? Who does it belong to?',\n",
    "         'Is a breakline present?',\n",
    "         'Unusual appearance'\n",
    "        ])\n",
    "\n",
    "to_drop = set(dfs.catalog.columns).intersection(l)\n",
    "dfs.catalog.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "d = {\n",
    "    'Timestamp' : 'Catalog timestamp',\n",
    "    'Sample Advertised/Acquired/Sold As': 'Catalog_SoldAs',\n",
    "    'Sample Form' : 'Catalog_Form',\n",
    "    'Has the Service User or a close friend tried this batch?': 'Catalog_Tried',\n",
    "    'What is the mass? (mg)': 'FullPillMass',\n",
    "    'What is the shape of the pill?': 'PillShape',\n",
    "    'What is the logo?': 'PillLogo',\n",
    "    'What colour is the pill?': 'PillColour'\n",
    "}\n",
    "dfs.catalog.rename(columns=d, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('COLS ', Index([                                                           u'Timestamp',\n",
      "                                                              u'Sample Number',\n",
      "                                                                     u'Tester',\n",
      "                                                                    u'Sold As',\n",
      "                                                                u'Sample Form',\n",
      "                                                              u'Already Tried',\n",
      "                                                             u'User Suspicion',\n",
      "                                                         u'Substance detected',\n",
      "                                                             u'Hit Confidence',\n",
      "                                                          u'Compound detected',\n",
      "                                                           u'Hit Confidence.1',\n",
      "                                                                 u'Brief Note',\n",
      "                           u'Is anything detected after subtraction analysis?',\n",
      "                                            u'Compound detected (Subtraction)',\n",
      "                                                           u'Hit Confidence.2',\n",
      "                                                       u'Substance detected.1',\n",
      "                                                           u'Hit Confidence.3',\n",
      "                                                               u'Brief Note.1',\n",
      "                                                             u'Next action(s)',\n",
      "                                                      u'Substance(s) detected',\n",
      "                                           u'\"Strength\" of powdered substance',\n",
      "       u'Does the substance detected match the substance that was advertised?',\n",
      "                                             u'Note for harm reduction worker',\n",
      "                                                            u'Send to HR team'],\n",
      "      dtype='object'))\n",
      "('SS ', 1691    N-Ethylpentylone\n",
      "1692    N-Ethylpentylone\n",
      "1694    N-Ethylpentylone\n",
      "1696             Cocaine\n",
      "0                   MDMA\n",
      "Name: Substance detected, dtype: object)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_ftir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f76c2c2a13ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Compound detected (Subtraction)'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'Other'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Compound detected (Subtraction)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_ftir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Substance detected.1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Copy values from 'Compound detected'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hit Confidence.2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hit Confidence.3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Substance detected.1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Hit Confidence.3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Brief Note.1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_ftir' is not defined"
     ]
    }
   ],
   "source": [
    "# For FTIR columns need to merge the data from the 'Compound detected', 'Hit Confidence.1' columns into the\n",
    "# 'Substance detected', 'Hit Confidence' column where the substance detected was 'other'\n",
    "print(\"COLS \",dfs.ftir.columns)\n",
    "print(\"SS \",dfs.ftir['Substance detected'][:5])\n",
    "mask = dfs.ftir['Substance detected'] != 'Other'\n",
    "dfs.ftir['Substance detected'].where(mask, dfs.ftir['Compound detected'], inplace=True) # Copy values from 'Compound detected'\n",
    "dfs.ftir['Hit Confidence'].where(mask, dfs.ftir['Hit Confidence.1'], inplace=True)\n",
    "dfs.ftir.drop(['Compound detected', 'Hit Confidence.1', 'Brief Note'], axis=1, inplace=True)\n",
    "\n",
    "mask = dfs.ftir['Compound detected (Subtraction)'] != 'Other'\n",
    "dfs.ftir['Compound detected (Subtraction)'].where(mask, df_ftir['Substance detected.1'], inplace=True) # Copy values from 'Compound detected'\n",
    "dfs.ftir['Hit Confidence.2'].where(mask, dfs.ftir['Hit Confidence.3'], inplace=True)\n",
    "dfs.ftir.drop(['Substance detected.1', 'Hit Confidence.3', 'Brief Note.1'], axis=1, inplace=True)\n",
    "\n",
    "#Â Drop all unwanted columns\n",
    "l = ['Your name and surname initial',\n",
    "     'User Suspicion',\n",
    "     'Is anything detected after subtraction analysis?',\n",
    "     'Analysis required', \n",
    "     'Next action(s)',\n",
    "     'Send to HR team'\n",
    "    ]\n",
    "#'Note for harm reduction worker'\n",
    "to_drop = set(dfs.ftir.columns).intersection(l)\n",
    "dfs.ftir.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Rename shared columns so that we can check for any errors and remove any columns not of interest to the master df\n",
    "d = {\n",
    "    'Timestamp' : 'FTIR timestamp',\n",
    "    'Sample Sold As': 'FTIR Sold As',\n",
    "    'Sample Form' : 'FTIR form',\n",
    "    'Has the Service User or a close friend tried this batch?': 'FTIR tried',\n",
    "    'Substance(s) detected' : 'FTIR final result',\n",
    "    'Substance detected' : 'FTIR result1',\n",
    "    'Hit Confidence' :  'FTIR hit1',\n",
    "    'Is anything detected after subtraction analysis?' : 'FTIR subtraction positive',\n",
    "    'Compound detected (Subtraction)' :  'FTIR result2',\n",
    "    'Hit Confidence.2' :  'FTIR hit2',\n",
    "    '\"Strength\" of powdered substance' : 'FTIR Powder Strength',\n",
    "    'Does the substance detected match the substance that was advertised?' : 'FTIR Matches Sold As',\n",
    "}\n",
    "dfs.ftir.rename(columns=d, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean up HR form\n",
    "\n",
    "# Drop all unwanted columns\n",
    "l = ['HR worker name:']\n",
    "dfs.hr.drop(l, axis=1, inplace=True)\n",
    "\n",
    "# Rename shared columns so that we can check for any errors and remove any columns not of interest to the master df\n",
    "d = {\n",
    "    'Timestamp' : 'HR timestamp',\n",
    "    'You submitted a substance for analysis. What were you told it was when you got it?': 'HR Sold as',\n",
    "    'Had you already tried this substance before getting it tested?': 'HR tried',\n",
    "    'What was your first sample number at this event? Did you take a photo or keep the ticket?': 'Previous Sample Number'\n",
    "}\n",
    "dfs.hr.rename(columns=d, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Catalog and FTIR data frames\n",
    "df_all = pd.merge(dfs.catalog, dfs.ftir, how='left', on=['Sample Number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge in any reagent test data\n",
    "df_all = pd.merge(df_all, dfs.reagent[['Sample Number', 'Reagent Result']], how='left', on=['Sample Number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge in any pill strength data\n",
    "df_all = pd.merge(df_all, dfs.mla[['Sample Number', 'MDMA / tablet (mg)', '% MDMA content']], how='left', on=['Sample Number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge in HR data\n",
    "df_all = pd.merge(df_all, dfs.hr, how='left', on=['Sample Number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fix column orders\n",
    "prefix = ['Sample Number',\n",
    "          'Catalog timestamp', 'FTIR timestamp', 'HR timestamp',\n",
    "          'Catalog Sold As', 'FTIR Sold As','HR Sold as', \n",
    "          'Catalog form', 'FTIR form',\n",
    "          'Catalog tried', 'FTIR tried', 'HR tried']\n",
    "columns = [c for c in df_all.columns if c not in prefix]\n",
    "columns = prefix + columns\n",
    "df_all = df_all[columns]\n",
    "df_all.to_csv('foo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
