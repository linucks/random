{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Module imports\n",
    "import copy\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def fix_sample_number(x):\n",
    "    \"\"\"Make sure all samples numbers are of form: AXXX (where A is one of A, F, W and X is a digit)\"\"\"\n",
    "    if isinstance(x, float) and np.isnan(x):\n",
    "        return x # leave NaN's alone\n",
    "    if (isinstance(x, str) or isinstance(x, unicode)) and len(x) == 0:\n",
    "        return np.nan\n",
    "    try:\n",
    "        sn = 'F{:04d}'.format(int(x))\n",
    "    except ValueError:\n",
    "        # Assume string so make sure it's of the right format\n",
    "        sn = str(x).strip().upper()\n",
    "    if len(sn) != 5 or sn[0] not in ['A', 'F', 'W', 'B']:\n",
    "        if sn[:2] != 'DF': # Duplicate labels\n",
    "            print(\"!!! Bad ID \\'%s\\'\" % sn)\n",
    "    return sn\n",
    "\n",
    "def now():\n",
    "    return datetime.datetime.now().strftime(\"%d/%m/%y %H:%M:%S\")\n",
    "\n",
    "def enumerate_duplicates(row):\n",
    "    \"\"\"Append a counter to duplicate labels\"\"\"\n",
    "    SEPARATOR = '.'\n",
    "    duplicates = {}\n",
    "    updated_row = []\n",
    "    for r in row:\n",
    "        count = duplicates.get(r, 0)\n",
    "        if count > 0:\n",
    "            label = \"{}{}{}\".format(r, SEPARATOR, count)\n",
    "        else:\n",
    "            label = r\n",
    "        updated_row.append(label)\n",
    "        duplicates[r] = count + 1\n",
    "    return updated_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = 'raise'\n",
    "\n",
    "# Need to define in main or we can't pickle the data objects\n",
    "class DataFrames(object):\n",
    "    def __init__(self):\n",
    "        catalog = None\n",
    "        ftir = None\n",
    "        reagent = None\n",
    "        mla = None\n",
    "        hr = None\n",
    "\n",
    "def gsheets_service():\n",
    "    from googleapiclient.discovery import build\n",
    "    from httplib2 import Http\n",
    "    from oauth2client import file, client, tools\n",
    "    # If modifying these scopes, delete the file token.json.\n",
    "    #Ensure that the creds file is always taken from the current working folder\n",
    "        #This allows two people on different PCs to merge changes more easily.\n",
    "    CREDS_FILE = os.path.join(os.path.realpath('./'),'JensDataExportJupyter_client_secret.json')\n",
    "    SCOPES = 'https://www.googleapis.com/auth/spreadsheets.readonly'\n",
    "    store = file.Storage('token.json')\n",
    "    creds = store.get()\n",
    "    if not creds or creds.invalid:\n",
    "        import argparse\n",
    "        flags = argparse.ArgumentParser(parents=[tools.argparser]).parse_args([])\n",
    "        flow = client.flow_from_clientsecrets(CREDS_FILE, SCOPES)\n",
    "        creds = tools.run_flow(flow, store, flags)\n",
    "    service = build('sheets', 'v4', http=creds.authorize(Http()))\n",
    "    return service\n",
    "\n",
    "def get_df(service, SPREADSHEET_ID, SS_RANGE, mla=False):\n",
    "    # Call the Sheets API\n",
    "    result = service.spreadsheets().values().get(spreadsheetId=SPREADSHEET_ID,\n",
    "                                                range=SS_RANGE).execute()\n",
    "    values = result.get('values', [])\n",
    "    if not values:\n",
    "        print('*** No data found ***')\n",
    "        return None\n",
    "\n",
    "    # mla has irrelevant stuff in columns 1 and 3 and sample numbers in first column\n",
    "    if mla:\n",
    "        values.pop(0)\n",
    "        values.pop(1)\n",
    "        def not_blank(row):\n",
    "            return len(row[0]) > 0       \n",
    "    else:\n",
    "        def not_blank(row):\n",
    "            return sum(map(len, row[:6])) > 0\n",
    "\n",
    "    rows = filter(not_blank, values)\n",
    "    if not rows:\n",
    "        print('*** No data found after pruning rows! ***')\n",
    "        return None\n",
    "    \n",
    "    columns = enumerate_duplicates(rows[0])\n",
    "    ncols = len(rows[0])\n",
    "    row_max = max(map(len, rows[1:]))\n",
    "    width = min(ncols, row_max)\n",
    "    return pd.DataFrame(rows[1:], columns=columns[:width])\n",
    "\n",
    "def canonicalise_df(df, source=None):\n",
    "    \"\"\"Initial cleaning of all dataframes\"\"\"\n",
    "    #from pandas._libs.tslib import OutOfBoundsDatetime\n",
    "    if source:\n",
    "        print(\"Canonicalising %s\" % source)\n",
    "    # Standardise names\n",
    "    d = {\n",
    "        'Sample Code':'SampleNumber',\n",
    "        'Sample Number:':'SampleNumber',\n",
    "        'Sample Number':'SampleNumber',\n",
    "        'Sample number':'SampleNumber',\n",
    "        'Sample Num':'SampleNumber',\n",
    "        'Sample Number i.e F0XXX' : 'SampleNumber',\n",
    "        \n",
    "        'Sample Advertised/Acquired/Sold As' : 'SoldAs',\n",
    "        'Sample Sold As' : 'SoldAs',\n",
    "        'You submitted a substance for analysis. What were you told it was when you got it?':  'SoldAs',\n",
    "        \n",
    "        \n",
    "        'Sample Source' :'SampleSource',\n",
    "\n",
    "        'User Suspicion' :'UserSuspicion',\n",
    "\n",
    "        'Sample Form' :'SampleForm',\n",
    "\n",
    "        'Has the Service User or a close friend tried this batch?' : 'AlreadyTried',\n",
    "        'Had you already tried this substance before getting it tested?' : 'AlreadyTried',\n",
    "\n",
    "        'Your initials' : 'Tester',\n",
    "        'Your name and first initial' : 'Tester',\n",
    "        'Your name and surname initial' : 'Tester'\n",
    "    }\n",
    "    df.rename(columns=d, inplace=True)\n",
    "    \n",
    "    def fix_timestamp(x):\n",
    "        return pd.to_datetime(str(x), format='%d/%m/%Y %H:%M:%S')\n",
    "    if 'Timestamp' in df.columns:\n",
    "        df.loc[:, 'Timestamp'] = df['Timestamp'].map(fix_timestamp)\n",
    "    df.loc[:, 'SampleNumber'] = df['SampleNumber'].apply(fix_sample_number)\n",
    "    df.dropna(subset=['SampleNumber'])\n",
    "    #df.sort_values(['Sample Number'], ascending=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "def get_data(service, SPREADSHEET_ID):\n",
    "\n",
    "    CATALOG_RANGE = 'Catalog!A:R'\n",
    "    FTIR_RANGE = 'FTIR!A:X'\n",
    "    REAGENT_RANGE = 'Reagent!A:W'\n",
    "    MLA_RANGE = 'MLA!A:R'\n",
    "    HR_RANGE = 'Interventions!A:BJ'\n",
    "\n",
    "    df_catalog = get_df(service, SPREADSHEET_ID, CATALOG_RANGE)\n",
    "    df_catalog = canonicalise_df(df_catalog, source='catalog')\n",
    "    df_ftir = get_df(service, SPREADSHEET_ID, FTIR_RANGE)\n",
    "    df_ftir = canonicalise_df(df_ftir, source='ftir')\n",
    "    df_reagent = get_df(service, SPREADSHEET_ID, REAGENT_RANGE)\n",
    "    df_reagent = canonicalise_df(df_reagent, source='reagent')\n",
    "    df_mla = get_df(service, SPREADSHEET_ID, MLA_RANGE, mla=True)\n",
    "    df_mla = canonicalise_df(df_mla, source='mla')\n",
    "    try:\n",
    "        df_hr = get_df(service, SPREADSHEET_ID, HR_RANGE)\n",
    "    except ValueError:\n",
    "        df_hr = None\n",
    "    if df_hr is not None:\n",
    "        df_hr = canonicalise_df(df_hr, source='hr')\n",
    "\n",
    "    df = DataFrames()\n",
    "    df.catalog = df_catalog\n",
    "    df.ftir = df_ftir\n",
    "    df.reagent = df_reagent\n",
    "    df.mla = df_mla\n",
    "    df.hr = df_hr\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script running from: /opt/random\n",
      "PROCESSING BOOMTOWN\n",
      "Canonicalising catalog\n",
      "!!! Bad ID 'TF0579'\n",
      "!!! Bad ID 'TF1665'\n",
      "!!! Bad ID 'TF1660'\n",
      "Canonicalising ftir\n",
      "!!! Bad ID 'TF1665'\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "Canonicalising hr\n",
      "!!! Bad ID 'TF0653'\n",
      "!!! Bad ID 'TF1172'\n",
      "!!! Bad ID 'TF1762'\n",
      "PROCESSING BOARDMASTERS\n",
      "Canonicalising catalog\n",
      "Canonicalising ftir\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "Canonicalising hr\n",
      "PROCESSING MADE\n",
      "Canonicalising catalog\n",
      "Canonicalising ftir\n",
      "!!! Bad ID 'XF0005'\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "Canonicalising hr\n",
      "PROCESSING SW4\n",
      "Canonicalising catalog\n",
      "Canonicalising ftir\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "PROCESSING LOST VILLAGE\n",
      "Canonicalising catalog\n",
      "!!! Bad ID 'DB0136'\n",
      "!!! Bad ID 'DB0355'\n",
      "!!! Bad ID 'DB0354'\n",
      "!!! Bad ID 'DB0001'\n",
      "!!! Bad ID 'DB0001'\n",
      "!!! Bad ID 'DB0053'\n",
      "!!! Bad ID 'DB0053'\n",
      "Canonicalising ftir\n",
      "!!! Bad ID 'DB0001'\n",
      "!!! Bad ID 'DB0004'\n",
      "!!! Bad ID 'DB0006'\n",
      "!!! Bad ID 'DB0010'\n",
      "!!! Bad ID 'DB0032'\n",
      "!!! Bad ID 'DB0041'\n",
      "!!! Bad ID 'DB0082'\n",
      "!!! Bad ID 'DB0115'\n",
      "!!! Bad ID 'DB0210'\n",
      "!!! Bad ID 'DB0219'\n",
      "!!! Bad ID 'DB0224'\n",
      "!!! Bad ID 'DB0227'\n",
      "!!! Bad ID 'DB0247'\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "!!! Bad ID 'DB0202'\n",
      "!!! Bad ID 'DB0202'\n",
      "!!! Bad ID 'DB0202'\n",
      "!!! Bad ID 'DB0202'\n",
      "!!! Bad ID 'DB0203'\n",
      "!!! Bad ID 'DB0068'\n",
      "!!! Bad ID 'DB0068'\n",
      "!!! Bad ID 'DB0068'\n",
      "!!! Bad ID 'DB0068'\n",
      "!!! Bad ID 'DB0068'\n",
      "!!! Bad ID 'DB0222'\n",
      "!!! Bad ID 'DB0328'\n",
      "!!! Bad ID 'DB0328'\n",
      "PROCESSING BESTIVAL\n",
      "Canonicalising catalog\n",
      "Canonicalising ftir\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "Canonicalising hr\n",
      "!!! Bad ID 'P1000'\n",
      "!!! Bad ID 'F20005'\n",
      "!!! Bad ID 'G9998'\n",
      "PROCESSING YNOT\n",
      "Canonicalising catalog\n",
      "!!! Bad ID 'DA0442'\n",
      "!!! Bad ID 'DA0443'\n",
      "!!! Bad ID 'DA0477'\n",
      "!!! Bad ID 'DA0573'\n",
      "Canonicalising ftir\n",
      "!!! Bad ID 'DA0433'\n",
      "!!! Bad ID 'DA0444'\n",
      "!!! Bad ID 'DA0468'\n",
      "!!! Bad ID 'DA0488'\n",
      "!!! Bad ID 'DA0553'\n",
      "!!! Bad ID 'DA0533'\n",
      "!!! Bad ID 'DA0553'\n",
      "!!! Bad ID 'DA0589'\n",
      "!!! Bad ID 'DA0585'\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "!!! Bad ID 'DA0371'\n",
      "!!! Bad ID 'DA0521'\n",
      "!!! Bad ID 'DA0521'\n",
      "!!! Bad ID 'DA0433'\n",
      "Canonicalising hr\n",
      "PROCESSING TRUCKFEST\n",
      "Canonicalising catalog\n",
      "Canonicalising ftir\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "PROCESSING LSTD\n",
      "Canonicalising catalog\n",
      "!!! Bad ID 'M0120'\n",
      "!!! Bad ID 'M0141'\n",
      "!!! Bad ID 'M0204'\n",
      "!!! Bad ID 'S0186'\n",
      "!!! Bad ID 'S0202'\n",
      "Canonicalising ftir\n",
      "!!! Bad ID 'S0050'\n",
      "!!! Bad ID 'M0120'\n",
      "!!! Bad ID 'S0186'\n",
      "!!! Bad ID 'M0204'\n",
      "Canonicalising reagent\n",
      "!!! Bad ID 'S0050'\n",
      "Canonicalising mla\n",
      "!!! Bad ID 'M0120'\n",
      "Canonicalising hr\n",
      "PROCESSING KENDAL CALLING\n",
      "Canonicalising catalog\n",
      "!!! Bad ID 'M0011'\n",
      "Canonicalising ftir\n",
      "!!! Bad ID 'DB0046'\n",
      "Canonicalising reagent\n",
      "!!! Bad ID 'DB0046'\n",
      "Canonicalising mla\n",
      "Canonicalising hr\n",
      "PROCESSING PARKLIFE\n",
      "Canonicalising catalog\n",
      "!!! Bad ID 'DA0348'\n",
      "!!! Bad ID 'DA0372'\n",
      "!!! Bad ID 'DA0424'\n",
      "!!! Bad ID 'DA0494'\n",
      "!!! Bad ID 'DA1468'\n",
      "!!! Bad ID 'DA2039'\n",
      "!!! Bad ID 'DA2070'\n",
      "!!! Bad ID 'DA2122'\n",
      "!!! Bad ID 'M2248'\n",
      "Canonicalising ftir\n",
      "!!! Bad ID 'DA0290'\n",
      "!!! Bad ID 'DA0309'\n",
      "!!! Bad ID 'DA0329'\n",
      "!!! Bad ID 'DA0366'\n",
      "!!! Bad ID 'DA0268'\n",
      "!!! Bad ID 'DA0279'\n",
      "!!! Bad ID 'DA0446'\n",
      "!!! Bad ID 'DA0349'\n",
      "!!! Bad ID 'DA1349'\n",
      "!!! Bad ID 'DA0492'\n",
      "!!! Bad ID 'DA1344'\n",
      "!!! Bad ID 'DA1364'\n",
      "!!! Bad ID 'DA0426'\n",
      "!!! Bad ID 'DA1324'\n",
      "!!! Bad ID 'DW1369'\n",
      "!!! Bad ID 'DA2047'\n",
      "!!! Bad ID 'DA2010'\n",
      "!!! Bad ID 'DA2041'\n",
      "!!! Bad ID 'DA1448'\n",
      "!!! Bad ID 'DA2066'\n",
      "!!! Bad ID 'DA2054'\n",
      "!!! Bad ID 'DA2015'\n",
      "!!! Bad ID 'DA2120'\n",
      "!!! Bad ID 'DA2022'\n",
      "!!! Bad ID 'M2248'\n",
      "!!! Bad ID 'DA0500'\n",
      "!!! Bad ID 'DA0424'\n",
      "!!! Bad ID 'DA1331'\n",
      "!!! Bad ID 'DA2015'\n",
      "Canonicalising reagent\n",
      "!!! Bad ID 'NOT A1451'\n",
      "Canonicalising mla\n",
      "!!! Bad ID 'DA0277'\n",
      "!!! Bad ID 'DA1489'\n",
      "!!! Bad ID 'DA1489'\n",
      "!!! Bad ID 'DA1492'\n",
      "!!! Bad ID 'DA2052'\n",
      "!!! Bad ID 'DA2052'\n",
      "!!! Bad ID 'DA2055'\n",
      "!!! Bad ID 'DA2055'\n"
     ]
    }
   ],
   "source": [
    "#S how the folder where the code file is being run from        \n",
    "print(\"Script running from: %s\" % os.path.realpath(os.getcwd()))\n",
    "\n",
    "# The ID and range of a sample spreadsheet.\n",
    "BOOMTOWN2018_SPREADSHEET_ID = '1RiA-FwG_954Ger2VPsOSA3JLh-7sEoTYr40eVS0mp24'\n",
    "MADE2018_SPREADSHEET_ID = '1daXdyL6uL8qnMsEsP0RLZE9nDzt6J7Zr1ygQdguvi-E'\n",
    "BOARDMASTERS2018_SPREADSHEET_ID = '1U1lhUWLazDBN-wb2eZM8YV674f46npVfQK3XUVZjPow'\n",
    "SW42018_SPREADSHEET_ID = '1agpMmJ9XukeWXS5_mwrDSKeshUaFtYwOzsPiR1DKsPU'\n",
    "LOSTVILLAGE2018_SPREADSHEET_ID = '1OL0gyXrpZnJ8e7yR7eF6S2OaBYBiPDoVp5xGpdK4wlA'\n",
    "BESTIVAL2018_SPREADSHEET_ID = '184qudGcw4PB0SMtOo0ZBDtckeGaH0RCLUXbA-u3BiHE'\n",
    "YNOT2018_SPREADSHEET_ID = '1D01cj-Mra06TuoG_MsKuLq9OdtvKzrvRdiE255po_ag'\n",
    "TRUCKFEST2018_SPREADSHEET_ID = '1sGG9WJxKyD2CGUjzJAXul3g9hVnRz6HbTiqKV5cUAyA'\n",
    "LSTD2018_SPREADSHEET_ID = '1R8YqDnrhvuVMwPFShwaaAUIyCXQMeozA230OXsFsDQM'\n",
    "KENDALCALLING2018_SPREADSHEET_ID = '16-PfwBOaUxwod3X75LGk1VAjBblkNsTJpCsX825aghI'\n",
    "PARKLIFE2018_SPREADSHEET_ID = '1oO5sHcUhUn_7M1Hap73sOZHNEfWFMcDkQuWDRFf4d-w'\n",
    "\n",
    "\n",
    "data = {}\n",
    "service = gsheets_service()\n",
    "print(\"PROCESSING BOOMTOWN\")\n",
    "data['boomtown'] = get_data(service, BOOMTOWN2018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING BOARDMASTERS\")\n",
    "data['boardmasters'] = get_data(service, BOARDMASTERS2018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING MADE\")\n",
    "data['made'] = get_data(service, MADE2018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING SW4\")\n",
    "data['sw4'] = get_data(service, SW42018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING LOST VILLAGE\")\n",
    "data['lostvillage'] = get_data(service, LOSTVILLAGE2018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING BESTIVAL\")\n",
    "data['bestival'] = get_data(service, BESTIVAL2018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING YNOT\")\n",
    "data['ynot'] = get_data(service, YNOT2018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING TRUCKFEST\")\n",
    "data['truckfest'] = get_data(service, TRUCKFEST2018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING LSTD\")\n",
    "data['lstd'] = get_data(service, LSTD2018_SPREADSHEET_ID)\n",
    "print( \"PROCESSING KENDAL CALLING\")\n",
    "data['kc'] = get_data(service, KENDALCALLING2018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING PARKLIFE\")\n",
    "data['parklife'] = get_data(service, PARKLIFE2018_SPREADSHEET_ID)\n",
    "\n",
    "import pickle\n",
    "with open('foo_multi.pkl','w') as w:\n",
    "    pickle.dump(data, w)\n",
    "# dfs = data['boomtown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 3 duplicated catalog SampleNumbers ['B0174', 'B0255', 'B0298'] ###\n",
      "### 4 duplicated ftir SampleNumbers ['B0103', 'B0131', 'B0221', 'B0229'] ###\n",
      "### 2 duplicated reagent SampleNumbers ['B0135', 'B0235'] ###\n",
      "### 28 duplicated mla SampleNumbers ['B0130', 'B0131', 'B0133', 'B0128', 'B0128', 'B0128', 'B0128', 'B0128', 'B0128', 'B0128', 'B0128', 'B0150', 'B0150', 'B0150', 'B0150', 'B0150', 'B0115', 'B0172', 'B0172', 'B0200', 'B0194', 'B0206', 'B0206', 'B0206', 'B0206', 'B0206', 'B0206', 'B0258'] ###\n"
     ]
    }
   ],
   "source": [
    "# with open('foo_multi.pkl') as f:\n",
    "#     data = pickle.load(f)\n",
    "# dfs = data['boomtown']\n",
    "\n",
    "# Need to define in main or we can't pickle the data objects\n",
    "class Duplicates(object):\n",
    "    def __init__(self, dfs):\n",
    "        self.dfs = dfs\n",
    "        dtypes = ['catalog', 'ftir', 'reagent', 'mla', 'hr']\n",
    "        for t in dtypes:\n",
    "            setattr(self, t, None)\n",
    "        for t in dtypes:\n",
    "            self.find_duplicates(t)\n",
    "        \n",
    "    def find_duplicates(self, dtype):\n",
    "        dataframe = getattr(self.dfs, dtype)\n",
    "        if dataframe is None:\n",
    "            return\n",
    "        duplicates = dataframe['SampleNumber'].duplicated()\n",
    "        if duplicates.any():\n",
    "            duplicates = list(dataframe.loc[duplicates, 'SampleNumber'].values)\n",
    "            print(\"### %d duplicated %s SampleNumbers %s ###\" % (len(duplicates), dtype, duplicates))\n",
    "#             dataframe[datafra,e['SampleNumber'].duplicated(keep=False)].to_csv('{}_duplicates.csv'.format(dtype))\n",
    "        else:\n",
    "            duplicates = None\n",
    "        setattr(self, dtype, duplicates)\n",
    "        \n",
    "    def has_hr_duplicates(self):\n",
    "        if self.hr:\n",
    "            outs = 'Please fix HR duplicates'\n",
    "            raise RuntimeError(outs)\n",
    "\n",
    "for festival, dfs in data.items():\n",
    "    print(\"CHECKING \",festival)\n",
    "    duplicates = Duplicates(dfs)\n",
    "    duplicates.has_hr_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orphaned FTIR SampleNumbers: ['A0297', 'A0351', 'A0500', 'A0545', 'A1272', 'A1289', 'A1320', 'A1339', 'A1340', 'A1368', 'A1467', 'A2011', 'A2015', 'A2038', 'A2049', 'A2245', 'A2250', 'A2255', 'A2259', 'A2260', 'A2266', 'A2267', 'DA0268', 'DA0279', 'DA0290', 'DA0309', 'DA0329', 'DA0349', 'DA0366', 'DA0426', 'DA0446', 'DA0492', 'DA0500', 'DA1324', 'DA1331', 'DA1344', 'DA1349', 'DA1364', 'DA1448', 'DA2010', 'DA2015', 'DA2022', 'DA2041', 'DA2047', 'DA2054', 'DA2066', 'DA2120', 'DW1369']\n",
      "Orphaned Reagent Test SampleNumbers: ['A1467', 'NOT A1451']\n",
      "Orphaned MLA SampleNumbers: ['A0500', 'A2049']\n",
      "Orphaned catalog SampleNumbers: ['A0274', 'A0299', 'A0364', 'A0373', 'A0384', 'A0403', 'A0411', 'A0432', 'A0459', 'A0462', 'A0489', 'A0490', 'A1256', 'A1268', 'A1269', 'A1270', 'A1273', 'A1276', 'A1282', 'A1326', 'A1327', 'A1345', 'A1346', 'A1356', 'A1375', 'A2035', 'A2054', 'A2113', 'A2124', 'A2128', 'A2130', 'A2133', 'A2148', 'A2149', 'A2154', 'A2159', 'A2160', 'A2161', 'A2163', 'A2165', 'A2174', 'A2178', 'A2182', 'A2186', 'A2187', 'A2189', 'A2191', 'A2193', 'A2194', 'A2196', 'A2197', 'A2198', 'A2200', 'A2201', 'A2203', 'A2206', 'A2208', 'A2213', 'A2214', 'A2218', 'A2223', 'A2224', 'A2225', 'A2227', 'A2228', 'A2230', 'A2232', 'A2236', 'A2239', 'A2242', 'A2243', 'A2244', 'A2258', 'DA0348', 'DA0372', 'DA0494', 'DA1468', 'DA2039', 'DA2070', 'DA2122']\n",
      "### Please fix orphaned/catalog only samples ###\n"
     ]
    }
   ],
   "source": [
    "# Check there are no SampleNumbers in any of the other spreadsheets that aren't in the cataolog sheet\n",
    "catalog_unique = set(dfs.catalog['SampleNumber'].unique())\n",
    "\n",
    "ftir_unique = set(dfs.ftir['SampleNumber'].unique())\n",
    "ftir_orphan = ftir_unique.difference(catalog_unique)\n",
    "if ftir_orphan:\n",
    "    print(\"Orphaned FTIR SampleNumbers: %s\" % sorted(ftir_orphan))\n",
    "\n",
    "reagent_unique = set(dfs.reagent['SampleNumber'].unique())\n",
    "reagent_orphan = reagent_unique.difference(catalog_unique)\n",
    "if reagent_orphan:\n",
    "    print(\"Orphaned Reagent Test SampleNumbers: %s\" % sorted(reagent_orphan))\n",
    "\n",
    "hr_orphan = None\n",
    "if dfs.hr is not None:\n",
    "    hr_unique = set(dfs.hr['SampleNumber'].unique())\n",
    "    # HR need to be both in catalog and ftir\n",
    "    hr_orphan = hr_unique.difference(ftir_unique.union(catalog_unique))\n",
    "    if hr_orphan:\n",
    "        print(\"Orphaned HR SampleNumbers: %s\" % sorted(hr_orphan))\n",
    "    \n",
    "mla_unique = set(dfs.mla['SampleNumber'].unique()).difference(catalog_unique)\n",
    "mla_orphan = mla_unique.difference(catalog_unique)\n",
    "if mla_orphan:\n",
    "    print(\"Orphaned MLA SampleNumbers: %s\" % sorted(mla_orphan))\n",
    "    \n",
    "# Check for any that are only in the catalog\n",
    "outside_catalog = set.union(ftir_unique, reagent_unique, hr_unique, mla_unique)\n",
    "catalog_orphan = catalog_unique.difference(outside_catalog)\n",
    "if catalog_orphan:\n",
    "    print(\"Orphaned catalog SampleNumbers: %s\" % sorted(catalog_orphan))\n",
    "    \n",
    "# Check for any that aren't in FTIR and don't have anything in reagent test\n",
    "ftir_missing = catalog_unique.difference(ftir_unique).difference(reagent_unique).difference(catalog_orphan)\n",
    "if len(ftir_missing):\n",
    "    print(\"Samples not in FTIR or Reagent: %s\" % sorted(ftir_missing))\n",
    "\n",
    "all_unique = copy.copy(ftir_unique)\n",
    "all_unique.update(reagent_unique, hr_unique, mla_unique)\n",
    "if (all_unique or catalog_only):\n",
    "    outs = \"### Please fix orphaned/catalog only samples ###\"\n",
    "    print(outs)\n",
    "    #raise RuntimeError(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell cleans the \"sample form\" field \n",
    "def clean_df(df):\n",
    "    sample_form_d = { 'pill' : ['Ecstasy Tablet',\n",
    "                                'ecstasy pill',\n",
    "                                'ecstacy pill',\n",
    "                                'Non-pharmaceutical tablet (ecstasy etc)',\n",
    "                                'other recreational pill',\n",
    "                                 'Whole pill',\n",
    "                                'Other pill',\n",
    "                                'Pharmaceutical'],\n",
    "                      'partial pill' : ['Partial ecstasy pill',\n",
    "                                        'Partial 2C-B pill',\n",
    "                                        'Crushed tablet'],\n",
    "                      'powder' : ['powder/capsule/bomb',\n",
    "                                  'Powder/capsule/bomb/crystal',\n",
    "                                  'Powder or crushed pill',\n",
    "                                  'Crystal, Capsule or Powder'],\n",
    "                      'liquid' : ['*Cannabinoid liquid',\n",
    "                                   '*Viscous liquid',\n",
    "                                  'Dissolved in Propylene Glycol',\n",
    "                                  'Oil'],\n",
    "                       'tab' : ['blotter', 'LSD Tab']\n",
    "                      }\n",
    "\n",
    "\n",
    "    # Firstly convert all columns to lower case and remove any spaces\n",
    "    def lower(value):\n",
    "        if type(value) in [str, unicode]:\n",
    "            value = value.strip().lower()\n",
    "        return value\n",
    "\n",
    "    for column in ['SampleForm']:\n",
    "        df[column] = df[column].map(lower, na_action='ignore')\n",
    "    \n",
    "    replace_d = {}\n",
    "    for column in ['SampleForm']:\n",
    "        replace_d[column] = {}\n",
    "        for drug, names in sample_form_d.items():\n",
    "            for name in names:\n",
    "                replace_d[column][name.lower()] = drug\n",
    "    \n",
    "    # Replace values\n",
    "    df.replace(replace_d, inplace=True)\n",
    "    return df\n",
    "    \n",
    "dfs.catalog = clean_df(dfs.catalog)\n",
    "dfs.ftir = clean_df(dfs.ftir)\n",
    "dfs.reagent = clean_df(dfs.reagent)\n",
    "print(\"Finished cleaning 'Sample form' field at %s\" % now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Catalog SampleNumber A0348 (line: 91) DIFFERENT FTIR sample (line: 92)\n",
      "[u'', u'', 'pill'] 2018-06-09 15:29:16\n",
      "[u'Found or otherwise not known', u'No', 'pill'] 2018-06-09 15:45:16\n",
      "Duplicate Catalog SampleNumber A0348 (line: 92) DIFFERENT FTIR sample (line: 92)\n",
      "[u'', u'', 'pill'] 2018-06-09 15:32:36\n",
      "[u'Found or otherwise not known', u'No', 'pill'] 2018-06-09 15:45:16\n",
      "Duplicate Catalog SampleNumber A0372 (line: 117) DIFFERENT FTIR sample (line: 156)\n",
      "[u'', u'', 'powder'] 2018-06-09 15:49:08\n",
      "[u'Found or otherwise not known', u'No', 'powder'] 2018-06-09 17:12:27\n",
      "Duplicate Catalog SampleNumber A0372 (line: 119) DIFFERENT FTIR sample (line: 156)\n",
      "[u'', u'', 'powder'] 2018-06-09 15:49:48\n",
      "[u'Found or otherwise not known', u'No', 'powder'] 2018-06-09 17:12:27\n",
      "Duplicate Catalog SampleNumber A0424 (line: 167) DIFFERENT FTIR sample (line: 323)\n",
      "[u'', u'', 'powder'] 2018-06-09 16:23:46\n",
      "[u'Found or otherwise not known', u'No', 'powder'] 2018-06-09 20:47:26\n",
      "Duplicate Catalog SampleNumber A0424 (line: 167) DIFFERENT FTIR sample (line: 662)\n",
      "[u'', u'', 'powder'] 2018-06-09 16:23:46\n",
      "[u'Found or otherwise not known', u'', 'powder'] 2018-09-17 20:50:54\n",
      "Duplicate Catalog SampleNumber A0424 (line: 169) DIFFERENT FTIR sample (line: 323)\n",
      "[u'', u'', 'powder'] 2018-06-09 16:24:13\n",
      "[u'Found or otherwise not known', u'No', 'powder'] 2018-06-09 20:47:26\n",
      "Duplicate Catalog SampleNumber A0424 (line: 169) DIFFERENT FTIR sample (line: 662)\n",
      "[u'', u'', 'powder'] 2018-06-09 16:24:13\n",
      "[u'Found or otherwise not known', u'', 'powder'] 2018-09-17 20:50:54\n",
      "Duplicate Catalog SampleNumber A0494 (line: 238) DIFFERENT FTIR sample (line: 159)\n",
      "[u'', u'', 'pill'] 2018-06-09 17:04:40\n",
      "[u'Found or otherwise not known', u'No', 'pill'] 2018-06-09 17:14:18\n",
      "Duplicate Catalog SampleNumber A0494 (line: 332) DIFFERENT FTIR sample (line: 159)\n",
      "[u'', u'', 'pill'] 2018-06-09 18:42:51\n",
      "[u'Found or otherwise not known', u'No', 'pill'] 2018-06-09 17:14:18\n",
      "Duplicate Catalog SampleNumber A1468 (line: 434) DIFFERENT FTIR sample (line: 407)\n",
      "[u'', u'', 'powder'] 2018-06-10 14:55:57\n",
      "[u'Found or otherwise not known', u'No', 'powder'] 2018-06-10 15:19:45\n",
      "Duplicate Catalog SampleNumber A1468 (line: 447) DIFFERENT FTIR sample (line: 407)\n",
      "[u'', u'', 'pill'] 2018-06-10 15:21:57\n",
      "[u'Found or otherwise not known', u'No', 'powder'] 2018-06-10 15:19:45\n",
      "Duplicate Catalog SampleNumber A2039 (line: 493) DIFFERENT FTIR sample (line: 466)\n",
      "[u'', u'', 'powder'] 2018-06-10 16:09:58\n",
      "[u'Found or otherwise not known', u'No', 'powder'] 2018-06-10 16:17:52\n",
      "Duplicate Catalog SampleNumber A2039 (line: 507) DIFFERENT FTIR sample (line: 466)\n",
      "[u'', u'', 'powder'] 2018-06-10 16:22:57\n",
      "[u'Found or otherwise not known', u'No', 'powder'] 2018-06-10 16:17:52\n",
      "Duplicate Catalog SampleNumber A2070 (line: 526) DIFFERENT FTIR sample (line: 529)\n",
      "[u'', u'', 'powder'] 2018-06-10 16:34:50\n",
      "[u'Found or otherwise not known', u'No', 'powder'] 2018-06-10 17:28:44\n",
      "Duplicate Catalog SampleNumber A2070 (line: 527) DIFFERENT FTIR sample (line: 529)\n",
      "[u'', u'', 'pill'] 2018-06-10 16:37:12\n",
      "[u'Found or otherwise not known', u'No', 'powder'] 2018-06-10 17:28:44\n",
      "Duplicate Catalog SampleNumber A2122 (line: 559) DIFFERENT FTIR sample (line: 555)\n",
      "[u'', u'', 'powder'] 2018-06-10 17:33:21\n",
      "[u'Found or otherwise not known', u'No', 'pill'] 2018-06-10 17:57:09\n",
      "Duplicate Catalog SampleNumber A2122 (line: 627) DIFFERENT FTIR sample (line: 555)\n",
      "[u'', u'', 'powder'] 2018-06-10 18:05:24\n",
      "[u'Found or otherwise not known', u'No', 'pill'] 2018-06-10 17:57:09\n"
     ]
    }
   ],
   "source": [
    "def find_duplicate_matches(duplicates, df1, df2, df1_name='DataFrame1', df2_name='DataFrame2'):\n",
    "    hr = False\n",
    "    if df1_name.lower()[:2] == 'hr':\n",
    "        hr = True\n",
    "    duplicate_matches = {}\n",
    "    min_stage_delay = 60 * 1\n",
    "    max_stage_delay = 60 * 60\n",
    "    for sample_number in duplicates:\n",
    "        duplicate_matches[sample_number] = {}\n",
    "        for df1_idx, df1_row in df1.loc[df1['SampleNumber'] == sample_number].iterrows():\n",
    "            for df2_idx, df2_row in df2.loc[df2['SampleNumber'] == sample_number].iterrows():\n",
    "                df1_data = df1_row.loc[['SoldAs', 'AlreadyTried']].values.tolist()\n",
    "                if not hr:\n",
    "                    df1_data.append(df1_row.SampleForm)\n",
    "                df1_time = df1_row.Timestamp\n",
    "                df2_data = df2_row.loc[['SoldAs', 'AlreadyTried']].values.tolist()\n",
    "                if not hr:\n",
    "                    df2_data.append(df2_row.SampleForm)\n",
    "                df2_time = df2_row.Timestamp\n",
    "                delta_t = (df2_time - df1_time).seconds\n",
    "                if df1_data == df2_data and min_stage_delay < delta_t <= max_stage_delay:\n",
    "                    print(\"Duplicate %s SampleNumber %s (line: %d) MATCHES %s sample (line: %d)\" % \\\n",
    "                          (df1_name, sample_number, df1_idx + 1, df2_name, df2_idx + 1))\n",
    "                    duplicate_matches[sample_number][df1_idx] = True\n",
    "                else:\n",
    "                    print(\"Duplicate %s SampleNumber %s (line: %d) DIFFERENT %s sample (line: %d)\\n%s %s\\n%s %s\" % \\\n",
    "                          (df1_name, sample_number, df1_idx + 1, df2_name, df2_idx + 1,\n",
    "                           df1_data, df1_time,\n",
    "                           df2_data, df2_time))\n",
    "                    duplicate_matches[sample_number][df1_idx] = False\n",
    "    return duplicate_matches\n",
    "\n",
    "def match_orphans_to_duplicates(df1_orphans, duplicate_matches, df1, df2):\n",
    "    for orphan_sample_number in df1_orphans:\n",
    "        df1_data = df1.loc[df1['SampleNumber'] == orphan_sample_number, ['SampleForm', 'SoldAs', 'AlreadyTried', 'Timestamp']]\n",
    "        df1_data = df1_data.values.tolist()[0]\n",
    "        df1_time = df1_data.pop()\n",
    "        for sample_number, indexd in duplicate_matches.items():\n",
    "            for k, v in indexd.items():\n",
    "                if not v:\n",
    "                    df2_data = dfs.catalog.iloc[k][['SampleForm', 'SoldAs', 'AlreadyTried', 'Timestamp']].values.tolist()\n",
    "                    df2_time = df2_data.pop()\n",
    "                    delta_t = (df1_time - df2_time).seconds\n",
    "                    if df2_data == df1_data and min_stage_delay < delta_t <= max_stage_delay:\n",
    "                        print(\"Orphan {} could be match for duplicate {} (line: {})\\n{} {}\\n{} {}\".format(orphan_sample_number,\n",
    "                                                                                                  sample_number, k+1,\n",
    "                                                                                                  df2_data, df2_time,\n",
    "                                                                                                  df1_data, df1_time))\n",
    "\n",
    "\n",
    "duplicate_matches = find_duplicate_matches(catalog_duplicates, dfs.catalog, dfs.ftir, df1_name='Catalog', df2_name='FTIR')\n",
    "# duplicate_matches = find_duplicate_matches(ftir_duplicates, dfs.ftir, dfs.catalog, df1_name='FTIR', df2_name='Catalog')\n",
    "# duplicate_matches = find_duplicate_matches(hr_duplicates, dfs.hr, dfs.catalog, df1_name='HR', df2_name='Catalog')\n",
    "\n",
    "#match_orphans_to_duplicates(ftir_orphan, duplicate_matches, dfs.ftir, dfs.catalog)\n",
    "# match_orphans_to_duplicates(catalog_orphan, duplicate_matches, dfs.catalog, dfs.ftir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR orphan P0306 (line: 271) could be match for FTIR SampleNumber F0306 (line: 281)\n",
      "[u'MDMA', u'Yes'] 2018-08-04 18:16:47\n",
      "[u'MDMA', u'Yes'] 2018-08-04 16:09:16\n",
      "HR orphan F20387 (line: 305) could be match for FTIR SampleNumber F0387 (line: 343)\n",
      "[u'MDMA', u'Yes'] 2018-08-05 13:33:36\n",
      "[u'MDMA', u'Yes'] 2018-08-05 13:02:51\n",
      "HR orphan F20357 (line: 297) could be match for FTIR SampleNumber F0357 (line: 322)\n",
      "[u'Cocaine', u'Yes'] 2018-08-05 12:54:45\n",
      "[u'Cocaine', u'Yes'] 2018-08-04 18:23:24\n",
      "HR orphan P0315 (line: 286) could be match for FTIR SampleNumber F0315 (line: 302)\n",
      "[u'MDMA', u'No'] 2018-08-04 18:53:56\n",
      "[u'MDMA', u'No'] 2018-08-04 17:26:33\n"
     ]
    }
   ],
   "source": [
    "# Check orphans against the FTIR sheet using just their numbers\n",
    "def match_orphans_with_sample_integer(orphans, orphan_df, ref_df):\n",
    "#     min_stage_delay = 60 * 1\n",
    "#     max_stage_delay = 60 * 60\n",
    "    def to_int(sn):\n",
    "        if type(sn) in [str, unicode]:\n",
    "            try:\n",
    "                sn = int(sn[-4:])\n",
    "            except ValueError:\n",
    "                print(\"Bad SampleNumber %s\" % sn)\n",
    "        return sn\n",
    "    orphan_df['SampleInteger'] = orphan_df['SampleNumber'].apply(to_int)\n",
    "    ref_df['SampleInteger'] = ref_df['SampleNumber'].apply(to_int)\n",
    "    orphan_ints = map(to_int, orphans)\n",
    "    \n",
    "    skipform = True\n",
    "    for orphan_sample_number, oint in zip(orphans, orphan_ints):\n",
    "        for orphan_idx, orphan_row in orphan_df.loc[orphan_df['SampleNumber'] == orphan_sample_number].iterrows():\n",
    "            for ref_idx, ref_row in ref_df.loc[ref_df['SampleInteger'] == oint].iterrows():\n",
    "                orphan_data = orphan_row.loc[['SampleForm', 'SoldAs', 'AlreadyTried']].values.tolist()\n",
    "                orphan_time = orphan_row.Timestamp\n",
    "                ref_sample_number = ref_row.SampleNumber\n",
    "                ref_data = ref_row.loc[['SampleForm', 'SoldAs', 'AlreadyTried']].values.tolist()\n",
    "                ref_time = ref_row.Timestamp\n",
    "                delta_t = (ref_time - orphan_time).seconds\n",
    "#                 if orphan_data == ref_data and min_stage_delay < delta_t <= max_stage_delay:\n",
    "                if skipform:\n",
    "                    orphan_data.pop(0)\n",
    "                    ref_data.pop(0)\n",
    "                if orphan_data == ref_data:\n",
    "                    print(\"HR orphan %s (line: %d) could be match for FTIR SampleNumber %s (line: %d)\\n%s %s\\n%s %s\" % \\\n",
    "                          (orphan_sample_number, orphan_idx + 1, ref_sample_number, ref_idx + 1, orphan_data, orphan_time, ref_data, ref_time))\n",
    "\n",
    "# match_orphans_with_sample_integer(catalog_orphan, dfs.catalog, dfs.ftir)\n",
    "match_orphans_with_sample_integer(hr_orphan, dfs.hr, dfs.catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOT  [u'n/a', u'No substance (empty baggy) ', u'No'] ['pill', u'MDMA', u'No'] 12253\n",
      "GOT  ['pill', u'Found or otherwise not known', u'No'] ['pill', u'MDMA', u'No'] 83605\n",
      "GOT  ['powder', u'Found or otherwise not known', u'No'] ['pill', u'MDMA', u'No'] 84606\n",
      "GOT  ['powder', u'Found or otherwise not known', u'No'] ['pill', u'MDMA', u'No'] 84648\n",
      "GOT  ['powder', u'Found or otherwise not known', u'No'] ['pill', u'MDMA', u'No'] 84683\n",
      "GOT  ['powder', u'Found or otherwise not known', u'No'] ['pill', u'MDMA', u'No'] 84715\n"
     ]
    }
   ],
   "source": [
    "# Check orphans against other orphans just using data\n",
    "def match_orphans_vs_orphans(orphan1_list, orphan1_df, orphan2_list, orphan2_df, hr=False):\n",
    "    min_stage_delay = 60 * 1\n",
    "    max_stage_delay = 60 * 60\n",
    "    for orphan1 in orphan1_list:\n",
    "        orphan1_row = orphan1_df.loc[orphan1_df['SampleNumber'] == orphan1].iloc[0]\n",
    "        for orphan2 in orphan2_list:\n",
    "            orphan2_row = orphan2_df.loc[orphan2_df['SampleNumber'] == orphan2].iloc[0]\n",
    "            orphan1_data = orphan1_row.loc[['SampleForm', 'SoldAs', 'AlreadyTried']].values.tolist()\n",
    "            orphan1_time = orphan1_row.Timestamp\n",
    "            orphan2_data = orphan2_row.loc[['SampleForm', 'SoldAs', 'AlreadyTried']].values.tolist()\n",
    "            orphan2_time = orphan2_row.Timestamp\n",
    "            delta_t = (orphan2_time - orphan1_time).seconds\n",
    "            if orphan1_data == orphan2_data and min_stage_delay <= delta_t <= max_stage_delay:\n",
    "                print(\"orphan1 %s could be match for orphan2 %s\\n%s %s\\n%s %s\" % \\\n",
    "                      (orphan1, orphan2, orphan1_data, orphan1_time, orphan1_data, orphan1_time))\n",
    "\n",
    "# match_orphans_with_sample_integer(catalog_orphan, dfs.catalog, dfs.ftir)\n",
    "match_orphans_vs_orphans(catalog_orphan, dfs.catalog, ftir_orphan, dfs.ftir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean up catalog\n",
    "# Drop all unwanted columns\n",
    "\n",
    "#  or 'Your initials'\n",
    "l = set(['Your initials',\n",
    "         'Your name and first initial',\n",
    "         'Which device was a photo taken with? Who does it belong to?',\n",
    "         'Is a breakline present?',\n",
    "         'Unusual appearance'\n",
    "        ])\n",
    "\n",
    "to_drop = set(dfs.catalog.columns).intersection(l)\n",
    "dfs.catalog.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "d = {\n",
    "    'Timestamp' : 'Catalog timestamp',\n",
    "    'Sample Advertised/Acquired/Sold As': 'Catalog_SoldAs',\n",
    "    'Sample Form' : 'Catalog_Form',\n",
    "    'Has the Service User or a close friend tried this batch?': 'Catalog_Tried',\n",
    "    'What is the mass? (mg)': 'FullPillMass',\n",
    "    'What is the shape of the pill?': 'PillShape',\n",
    "    'What is the logo?': 'PillLogo',\n",
    "    'What colour is the pill?': 'PillColour'\n",
    "}\n",
    "dfs.catalog.rename(columns=d, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('COLS ', Index([                                                           u'Timestamp',\n",
      "                                                              u'Sample Number',\n",
      "                                                                     u'Tester',\n",
      "                                                                    u'Sold As',\n",
      "                                                                u'Sample Form',\n",
      "                                                              u'Already Tried',\n",
      "                                                             u'User Suspicion',\n",
      "                                                         u'Substance detected',\n",
      "                                                             u'Hit Confidence',\n",
      "                                                          u'Compound detected',\n",
      "                                                           u'Hit Confidence.1',\n",
      "                                                                 u'Brief Note',\n",
      "                           u'Is anything detected after subtraction analysis?',\n",
      "                                            u'Compound detected (Subtraction)',\n",
      "                                                           u'Hit Confidence.2',\n",
      "                                                       u'Substance detected.1',\n",
      "                                                           u'Hit Confidence.3',\n",
      "                                                               u'Brief Note.1',\n",
      "                                                             u'Next action(s)',\n",
      "                                                      u'Substance(s) detected',\n",
      "                                           u'\"Strength\" of powdered substance',\n",
      "       u'Does the substance detected match the substance that was advertised?',\n",
      "                                             u'Note for harm reduction worker',\n",
      "                                                            u'Send to HR team'],\n",
      "      dtype='object'))\n",
      "('SS ', 1691    N-Ethylpentylone\n",
      "1692    N-Ethylpentylone\n",
      "1694    N-Ethylpentylone\n",
      "1696             Cocaine\n",
      "0                   MDMA\n",
      "Name: Substance detected, dtype: object)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_ftir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f76c2c2a13ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Compound detected (Subtraction)'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'Other'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Compound detected (Subtraction)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_ftir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Substance detected.1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Copy values from 'Compound detected'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hit Confidence.2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hit Confidence.3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Substance detected.1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Hit Confidence.3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Brief Note.1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_ftir' is not defined"
     ]
    }
   ],
   "source": [
    "# For FTIR columns need to merge the data from the 'Compound detected', 'Hit Confidence.1' columns into the\n",
    "# 'Substance detected', 'Hit Confidence' column where the substance detected was 'other'\n",
    "print(\"COLS \",dfs.ftir.columns)\n",
    "print(\"SS \",dfs.ftir['Substance detected'][:5])\n",
    "mask = dfs.ftir['Substance detected'] != 'Other'\n",
    "dfs.ftir['Substance detected'].where(mask, dfs.ftir['Compound detected'], inplace=True) # Copy values from 'Compound detected'\n",
    "dfs.ftir['Hit Confidence'].where(mask, dfs.ftir['Hit Confidence.1'], inplace=True)\n",
    "dfs.ftir.drop(['Compound detected', 'Hit Confidence.1', 'Brief Note'], axis=1, inplace=True)\n",
    "\n",
    "mask = dfs.ftir['Compound detected (Subtraction)'] != 'Other'\n",
    "dfs.ftir['Compound detected (Subtraction)'].where(mask, df_ftir['Substance detected.1'], inplace=True) # Copy values from 'Compound detected'\n",
    "dfs.ftir['Hit Confidence.2'].where(mask, dfs.ftir['Hit Confidence.3'], inplace=True)\n",
    "dfs.ftir.drop(['Substance detected.1', 'Hit Confidence.3', 'Brief Note.1'], axis=1, inplace=True)\n",
    "\n",
    "# Drop all unwanted columns\n",
    "l = ['Your name and surname initial',\n",
    "     'User Suspicion',\n",
    "     'Is anything detected after subtraction analysis?',\n",
    "     'Analysis required', \n",
    "     'Next action(s)',\n",
    "     'Send to HR team'\n",
    "    ]\n",
    "#'Note for harm reduction worker'\n",
    "to_drop = set(dfs.ftir.columns).intersection(l)\n",
    "dfs.ftir.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Rename shared columns so that we can check for any errors and remove any columns not of interest to the master df\n",
    "d = {\n",
    "    'Timestamp' : 'FTIR timestamp',\n",
    "    'Sample Sold As': 'FTIR Sold As',\n",
    "    'Sample Form' : 'FTIR form',\n",
    "    'Has the Service User or a close friend tried this batch?': 'FTIR tried',\n",
    "    'Substance(s) detected' : 'FTIR final result',\n",
    "    'Substance detected' : 'FTIR result1',\n",
    "    'Hit Confidence' :  'FTIR hit1',\n",
    "    'Is anything detected after subtraction analysis?' : 'FTIR subtraction positive',\n",
    "    'Compound detected (Subtraction)' :  'FTIR result2',\n",
    "    'Hit Confidence.2' :  'FTIR hit2',\n",
    "    '\"Strength\" of powdered substance' : 'FTIR Powder Strength',\n",
    "    'Does the substance detected match the substance that was advertised?' : 'FTIR Matches Sold As',\n",
    "}\n",
    "dfs.ftir.rename(columns=d, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean up HR form\n",
    "\n",
    "# Drop all unwanted columns\n",
    "l = ['HR worker name:']\n",
    "dfs.hr.drop(l, axis=1, inplace=True)\n",
    "\n",
    "# Rename shared columns so that we can check for any errors and remove any columns not of interest to the master df\n",
    "d = {\n",
    "    'Timestamp' : 'HR timestamp',\n",
    "    'You submitted a substance for analysis. What were you told it was when you got it?': 'HR Sold as',\n",
    "    'Had you already tried this substance before getting it tested?': 'HR tried',\n",
    "    'What was your first sample number at this event? Did you take a photo or keep the ticket?': 'Previous Sample Number'\n",
    "}\n",
    "dfs.hr.rename(columns=d, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Catalog and FTIR data frames\n",
    "df_all = pd.merge(dfs.catalog, dfs.ftir, how='left', on=['Sample Number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge in any reagent test data\n",
    "df_all = pd.merge(df_all, dfs.reagent[['Sample Number', 'Reagent Result']], how='left', on=['Sample Number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge in any pill strength data\n",
    "df_all = pd.merge(df_all, dfs.mla[['Sample Number', 'MDMA / tablet (mg)', '% MDMA content']], how='left', on=['Sample Number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge in HR data\n",
    "df_all = pd.merge(df_all, dfs.hr, how='left', on=['Sample Number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fix column orders\n",
    "prefix = ['Sample Number',\n",
    "          'Catalog timestamp', 'FTIR timestamp', 'HR timestamp',\n",
    "          'Catalog Sold As', 'FTIR Sold As','HR Sold as', \n",
    "          'Catalog form', 'FTIR form',\n",
    "          'Catalog tried', 'FTIR tried', 'HR tried']\n",
    "columns = [c for c in df_all.columns if c not in prefix]\n",
    "columns = prefix + columns\n",
    "df_all = df_all[columns]\n",
    "df_all.to_csv('foo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
