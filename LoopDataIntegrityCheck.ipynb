{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module imports\n",
    "import copy\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.mode.chained_assignment = 'raise'\n",
    "\n",
    "VERSION = 0.8\n",
    "\n",
    "\n",
    "# Need to define in main or we can't pickle the data objects\n",
    "class DataFrames(object):\n",
    "    def __init__(self):\n",
    "        self.catalog = None\n",
    "        self.ftir = None\n",
    "        self.reagent = None\n",
    "        self.mla = None\n",
    "        self.hr = None\n",
    "        self.combined = None\n",
    "\n",
    "\n",
    "def now():\n",
    "    return datetime.datetime.now().strftime(\"%d/%m/%y %H:%M:%S\")\n",
    "\n",
    "\n",
    "def get_rename_columns_map():\n",
    "    sheet_id = '1CgqTjdKizat-g7K7-AAuVIazQFKJ3WAAPHR-Qpa49lU'\n",
    "    ss_range = 'ColumnMap!A:B'\n",
    "    result = GSHEETS_SERVICE.spreadsheets().values().get(spreadsheetId=sheet_id,\n",
    "                                                         range=ss_range).execute()\n",
    "    values = result.get('values', [])\n",
    "    assert values[0] == ['OriginalColumn', 'CanonicalColumn'], values[0]\n",
    "    return { cm[0] : cm[1] for cm in values[1:] if len(cm) >= 2 }\n",
    "\n",
    "\n",
    "def gsheets_service():\n",
    "    from googleapiclient.discovery import build\n",
    "    from httplib2 import Http\n",
    "    from oauth2client import file, client, tools\n",
    "    # If modifying these scopes, delete the file token.json.\n",
    "    #Ensure that the creds file is always taken from the current working folder\n",
    "        #This allows two people on different PCs to merge changes more easily.\n",
    "    CREDS_FILE = os.path.join(os.path.realpath('./'),'JensDataExportJupyter_client_secret.json')\n",
    "    SCOPES = 'https://www.googleapis.com/auth/spreadsheets.readonly'\n",
    "    store = file.Storage('token.json')\n",
    "    creds = store.get()\n",
    "    if not creds or creds.invalid:\n",
    "        import argparse\n",
    "        flags = argparse.ArgumentParser(parents=[tools.argparser]).parse_args([])\n",
    "        flow = client.flow_from_clientsecrets(CREDS_FILE, SCOPES)\n",
    "        creds = tools.run_flow(flow, store, flags)\n",
    "    service = build('sheets', 'v4', http=creds.authorize(Http()))\n",
    "    return service\n",
    "\n",
    "\n",
    "def get_data(spreadsheet_id):\n",
    "    catalog_range = 'Catalog!A:R'\n",
    "    ftir_range = 'FTIR!A:X'\n",
    "    reagent_range = 'Reagent!A:W'\n",
    "    mla_range = 'MLA!A:R'\n",
    "    hr_range = 'Interventions!A:BJ'\n",
    "    \n",
    "    df_catalog = get_df(spreadsheet_id, catalog_range)\n",
    "    df_catalog = canonicalise_df(df_catalog, source='catalog')\n",
    "    df_ftir = get_df(spreadsheet_id, ftir_range)\n",
    "    df_ftir = canonicalise_df(df_ftir, source='ftir')\n",
    "    df_reagent = get_df(spreadsheet_id, reagent_range)\n",
    "    df_reagent = canonicalise_df(df_reagent, source='reagent')\n",
    "    df_mla = get_df(spreadsheet_id, mla_range, mla=True)\n",
    "    df_mla = canonicalise_df(df_mla, source='mla')\n",
    "    try:\n",
    "        df_hr = get_df(spreadsheet_id, hr_range)\n",
    "    except ValueError:\n",
    "        df_hr = None\n",
    "    if df_hr is not None:\n",
    "        pass\n",
    "        df_hr = canonicalise_df(df_hr, source='hr')\n",
    "\n",
    "    df = DataFrames()\n",
    "    df.catalog = df_catalog\n",
    "    df.ftir = df_ftir\n",
    "    df.reagent = df_reagent\n",
    "    df.mla = df_mla\n",
    "    df.hr = df_hr\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_df(spreadsheet_id, ss_range, mla=False):\n",
    "    # Call the Sheets API\n",
    "    result = GSHEETS_SERVICE.spreadsheets().values().get(spreadsheetId=spreadsheet_id,\n",
    "                                                         range=ss_range).execute()\n",
    "    values = result.get('values', [])\n",
    "    if not values:\n",
    "        print('*** No data found ***')\n",
    "        return None\n",
    "\n",
    "    # mla has irrelevant stuff in columns 1 and 3 and sample numbers in first column\n",
    "    if mla:\n",
    "        values.pop(0)\n",
    "        values.pop(1)\n",
    "        def not_blank(row):\n",
    "            return len(row[0]) > 0       \n",
    "    else:\n",
    "        def not_blank(row):\n",
    "            return sum(map(len, row[:6])) > 0\n",
    "\n",
    "    rows = list(filter(not_blank, values))\n",
    "    if not rows:\n",
    "        print('*** No data found after pruning rows! ***')\n",
    "        return None\n",
    "    \n",
    "    columns = enumerate_duplicates(rows[0])\n",
    "    ncols = len(rows[0])\n",
    "    row_max = max(map(len, rows[1:]))\n",
    "    width = min(ncols, row_max)\n",
    "    return pd.DataFrame(rows[1:], columns=columns[:width])\n",
    "\n",
    "\n",
    "def enumerate_duplicates(row):\n",
    "    \"\"\"Append a counter to duplicate labels\"\"\"\n",
    "    SEPARATOR = '.'\n",
    "    duplicates = {}\n",
    "    updated_row = []\n",
    "    for r in row:\n",
    "        count = duplicates.get(r, 0)\n",
    "        if count > 0:\n",
    "            label = \"{}{}{}\".format(r, SEPARATOR, count)\n",
    "        else:\n",
    "            label = r\n",
    "        updated_row.append(label)\n",
    "        duplicates[r] = count + 1\n",
    "    return updated_row\n",
    "\n",
    "\n",
    "def canonicalise_df(df, source=None):\n",
    "    \"\"\"Initial cleaning of all dataframes\"\"\"\n",
    "    if source:\n",
    "        print(\"Canonicalising %s\" % source)\n",
    "    # Standardise names\n",
    "    df.rename(columns=RENAME_COLUMN_MAP, inplace=True)\n",
    "    def fix_timestamp(x):\n",
    "        return pd.to_datetime(str(x), format='%d/%m/%Y %H:%M:%S')\n",
    "    if 'Timestamp' in df.columns:\n",
    "        df.loc[:, 'Timestamp'] = df['Timestamp'].map(fix_timestamp)\n",
    "    #df.loc[:, 'SampleNumber'] = df['SampleNumber'].apply(fix_sample_number)\n",
    "    df.dropna(subset=['SampleNumber'], inplace=True)\n",
    "    if set(df.columns.values).intersection(set([np.nan, ''])):\n",
    "        raise RuntimeError(\"Blank column names in Dataframe\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def check_individual_dataframes(dfs):\n",
    "    errors = []\n",
    "    for name in ['catalog', 'ftir', 'reagent', 'mla', 'hr']:\n",
    "        df = getattr(dfs, name)\n",
    "        error = _check_individual_dataframe(df, name)\n",
    "        errors.append(error)\n",
    "    if any(errors):\n",
    "#         raise RuntimeError(\"Data errors need to be fixed!\")\n",
    "        print(\"Data errors need to be fixed!\")\n",
    "\n",
    "        \n",
    "def _check_individual_dataframe(df, source):\n",
    "    cname = 'SampleNumber'\n",
    "    dups = df[cname].duplicated()\n",
    "    if any(dups):\n",
    "        NSTAR = 80\n",
    "        print(\"*\" * NSTAR)\n",
    "        print(\"WARNING!! %s sheet has duplicate sample numbers: %s\" % (source, df[cname][dups].values))\n",
    "        print(\"*\" * NSTAR)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def person_id_from_samplenumber(x):\n",
    "    if isinstance(x, float) and np.isnan(x):\n",
    "        return x # leave NaN's alone\n",
    "    if (isinstance(x, str) or isinstance(x, unicode)) and len(x) == 0:\n",
    "        return np.nan\n",
    "    if len(x) != 5:\n",
    "        return np.nan\n",
    "    try:\n",
    "        pn = 'P{:04d}'.format(int(x[-3:]))\n",
    "    except ValueError:\n",
    "        print(\"!!! Bad ID \\'%s\\'\" % pn)\n",
    "        pn = x\n",
    "    return pn\n",
    "\n",
    "\n",
    "def add_person_id(hr_df):\n",
    "    \"\"\"Create the unique PID column\"\"\"\n",
    "    hr_df['PID'] = dfs.hr['SampleNumber']\n",
    "    # Copy over SampleNumbers from Previous_sample\n",
    "    mask = hr_df['Previous_sample'].isnull()\n",
    "    hr_df['PID'] = hr_df['PID'].where(mask, hr_df['Previous_sample'])\n",
    "    # Clean up values\n",
    "    hr_df['PID'] = hr_df['PID'].apply(person_id_from_samplenumber)\n",
    "    return hr_df\n",
    "\n",
    "\n",
    "def merge_ftir_drug_columns(df):\n",
    "    # Copy over 'Other' substances into the main column\n",
    "    target_label = 'Substance detected'\n",
    "    source_label = 'Compound detected'\n",
    "    to_drop = [source_label, 'Hit Confidence.1']\n",
    "    other_mask = ~df[target_label].str.startswith('Other').fillna(value=False)\n",
    "    df[target_label].where(other_mask, df[source_label], inplace=True) # Copy values from source_label column over\n",
    "    df.drop(to_drop, axis=1, inplace=True) # Remove now redundant columns\n",
    "    df.rename(columns={target_label : 'Primary_hit', 'Hit Confidence' : 'Primary_confidence'}, inplace=True) # Rename Columns\n",
    "\n",
    "    # Column names appear to be reversed - compound now is substance!!\n",
    "    target_label = 'Compound detected (Subtraction)'\n",
    "    source_label = 'Substance detected.1'\n",
    "    to_drop = [source_label, 'Hit Confidence.3']\n",
    "    other_mask = ~df[target_label].str.startswith('Other').fillna(value=False)\n",
    "    df[target_label].where(other_mask, df[source_label], inplace=True) # Copy values from source_label column over\n",
    "    df.drop(to_drop, axis=1, inplace=True) # Remove now redundant columns\n",
    "    df.rename(columns={target_label : 'Secondary_hit', 'Hit Confidence.2' : 'Secondary_confidence'}, inplace=True) # Rename Columns\n",
    "\n",
    "    \n",
    "def calculate_final_results(df):\n",
    "    \"\"\"Calculate final result\"\"\"\n",
    "    # Where 'ftir_Substance(s) detected' is null we use the ftir_Primary_hit\n",
    "    mask = ~df['ftir_Substance(s) detected'].isin(['', np.nan, None])\n",
    "    df['ftir_Substance(s) detected'] = df['ftir_Substance(s) detected'].where(mask, df['ftir_Primary_hit'])\n",
    "    # Find where'reagent_Substance(s) detected' contains anything but 'No active component identified'\n",
    "    mask = df['reagent_Substance(s) detected'].isin([None, np.nan,'No active component identified' ])\n",
    "    # Default is 'ftir_Substance(s) detected'\n",
    "    df['Final_result_calculated'] = df['ftir_Substance(s) detected']\n",
    "    # Copy over anything from 'reagent_Substance(s) detected'\n",
    "    df['Final_result_calculated'] = df['Final_result_calculated'].where(mask, df['reagent_Substance(s) detected'])\n",
    "    # Need to lowercase for comparison\n",
    "    df['Final_result_calculated'] = df['Final_result_calculated'].astype(str).str.lower()\n",
    "    df['catalog_SoldAs'] = df['catalog_SoldAs'].astype(str).str.lower()\n",
    "    \n",
    "    # Calculate where they do/don't match\n",
    "    df['As_expected'] = (df['Final_result_calculated'] == df['catalog_SoldAs']).map({True : 'Yes', False : 'No'})\n",
    "\n",
    "    # Guy 28/10/18: 'As_expected' should be null whenever the sample is found,\n",
    "    # when the submission 'acquired as\" data is blank or unknown, or when the sample is from Amnesty\n",
    "    mask1 = df['catalog_SoldAs'].isin(['found', 'found or otherwise not known', np.nan, None])\n",
    "    mask2 = df['catalog_SampleSource'] != 'Public'\n",
    "    mask = mask1 | mask2\n",
    "    # jmht - could check against: 'hr_Was the sample bought, given or found?\n",
    "    df.loc[mask, ['As_expected']] = np.nan\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_dataframes(dfs):\n",
    "    # Clean up ftir sheet\n",
    "    merge_ftir_drug_columns(dfs.ftir)\n",
    "\n",
    "    if dfs.hr is not None:\n",
    "        dfs.hr = add_person_id(dfs.hr)\n",
    "    \n",
    "    # Rename columns to identify source dataframe\n",
    "    dfs.catalog.columns = ['catalog_'+ name if name != 'SampleNumber' else name for name in dfs.catalog.columns]\n",
    "    dfs.ftir.columns = ['ftir_'+ name if name != 'SampleNumber' else name for name in dfs.ftir.columns]\n",
    "    dfs.mla.columns = ['mla_'+ name if name != 'SampleNumber' else name for name in dfs.mla.columns]\n",
    "    dfs.reagent.columns = ['reagent_'+ name if name != 'SampleNumber' else name for name in dfs.reagent.columns]\n",
    "    if dfs.hr is not None:\n",
    "        dfs.hr.columns = ['hr_'+ name if name != 'SampleNumber' else name for name in dfs.hr.columns]\n",
    "\n",
    "    # Remove all but the last of any duplicate SampleNumber\n",
    "    remove_duplicates = False\n",
    "    if remove_duplicates:\n",
    "        mask = ~dfs.catalog['SampleNumber'].duplicated(keep=False) | ~dfs.catalog['SampleNumber'].duplicated(keep='last')\n",
    "        dfs.catalog = dfs.catalog[mask]\n",
    "        mask = ~dfs.ftir['SampleNumber'].duplicated(keep=False) | ~dfs.ftir['SampleNumber'].duplicated(keep='last')\n",
    "        dfs.ftir = dfs.ftir[mask]\n",
    "        mask = ~dfs.mla['SampleNumber'].duplicated(keep=False) | ~dfs.mla['SampleNumber'].duplicated(keep='last')\n",
    "        dfs.mla = dfs.mla[mask]\n",
    "        mask = ~dfs.reagent['SampleNumber'].duplicated(keep=False) | ~dfs.reagent['SampleNumber'].duplicated(keep='last')\n",
    "        dfs.reagent = dfs.reagent[mask]\n",
    "        if dfs.hr is not None:\n",
    "            mask = ~dfs.hr['SampleNumber'].duplicated(keep=False) | ~dfs.hr['SampleNumber'].duplicated(keep='last')\n",
    "            dfs.hr = dfs.hr[mask]\n",
    "\n",
    "    # First outer join on catalog/ftir to make sure we collect all possible information - this will result in\n",
    "    # some rows where there was no catalog data, only ftir data, but this is ok as when we merge with hr we will\n",
    "    # throw away any row that doesn't have a corresponding sample number in HR. This was even if catalog data is\n",
    "    # missing, we still get the FTIR data, which may be enough for our purposes\n",
    "    df_all = pd.merge(dfs.catalog, dfs.ftir, how='outer', on=['SampleNumber'])\n",
    "    # Add in mla data - only for when there are existing sample numbers\n",
    "    df_all = pd.merge(df_all, dfs.mla, how='left', on=['SampleNumber'])\n",
    "    df_all = pd.merge(df_all, dfs.reagent, how='left', on=['SampleNumber'])\n",
    "    if dfs.hr is not None:\n",
    "        # inner join -> merge only where there are matching sample numbers\n",
    "        df_all = pd.merge(df_all, dfs.hr, how='inner', on=['SampleNumber'])\n",
    "    dfs.combined = df_all\n",
    "\n",
    "\n",
    "def add_final_data(dfs):\n",
    "    # Calculate final result\n",
    "    dfs.combined = calculate_final_results(dfs.combined)\n",
    "    \n",
    "    # Add unique columns\n",
    "    #dfs.combined.insert(loc=0, column='Festival', value=festival)\n",
    "    #dfs.combined.insert(loc=1, column='UID', value=dfs.combined[['Festival', 'SampleNumber']].apply(lambda x: '_'.join(x), axis=1))\n",
    "    dfs.combined['Version'] = VERSION\n",
    "\n",
    "\n",
    "def check_matching_values(cols):\n",
    "    # Check all lower-case string values are the same\n",
    "    return len(np.unique(cols.astype(str).str.strip().str.lower().values)) != 1\n",
    "\n",
    "\n",
    "def check_merged_data(dfs):\n",
    "    NSTAR = 80\n",
    "    cols = ['catalog_SoldAs', 'ftir_SoldAs', 'hr_SoldAs']\n",
    "    unmatched = dfs.combined[cols].apply(check_matching_values, axis=1)\n",
    "    if any(unmatched):\n",
    "        print(\"*\" * NSTAR)\n",
    "        print(\"Unmatched SoldAs data\")\n",
    "        print(dfs.combined.loc[unmatched, ['SampleNumber'] + cols])\n",
    "        print(\"*\" * NSTAR)\n",
    "\n",
    "\n",
    "    cols = ['catalog_AlreadyTried', 'ftir_AlreadyTried', 'hr_AlreadyTried']\n",
    "    unmatched = dfs.combined[cols].apply(check_matching_values, axis=1)\n",
    "    if any(unmatched):\n",
    "        print(\"*\" * NSTAR)\n",
    "        print(\"Unmatched AlreadyTried data\")\n",
    "        print(dfs.combined.loc[unmatched, ['SampleNumber'] + cols])\n",
    "        print(\"*\" * NSTAR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script running from: /opt/random\n",
      "Canonicalising catalog\n",
      "Canonicalising ftir\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "Canonicalising hr\n",
      "********************************************************************************\n",
      "Unmatched SoldAs data\n",
      "  SampleNumber catalog_SoldAs ftir_SoldAs hr_SoldAs\n",
      "1        F0077           mdma        MDMA     pill \n",
      "8        F0095            lsd         NaN    \"Acid\"\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Unmatched AlreadyTried data\n",
      "   SampleNumber catalog_AlreadyTried ftir_AlreadyTried hr_AlreadyTried\n",
      "4         F0089                   No                No             Yes\n",
      "8         F0095                  Yes               NaN             Yes\n",
      "10        F0108                  Yes               Yes              No\n",
      "12        F0120                   No               Yes             Yes\n",
      "13        F0119                   No               Yes             Yes\n",
      "********************************************************************************\n",
      "Finished processing dataframes at 15/12/18 17:32:52\n"
     ]
    }
   ],
   "source": [
    "SPREADSHEET_ID = '1LUFyELCP6VXH3rU8-RVa7NQvCQBYQXRsIoFDbokDd4s'\n",
    "\n",
    "print(\"Script running from: %s\" % os.path.realpath(os.getcwd()))\n",
    "try:\n",
    "    RENAME_COLUMN_MAP = get_rename_columns_map()\n",
    "except ConnectionResetError as e:\n",
    "    raise RuntimeError(\"Cannot connect to server to get RENAME_COLUMN_MAP: %s\" % e)\n",
    "\n",
    "GSHEETS_SERVICE = gsheets_service()\n",
    "dfs = get_data(SPREADSHEET_ID)\n",
    "\n",
    "# Run initial checks\n",
    "check_individual_dataframes(dfs)\n",
    "\n",
    "# Merge everything together\n",
    "merge_dataframes(dfs)\n",
    "add_final_data(dfs)\n",
    "\n",
    "# Check merged data\n",
    "check_merged_data(dfs)\n",
    "\n",
    "print(\"Finished processing dataframes at %s\" % now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/12/18 17:32:52 Wrote version 0.8 to file: LoopData_0.8.xls\n"
     ]
    }
   ],
   "source": [
    "filename = 'LoopData_%s.xls' % VERSION\n",
    "writer = pd.ExcelWriter(filename)\n",
    "dfs.combined.to_excel(writer, 'MergedData', index=False)\n",
    "writer.save()\n",
    "print(now() + \" Wrote version %s to file: %s\" % (VERSION, filename))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
