{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Map lives at: https://docs.google.com/spreadsheets/d/1CgqTjdKizat-g7K7-AAuVIazQFKJ3WAAPHR-Qpa49lU/edit#gid=761153638"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module imports\n",
    "import copy\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "VERSION = 1.4\n",
    "IMPORT_PKL = 'import.pkl'\n",
    "\n",
    "\n",
    "def is_str(x):\n",
    "    return isinstance(x, str)\n",
    "\n",
    "\n",
    "def fix_sample_number(x):\n",
    "    \"\"\"Make sure all samples numbers are of form: AXXX (where A is one of A, F, W and X is a digit)\"\"\"\n",
    "    if isinstance(x, float) and np.isnan(x):\n",
    "        return x # leave NaN's alone\n",
    "    if is_str(x) and len(x) == 0:\n",
    "        return np.nan\n",
    "    try:\n",
    "        sn = 'F{:04d}'.format(int(x))\n",
    "    except ValueError:\n",
    "        # Assume string so make sure it's of the right format\n",
    "        sn = str(x).strip().upper()\n",
    "    len_sn = len(sn)\n",
    "    if not ((len_sn == 5 and sn[0] in ['A', 'F', 'W', 'B']) or (len_sn == 6 and sn[0] == 'D')):\n",
    "        print(\"!!! Bad ID \\'%s\\'\" % sn)\n",
    "    return sn\n",
    "\n",
    "\n",
    "def now():\n",
    "    return datetime.datetime.now().strftime(\"%d/%m/%y %H:%M:%S\")\n",
    "\n",
    "\n",
    "def enumerate_duplicates(row):\n",
    "    \"\"\"Append a counter to duplicate labels\"\"\"\n",
    "    SEPARATOR = '.'\n",
    "    duplicates = {}\n",
    "    updated_row = []\n",
    "    for r in row:\n",
    "        count = duplicates.get(r, 0)\n",
    "        if count > 0:\n",
    "            label = \"{}{}{}\".format(r, SEPARATOR, count)\n",
    "        else:\n",
    "            label = r\n",
    "        updated_row.append(label)\n",
    "        duplicates[r] = count + 1\n",
    "    return updated_row\n",
    "\n",
    "\n",
    "# Need to define in main or we can't pickle the data objects\n",
    "class DataFrames(object):\n",
    "    def __init__(self):\n",
    "        self.catalog = None\n",
    "        self.ftir = None\n",
    "        self.reagent = None\n",
    "        self.mla = None\n",
    "        self.hr = None\n",
    "        self.combined = None\n",
    "\n",
    "pd.options.mode.chained_assignment = 'raise'\n",
    "\n",
    "\n",
    "def gsheets_service():\n",
    "    from googleapiclient.discovery import build\n",
    "    from httplib2 import Http\n",
    "    from oauth2client import file, client, tools\n",
    "    # If modifying these scopes, delete the file token.json.\n",
    "    #Ensure that the creds file is always taken from the current working folder\n",
    "        #This allows two people on different PCs to merge changes more easily.\n",
    "    CREDS_FILE = os.path.join(os.path.realpath('./'),'JensDataExportJupyter_client_secret.json')\n",
    "    SCOPES = 'https://www.googleapis.com/auth/spreadsheets.readonly'\n",
    "    store = file.Storage('token.json')\n",
    "    creds = store.get()\n",
    "    if not creds or creds.invalid:\n",
    "        import argparse\n",
    "        flags = argparse.ArgumentParser(parents=[tools.argparser]).parse_args([])\n",
    "        flow = client.flow_from_clientsecrets(CREDS_FILE, SCOPES)\n",
    "        creds = tools.run_flow(flow, store, flags)\n",
    "    service = build('sheets', 'v4', http=creds.authorize(Http()))\n",
    "    return service\n",
    "\n",
    "\n",
    "def get_df(spreadsheet_id, ss_range, mla=False):\n",
    "    # Call the Sheets API\n",
    "    result = GSHEETS_SERVICE.spreadsheets().values().get(spreadsheetId=spreadsheet_id,\n",
    "                                                         range=ss_range).execute()\n",
    "    values = result.get('values', [])\n",
    "    if not values:\n",
    "        print('*** No data found ***')\n",
    "        return None\n",
    "\n",
    "    # mla has irrelevant stuff in columns 1 and 3 and sample numbers in first column\n",
    "    if mla:\n",
    "        values.pop(0)\n",
    "        values.pop(1)\n",
    "        def not_blank(row):\n",
    "            return len(row[0]) > 0       \n",
    "    else:\n",
    "        def not_blank(row):\n",
    "            return sum(map(len, row[:6])) > 0\n",
    "\n",
    "    rows = list(filter(not_blank, values))\n",
    "    if not rows:\n",
    "        print('*** No data found after pruning rows! ***')\n",
    "        return None\n",
    "    \n",
    "    columns = enumerate_duplicates(rows[0])\n",
    "    ncols = len(rows[0])\n",
    "    row_max = max(map(len, rows[1:]))\n",
    "    width = min(ncols, row_max)\n",
    "    return pd.DataFrame(rows[1:], columns=columns[:width])\n",
    "\n",
    "\n",
    "def canonicalise_df(df, source=None):\n",
    "    \"\"\"Initial cleaning of all dataframes\"\"\"\n",
    "    #from pandas._libs.tslib import OutOfBoundsDatetime\n",
    "    if source:\n",
    "        print(\"Canonicalising %s\" % source)\n",
    "    # Standardise names\n",
    "    df.rename(columns=RENAME_COLUMN_MAP, inplace=True)\n",
    "    \n",
    "    def fix_timestamp(x):\n",
    "        return pd.to_datetime(str(x), format='%d/%m/%Y %H:%M:%S')\n",
    "    if 'Timestamp' in df.columns:\n",
    "        df.loc[:, 'Timestamp'] = df['Timestamp'].map(fix_timestamp)\n",
    "    df.loc[:, 'SampleNumber'] = df['SampleNumber'].apply(fix_sample_number)\n",
    "    df.dropna(subset=['SampleNumber'], inplace=True)\n",
    "    #df.sort_values(['Sample Number'], ascending=True, inplace=True)\n",
    "    # Make sure we don't have any blank columns\n",
    "    if set(df.columns.values).intersection(set([np.nan, ''])):\n",
    "        raise RuntimeError(\"Blank column names in Dataframe\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_data(spreadsheet_id):\n",
    "    catalog_range = 'Catalog!A:R'\n",
    "    ftir_range = 'FTIR!A:X'\n",
    "    reagent_range = 'Reagent!A:W'\n",
    "    mla_range = 'MLA!A:R'\n",
    "    hr_range = 'Interventions!A:BJ'\n",
    "    \n",
    "    df_catalog = get_df(spreadsheet_id, catalog_range)\n",
    "    df_catalog = canonicalise_df(df_catalog, source='catalog')\n",
    "    df_ftir = get_df(spreadsheet_id, ftir_range)\n",
    "    df_ftir = canonicalise_df(df_ftir, source='ftir')\n",
    "    df_reagent = get_df(spreadsheet_id, reagent_range)\n",
    "    df_reagent = canonicalise_df(df_reagent, source='reagent')\n",
    "    df_mla = get_df(spreadsheet_id, mla_range, mla=True)\n",
    "    df_mla = canonicalise_df(df_mla, source='mla')\n",
    "    try:\n",
    "        df_hr = get_df(spreadsheet_id, hr_range)\n",
    "    except ValueError:\n",
    "        df_hr = None\n",
    "    if df_hr is not None:\n",
    "        pass\n",
    "        df_hr = canonicalise_df(df_hr, source='hr')\n",
    "\n",
    "    df = DataFrames()\n",
    "    df.catalog = df_catalog\n",
    "    df.ftir = df_ftir\n",
    "    df.reagent = df_reagent\n",
    "    df.mla = df_mla\n",
    "    df.hr = df_hr\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_rename_map(map_name, lower=True):\n",
    "    \"\"\"Query google sheets spreadsheet for the translation table for names\"\"\"\n",
    "    pkl = map_name + '.pkl'\n",
    "    if not os.path.isfile(pkl):\n",
    "        # Get the drugs map\n",
    "        if 'GSHEETS_SERVICE' not in locals():\n",
    "            GSHEETS_SERVICE = gsheets_service()\n",
    "        sheet_id = '1CgqTjdKizat-g7K7-AAuVIazQFKJ3WAAPHR-Qpa49lU'\n",
    "        ss_range = '{}!A:B'.format(map_name)\n",
    "        result = GSHEETS_SERVICE.spreadsheets().values().get(spreadsheetId=sheet_id,\n",
    "                                                             range=ss_range).execute()\n",
    "        values = result.get('values', [])\n",
    "        rename_map = { v[0].strip() : v[1].strip() for v in values[1:] if len(v) == 2 }\n",
    "        if lower:\n",
    "            rename_map = { k.lower() : v.lower() for k, v in rename_map.items()}\n",
    "        rename_map = { k : v for k, v in rename_map.items() if k != v } # remove duplicates\n",
    "        with open(pkl, 'wb') as w:\n",
    "            pickle.dump(rename_map, w)\n",
    "    else:\n",
    "        with open(pkl, 'rb') as r:\n",
    "            rename_map = pickle.load(r)\n",
    "    return rename_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script running from: /opt/random/loop\n",
      "PROCESSING BOOMTOWN\n",
      "Canonicalising catalog\n",
      "!!! Bad ID 'TF0579'\n",
      "!!! Bad ID 'TF1665'\n",
      "!!! Bad ID 'TF1660'\n",
      "Canonicalising ftir\n",
      "!!! Bad ID 'TF1665'\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "Canonicalising hr\n",
      "!!! Bad ID 'FXXX'\n",
      "!!! Bad ID 'TF0653'\n",
      "!!! Bad ID 'TF1172'\n",
      "!!! Bad ID 'TF1762'\n",
      "PROCESSING BOARDMASTERS\n",
      "Canonicalising catalog\n",
      "Canonicalising ftir\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "Canonicalising hr\n",
      "PROCESSING MADE\n",
      "Canonicalising catalog\n",
      "Canonicalising ftir\n",
      "!!! Bad ID 'XF0005'\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "Canonicalising hr\n",
      "PROCESSING SW4\n",
      "Canonicalising catalog\n",
      "Canonicalising ftir\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "PROCESSING LOST VILLAGE\n",
      "Canonicalising catalog\n",
      "Canonicalising ftir\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "PROCESSING BESTIVAL\n",
      "Canonicalising catalog\n",
      "Canonicalising ftir\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "Canonicalising hr\n",
      "!!! Bad ID 'P1000'\n",
      "!!! Bad ID 'F20005'\n",
      "!!! Bad ID 'G9998'\n",
      "PROCESSING YNOT\n",
      "Canonicalising catalog\n",
      "Canonicalising ftir\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "Canonicalising hr\n",
      "PROCESSING TRUCKFEST\n",
      "Canonicalising catalog\n",
      "Canonicalising ftir\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "PROCESSING LSTD\n",
      "Canonicalising catalog\n",
      "!!! Bad ID 'S0050'\n",
      "!!! Bad ID 'M0120'\n",
      "!!! Bad ID 'M0141'\n",
      "!!! Bad ID 'M0204'\n",
      "!!! Bad ID 'S0186'\n",
      "!!! Bad ID 'S0202'\n",
      "Canonicalising ftir\n",
      "!!! Bad ID 'S0050'\n",
      "!!! Bad ID 'M0141'\n",
      "!!! Bad ID 'M0120'\n",
      "!!! Bad ID 'S0186'\n",
      "!!! Bad ID 'M0204'\n",
      "!!! Bad ID 'S0202'\n",
      "Canonicalising reagent\n",
      "!!! Bad ID 'S0050'\n",
      "Canonicalising mla\n",
      "!!! Bad ID 'M0120'\n",
      "Canonicalising hr\n",
      "PROCESSING KENDAL CALLING\n",
      "Canonicalising catalog\n",
      "!!! Bad ID 'M0011'\n",
      "Canonicalising ftir\n",
      "Canonicalising reagent\n",
      "Canonicalising mla\n",
      "Canonicalising hr\n",
      "PROCESSING PARKLIFE\n",
      "Canonicalising catalog\n",
      "!!! Bad ID 'M2248'\n",
      "Canonicalising ftir\n",
      "!!! Bad ID 'M2248'\n",
      "Canonicalising reagent\n",
      "!!! Bad ID 'NOT A1451'\n",
      "Canonicalising mla\n",
      "Finished importing data at 02/01/19 10:54:40\n"
     ]
    }
   ],
   "source": [
    "print(\"Script running from: %s\" % os.path.realpath(os.getcwd()))\n",
    "\n",
    "# The ID and range of a sample spreadsheet.\n",
    "BOOMTOWN2018_SPREADSHEET_ID = '1RiA-FwG_954Ger2VPsOSA3JLh-7sEoTYr40eVS0mp24'\n",
    "MADE2018_SPREADSHEET_ID = '1daXdyL6uL8qnMsEsP0RLZE9nDzt6J7Zr1ygQdguvi-E'\n",
    "BOARDMASTERS2018_SPREADSHEET_ID = '1U1lhUWLazDBN-wb2eZM8YV674f46npVfQK3XUVZjPow'\n",
    "SW42018_SPREADSHEET_ID = '1agpMmJ9XukeWXS5_mwrDSKeshUaFtYwOzsPiR1DKsPU'\n",
    "LOSTVILLAGE2018_SPREADSHEET_ID = '1OL0gyXrpZnJ8e7yR7eF6S2OaBYBiPDoVp5xGpdK4wlA'\n",
    "BESTIVAL2018_SPREADSHEET_ID = '184qudGcw4PB0SMtOo0ZBDtckeGaH0RCLUXbA-u3BiHE'\n",
    "YNOT2018_SPREADSHEET_ID = '1D01cj-Mra06TuoG_MsKuLq9OdtvKzrvRdiE255po_ag'\n",
    "TRUCKFEST2018_SPREADSHEET_ID = '1sGG9WJxKyD2CGUjzJAXul3g9hVnRz6HbTiqKV5cUAyA'\n",
    "LSTD2018_SPREADSHEET_ID = '1R8YqDnrhvuVMwPFShwaaAUIyCXQMeozA230OXsFsDQM'\n",
    "KENDALCALLING2018_SPREADSHEET_ID = '16-PfwBOaUxwod3X75LGk1VAjBblkNsTJpCsX825aghI'\n",
    "PARKLIFE2018_SPREADSHEET_ID = '1oO5sHcUhUn_7M1Hap73sOZHNEfWFMcDkQuWDRFf4d-w'\n",
    "\n",
    "\n",
    "data = {}\n",
    "GSHEETS_SERVICE = gsheets_service()\n",
    "RENAME_COLUMN_MAP = get_rename_map('ColumnMap', lower=False)\n",
    "\n",
    "print(\"PROCESSING BOOMTOWN\")\n",
    "data['boomtown'] = get_data(BOOMTOWN2018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING BOARDMASTERS\")\n",
    "data['boardmasters'] = get_data(BOARDMASTERS2018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING MADE\")\n",
    "data['made'] = get_data(MADE2018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING SW4\")\n",
    "data['sw4'] = get_data(SW42018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING LOST VILLAGE\")\n",
    "data['lostvillage'] = get_data(LOSTVILLAGE2018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING BESTIVAL\")\n",
    "data['bestival'] = get_data(BESTIVAL2018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING YNOT\")\n",
    "data['ynot'] = get_data(YNOT2018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING TRUCKFEST\")\n",
    "data['truckfest'] = get_data(TRUCKFEST2018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING LSTD\")\n",
    "data['lstd'] = get_data(LSTD2018_SPREADSHEET_ID)\n",
    "print( \"PROCESSING KENDAL CALLING\")\n",
    "data['kc'] = get_data(KENDALCALLING2018_SPREADSHEET_ID)\n",
    "print(\"PROCESSING PARKLIFE\")\n",
    "data['parklife'] = get_data(PARKLIFE2018_SPREADSHEET_ID)\n",
    "\n",
    "with open(IMPORT_PKL,'wb') as w:\n",
    "    pickle.dump(data, w)\n",
    "\n",
    "print(\"Finished importing data at %s\" % now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def person_id_from_samplenumber(x):\n",
    "    if isinstance(x, float) and np.isnan(x):\n",
    "        return x # leave NaN's alone\n",
    "    if is_str(x) and len(x) == 0:\n",
    "        return np.nan\n",
    "    if len(x) != 5:\n",
    "        return np.nan\n",
    "    try:\n",
    "        pn = 'P{:04d}'.format(int(x[-3:]))\n",
    "    except ValueError:\n",
    "        print(\"!!! Bad ID \\'%s\\'\" % pn)\n",
    "        pn = x\n",
    "    return pn\n",
    "\n",
    "\n",
    "def rm_bad_samplenumber(x):\n",
    "    if pd.notnull(x):\n",
    "        if len(x) != 5 and x[0] != 'F':\n",
    "            return np.nan\n",
    "    return x\n",
    "\n",
    "\n",
    "def add_person_id(hr_df):\n",
    "    \"\"\"Create the unique PID column\"\"\"\n",
    "    hr_df['Previous_sample'] = hr_df['Previous_sample'].apply(fix_sample_number)\n",
    "    hr_df['Previous_sample'] = hr_df['Previous_sample'].apply(rm_bad_samplenumber)\n",
    "    hr_df['PID'] = dfs.hr['SampleNumber']\n",
    "    # Copy over SampleNumbers from Previous_sample\n",
    "    mask = hr_df['Previous_sample'].isnull()\n",
    "    hr_df['PID'] = hr_df['PID'].where(mask, hr_df['Previous_sample'])\n",
    "    # Clean up values\n",
    "    hr_df['PID'] = hr_df['PID'].apply(person_id_from_samplenumber)\n",
    "    return hr_df\n",
    "\n",
    "\n",
    "def merge_ftir_drug_columns(df):\n",
    "    # Copy over 'Other' substances into the main column\n",
    "    target_label = 'Substance detected'\n",
    "    source_label = 'Compound detected'\n",
    "    to_drop = [source_label, 'Hit Confidence.1']\n",
    "    other_mask = ~df[target_label].str.startswith('Other').fillna(value=False)\n",
    "    df[target_label].where(other_mask, df[source_label], inplace=True) # Copy values from source_label column over\n",
    "    df.drop(to_drop, axis=1, inplace=True) # Remove now redundant columns\n",
    "    df.rename(columns={target_label : 'Primary_hit', 'Hit Confidence' : 'Primary_confidence'}, inplace=True) # Rename Columns\n",
    "\n",
    "    # Column names appear to be reversed - compound now is substance!!\n",
    "    target_label = 'Compound detected (Subtraction)'\n",
    "    source_label = 'Substance detected.1'\n",
    "    to_drop = [source_label, 'Hit Confidence.3']\n",
    "    other_mask = ~df[target_label].str.startswith('Other').fillna(value=False)\n",
    "    df[target_label].where(other_mask, df[source_label], inplace=True) # Copy values from source_label column over\n",
    "    df.drop(to_drop, axis=1, inplace=True) # Remove now redundant columns\n",
    "    df.rename(columns={target_label : 'Secondary_hit', 'Hit Confidence.2' : 'Secondary_confidence'}, inplace=True) # Rename Columns\n",
    "\n",
    "\n",
    "def rename_values(df, columns, replace_map):\n",
    "    \"\"\"Replace all names in columns with those from replace_map - lower cases everything\"\"\"\n",
    "    # Firstly convert all columns to lower case and remove any spaces\n",
    "    def clean(value):\n",
    "        if is_str(value):\n",
    "            value = value.strip().lower()\n",
    "        return value\n",
    "\n",
    "    for column in columns:\n",
    "        if column in df.columns:\n",
    "            df[column] = df[column].map(clean, na_action='ignore')\n",
    "\n",
    "    replace_d = {column : replace_map for column in columns}\n",
    "    df.replace(replace_d, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def canonicalise_columns(dfs):\n",
    "    \"\"\"Select the column names we require\"\"\"\n",
    "    catalog_columns = ['Timestamp', 'SampleNumber', 'Tester', 'SampleSource', 'SoldAs', 'AlreadyTried', \n",
    "                       'UserSuspicion', 'SampleForm', 'What is the logo?', 'Pill_mass_mg', \n",
    "                       'What is the shape of the pill?', 'Is a breakline present?', 'What colour is the pill?']\n",
    "    ftir_columns = ['Timestamp', 'SampleNumber', 'Tester', 'SoldAs', 'SampleForm', 'AlreadyTried', 'UserSuspicion', \n",
    "                    'Substance detected', 'Hit Confidence', 'Compound detected', 'Hit Confidence.1', \n",
    "                    'Compound detected (Subtraction)', 'Hit Confidence.2', 'Substance detected.1', \n",
    "                    'Hit Confidence.3', 'Substance(s) detected', 'Powder_strength']\n",
    "    mla_columns = ['SampleNumber', 'Tester', 'SampleForm', 'Logo', 'Colour', 'FTIR_hit1', 'MDMA / tablet (mg)', \n",
    "                   'Percent_MDMA']\n",
    "    reagent_columns = ['Timestamp', 'SampleNumber', 'Tester', 'SoldAs', 'SampleForm', 'Froehde', \n",
    "                       'Froehde possible substances', 'Liebermann', 'Liebermann possible substances',\n",
    "                       'Marquis', 'Marquis possible substances', 'Mandelin', 'Mandelin possible substances', 'Ehrlich', \n",
    "                       'Likely drug or class', 'Analysis required', 'Substance(s) detected', 'Powder_strength', \n",
    "                       'Pill Strength in mg (if known)', 'Matches_soldas']\n",
    "\n",
    "    hr_columns = ['Timestamp', 'SampleNumber', 'HR_worker', 'Gender', 'Ethnicity', 'Age', \n",
    "                  'Number of people present', 'When was the last time you used this service?', \n",
    "                  'Previous_sample', 'Had_alcohol_today', 'DRINK 1 - Type', 'DRINK 1: Vessel',\n",
    "                  'DRINK 1: Quantity', 'DRINK 2 - Type', 'DRINK 2: Vessel', 'DRINK 2: Quantity', \n",
    "                  'DRINK 3 - Type', 'DRINK 3: Vessel', 'DRINK 3: Quantity', 'DRINK 4 - Type', \n",
    "                  'DRINK 4: Vessel', 'DRINK 4: Quantity', 'Had_other_illegal_drugs', 'Prescribed Medication', \n",
    "                  'Over The Counter Medication', 'Feeling_concerns', 'Had_amphetamine', 'Had_cannabis', \n",
    "                  'Had_n2o', 'Had_cocaine', 'Had_ecstascy_pill', 'Had_mdma_crystal', 'Had_ketamine', 'Had_lsd',\n",
    "                  'Had_2cb', 'Had_magic_mushrooms', 'Had_mephedrone', 'Had_spice', 'Had_codeine', \n",
    "                  'Had_tramadol', 'Had_heroin_or_opioids', 'Had_valium_or_benzos', 'Had_xanax', \n",
    "                  'Had_unknown_power', 'Had_other_drugs', 'SoldAs', 'AlreadyTried', \n",
    "                  'When did you first use this batch?', 'Bad_experience_with_batch', \n",
    "                  'Was the sample bought, given or found?', 'Where did you obtain this substance from?', \n",
    "                  'In general terms, who did you get the substance from?', 'Why did you bring this substance to be tested?', \n",
    "                  'Have you ever accessed a health service for your alcohol or drug use?', \n",
    "                  'Require further advice', 'Plan to do?', 'What other actions will you do? (tick all that apply)', \n",
    "                  'Interview_abandonded']\n",
    "    \n",
    "    dfs.catalog = dfs.catalog[catalog_columns]\n",
    "    dfs.ftir = dfs.ftir[ftir_columns]\n",
    "    dfs.mla = dfs.mla[mla_columns]\n",
    "    dfs.reagent = dfs.reagent[reagent_columns]\n",
    "    if dfs.hr is not None:\n",
    "        dfs.hr = dfs.hr[hr_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FESTIVAL  boomtown\n",
      "!!! Bad ID 'TESTED SOME OTHER STUFF YESTERDAY THAT WASNT WHAT I THOUGHT'\n",
      "!!! Bad ID 'KEPT THE TICKET'\n",
      "!!! Bad ID 'F1868 GJ:THIS HAS TO BE INCORRECT. 1868 WAS GIVEN RESULTS MUCH LATER IN THE DAY'\n",
      "!!! Bad ID 'G1879'\n",
      "FESTIVAL  boardmasters\n",
      "FESTIVAL  made\n",
      "FESTIVAL  sw4\n",
      "FESTIVAL  lostvillage\n",
      "FESTIVAL  bestival\n",
      "FESTIVAL  ynot\n",
      "FESTIVAL  truckfest\n",
      "FESTIVAL  lstd\n",
      "FESTIVAL  kc\n",
      "FESTIVAL  parklife\n",
      "Finished cleaning dataframes at 02/01/19 10:54:56\n"
     ]
    }
   ],
   "source": [
    "# Clean the cells\n",
    "with open(IMPORT_PKL, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "sample_form_map = get_rename_map('SampleFormMap')\n",
    "user_drugs_map = get_rename_map('UserDrugsMap')\n",
    "tester_drugs_map = get_rename_map('TesterDrugsMap')\n",
    "\n",
    "# Combine all drugs maps\n",
    "drugs_map = copy.copy(user_drugs_map)\n",
    "drugs_map.update(tester_drugs_map)\n",
    "user_drug_columns = ['Had_other_drugs', 'SoldAs', 'UserSuspicion']\n",
    "tester_drug_columns = ['Primary_hit', 'Secondary_hit', 'FTIR_final_result', 'Substance(s) detected']\n",
    "drug_columns = user_drug_columns + tester_drug_columns\n",
    "\n",
    "for festival, dfs in data.items():\n",
    "    print(\"FESTIVAL \",festival)\n",
    "    canonicalise_columns(dfs)\n",
    "    \n",
    "    # Clean up ftir sheet\n",
    "    merge_ftir_drug_columns(dfs.ftir)\n",
    "    \n",
    "    if dfs.hr is not None:\n",
    "        # Add person ID to HD\n",
    "        dfs.hr = add_person_id(dfs.hr)\n",
    "    \n",
    "    # Sample form\n",
    "    dfs.catalog = rename_values(dfs.catalog, ['SampleForm'], sample_form_map)\n",
    "    dfs.ftir = rename_values(dfs.ftir, ['SampleForm'], sample_form_map)\n",
    "    dfs.mla = rename_values(dfs.mla, ['SampleForm'], sample_form_map)\n",
    "\n",
    "    # Drug names\n",
    "    dfs.catalog = rename_values(dfs.catalog, drug_columns, drugs_map)\n",
    "    dfs.ftir = rename_values(dfs.ftir, drug_columns, drugs_map)\n",
    "    dfs.mla = rename_values(dfs.mla, drug_columns, drugs_map)\n",
    "    dfs.reagent = rename_values(dfs.reagent, drug_columns, drugs_map)\n",
    "    if dfs.hr is not None:\n",
    "        dfs.hr = rename_values(dfs.hr, drug_columns, drugs_map)\n",
    "    \n",
    "print(\"Finished cleaning dataframes at %s\" % now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FESTIVAL  boomtown\n",
      "FESTIVAL  boardmasters\n",
      "FESTIVAL  made\n",
      "FESTIVAL  sw4\n",
      "FESTIVAL  lostvillage\n",
      "FESTIVAL  bestival\n",
      "FESTIVAL  ynot\n",
      "FESTIVAL  truckfest\n",
      "FESTIVAL  lstd\n",
      "FESTIVAL  kc\n",
      "FESTIVAL  parklife\n",
      "Finished merging first round at 02/01/19 10:54:57\n"
     ]
    }
   ],
   "source": [
    "def calculate_final_results(df):\n",
    "    \"\"\"Calculate final result\"\"\"\n",
    "    # Where 'ftir_Substance(s) detected' is null we use the ftir_Primary_hit\n",
    "    mask = ~df['ftir_Substance(s) detected'].isin(['', np.nan, None])\n",
    "    df['ftir_Substance(s) detected'] = df['ftir_Substance(s) detected'].where(mask, df['ftir_Primary_hit'])\n",
    "    # Find where'reagent_Substance(s) detected' contains anything but 'No active component identified'\n",
    "    mask = df['reagent_Substance(s) detected'].isin([None, np.nan,'No active component identified' ])\n",
    "    # Default is 'ftir_Substance(s) detected'\n",
    "    df['Final_result_calculated'] = df['ftir_Substance(s) detected']\n",
    "    # Copy over anything from 'reagent_Substance(s) detected'\n",
    "    df['Final_result_calculated'] = df['Final_result_calculated'].where(mask, df['reagent_Substance(s) detected'])\n",
    "    # Need to lowercase for comparison\n",
    "    df['Final_result_calculated'] = df['Final_result_calculated'].astype(str).str.lower()\n",
    "    df['catalog_SoldAs'] = df['catalog_SoldAs'].astype(str).str.lower()\n",
    "    \n",
    "    # Calculate where they do/don't match\n",
    "    df['As_expected'] = (df['Final_result_calculated'] == df['catalog_SoldAs']).map({True : 'Yes', False : 'No'})\n",
    "\n",
    "    # Guy 28/10/18: 'As_expected' should be null whenever the sample is found,\n",
    "    # when the submission 'acquired as\" data is blank or unknown, or when the sample is from Amnesty\n",
    "    mask1 = df['catalog_SoldAs'].isin(['found', 'found or otherwise not known', np.nan, None])\n",
    "    mask2 = df['catalog_SampleSource'] != 'Public'\n",
    "    mask = mask1 | mask2\n",
    "    # jmht - could check against: 'hr_Was the sample bought, given or found?\n",
    "    df.loc[mask, ['As_expected']] = np.nan\n",
    "    return df\n",
    "\n",
    "\n",
    "for festival, dfs in data.items():\n",
    "    print(\"FESTIVAL \",festival)\n",
    "\n",
    "    # Rename columns to identify source dataframe\n",
    "    dfs.catalog.columns = ['catalog_'+ name if name != 'SampleNumber' else name for name in dfs.catalog.columns]\n",
    "    dfs.ftir.columns = ['ftir_'+ name if name != 'SampleNumber' else name for name in dfs.ftir.columns]\n",
    "    dfs.mla.columns = ['mla_'+ name if name != 'SampleNumber' else name for name in dfs.mla.columns]\n",
    "    dfs.reagent.columns = ['reagent_'+ name if name != 'SampleNumber' else name for name in dfs.reagent.columns]\n",
    "    if dfs.hr is not None:\n",
    "        dfs.hr.columns = ['hr_'+ name if name != 'SampleNumber' else name for name in dfs.hr.columns]\n",
    "\n",
    "    # Remove all but the last of any duplicate SampleNumber\n",
    "    # want a list of all but the last duplicates\n",
    "    mask = ~dfs.catalog['SampleNumber'].duplicated(keep=False) | ~dfs.catalog['SampleNumber'].duplicated(keep='last')\n",
    "    dfs.catalog = dfs.catalog[mask]\n",
    "    mask = ~dfs.ftir['SampleNumber'].duplicated(keep=False) | ~dfs.ftir['SampleNumber'].duplicated(keep='last')\n",
    "    dfs.ftir = dfs.ftir[mask]\n",
    "    mask = ~dfs.mla['SampleNumber'].duplicated(keep=False) | ~dfs.mla['SampleNumber'].duplicated(keep='last')\n",
    "    dfs.mla = dfs.mla[mask]\n",
    "    mask = ~dfs.reagent['SampleNumber'].duplicated(keep=False) | ~dfs.reagent['SampleNumber'].duplicated(keep='last')\n",
    "    dfs.reagent = dfs.reagent[mask]\n",
    "    if dfs.hr is not None:\n",
    "        mask = ~dfs.hr['SampleNumber'].duplicated(keep=False) | ~dfs.hr['SampleNumber'].duplicated(keep='last')\n",
    "        dfs.hr = dfs.hr[mask]\n",
    "\n",
    "    # First outer join on catalog/ftir to make sure we collect all possible information - this will result in\n",
    "    # some rows where there was no catalog data, only ftir data, but this is ok as when we merge with hr we will\n",
    "    # throw away any row that doesn't have a corresponding sample number in HR. This was even if catalog data is\n",
    "    # missing, we still get the FTIR data, which may be enough for our purposes\n",
    "    df_all = pd.merge(dfs.catalog, dfs.ftir, how='outer', on=['SampleNumber'])\n",
    "    # Add in mla data - only for when there are existing sample numbers\n",
    "    df_all = pd.merge(df_all, dfs.mla, how='left', on=['SampleNumber'])\n",
    "    df_all = pd.merge(df_all, dfs.reagent, how='left', on=['SampleNumber'])\n",
    "    if dfs.hr is not None:\n",
    "        # inner join -> merge only where there are matching sample numbers\n",
    "        df_all = pd.merge(df_all, dfs.hr, how='inner', on=['SampleNumber'])\n",
    "    dfs.combined = df_all\n",
    "\n",
    "    # Calculate final result\n",
    "    dfs.combined = calculate_final_results(dfs.combined)\n",
    "    \n",
    "    # Add unique columns\n",
    "    dfs.combined.insert(loc=0, column='Festival', value=festival)\n",
    "    dfs.combined.insert(loc=1, column='UID', value=dfs.combined[['Festival', 'SampleNumber']].apply(lambda x: '_'.join(x), axis=1))\n",
    "\n",
    "# Create final dataframe\n",
    "df_final = pd.concat([d.combined for d in data.values()], ignore_index=True, sort=False)\n",
    "\n",
    "print(\"Finished merging first round at %s\" % now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished drugs columns unpack at 02/01/19 10:54:59\n"
     ]
    }
   ],
   "source": [
    "# Unpack drug use columns\n",
    "drug_columns = ['hr_Had_2cb',\n",
    "             'hr_Had_amphetamine',\n",
    "             'hr_Had_cannabis',\n",
    "             'hr_Had_cocaine',\n",
    "             'hr_Had_codeine',\n",
    "             'hr_Had_ecstascy_pill',\n",
    "             'hr_Had_heroin_or_opioids',\n",
    "             'hr_Had_ketamine',\n",
    "             'hr_Had_lsd',\n",
    "             'hr_Had_magic_mushrooms',\n",
    "             'hr_Had_mdma_crystal',\n",
    "             'hr_Had_mephedrone',\n",
    "             'hr_Had_n2o',\n",
    "            #  'hr_Had_other_drugs',\n",
    "            #  'hr_Had_other_illegal_drugs',\n",
    "             'hr_Had_spice',\n",
    "             'hr_Had_tramadol',\n",
    "             'hr_Had_unknown_power',\n",
    "             'hr_Had_valium_or_benzos',\n",
    "             'hr_Had_xanax']\n",
    "\n",
    "def unpack_drug_use(drug_column):\n",
    "    def f(x):\n",
    "        _false = 0\n",
    "        _true = 1\n",
    "        l = ['Ever had', 'Within last year', 'Within last month', 'Within last week', 'Had yesterday', 'Had today', '(Probably) intending later']\n",
    "        r = [_false, _false, _false, _false, _false, _false, _false]\n",
    "        if not isinstance(x, str):\n",
    "            return r\n",
    "        for v in [s.strip() for s in x.split(',') if s.strip()]:\n",
    "            r[l.index(v)] = _true\n",
    "        return r\n",
    "    res = drug_column.apply(f)\n",
    "    cname = drug_column.name\n",
    "    drug = cname[7:] # strip 'hr_Had'\n",
    "    cname_stem = ['ever', 'year', 'month', 'week', 'yesterday', 'today', 'later']\n",
    "    columns = ['{}_{}'.format(drug, c) for c in cname_stem]\n",
    "    return pd.DataFrame(res.values.tolist(),\n",
    "                        index=res.index,\n",
    "                        columns=columns)\n",
    "\n",
    "for column in drug_columns:\n",
    "    d = unpack_drug_use(df_final[column])\n",
    "    df_final = pd.concat([df_final, d], axis=1)\n",
    "    df_final.drop(column, axis=1, inplace=True)\n",
    "\n",
    "print(\"Finished drugs columns unpack at %s\" % now())    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final fixing at 02/01/19 10:55:00\n"
     ]
    }
   ],
   "source": [
    "fpkl = 'final_pre.pkl'\n",
    "if os.path.isfile(fpkl):\n",
    "    with open(fpkl, 'rb') as f:\n",
    "        df_final = pickle.load(f)\n",
    "else:\n",
    "    with open(fpkl, 'wb') as w:\n",
    "        pickle.dump(df_final, w)    \n",
    "\n",
    "\n",
    "def columns_strip_and_to_lower_case(df):\n",
    "    cols = ['hr_Gender', 'hr_Ethnicity',\n",
    "           'hr_In general terms, who did you get the substance from?',\n",
    "           'hr_Was the sample bought, given or found?',\n",
    "           'hr_Where did you obtain this substance from?']\n",
    "    df.loc[:,cols] = df[cols].apply(lambda x: x.str.lower().str.strip())\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_numeric_columns(df):\n",
    "    df['mla_Percent_MDMA'] = df['mla_Percent_MDMA'].str.strip('%')\n",
    "    df.replace('', np.nan, inplace = True)\n",
    "    numeric_columns = ['catalog_Pill_mass_mg',\n",
    "                       'mla_MDMA / tablet (mg)',\n",
    "                       'hr_DRINK 1: Quantity',\n",
    "                       'ftir_Primary_confidence',\n",
    "                       'mla_Percent_MDMA']\n",
    "    df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_pid(df):\n",
    "    def join(x):\n",
    "        if type(x[0]) is str and type(x[1]) is str:\n",
    "            return \"_\".join(x)\n",
    "        else:\n",
    "            return np.nan\n",
    "    df['PID'] = df[['Festival', 'hr_PID']].apply(join, axis=1)\n",
    "    # # Need to set to null where 'hr_PID' is null\n",
    "    df['PID'] = df['PID'].where(df['hr_PID'].notnull(), np.nan)\n",
    "    df.drop('hr_PID', axis=1, inplace=True)    \n",
    "    return df\n",
    "\n",
    "\n",
    "def fix_ethnicities(df):\n",
    "    ethnicity_map = { 'asian' : ['arabic', 'asian (including chinese)', 'indian', 'white asian'],\n",
    "                      'black' : [],\n",
    "                      'mixed_race' : ['mixed heritage'],\n",
    "                      'other' : ['citizen of the world', 'glittery', 'prefer not to say'],\n",
    "                      'white' : ['irish', 'portuguese brazillian', 'white (including european)', 'white other'] }\n",
    "\n",
    "    replace_d = {}\n",
    "    for category, possibilities in ethnicity_map.items():\n",
    "        for possible in possibilities:\n",
    "            replace_d[possible] = category    \n",
    "    df['hr_Ethnicity'].replace(replace_d, inplace=True)\n",
    "    return df\n",
    "    \n",
    "\n",
    "def fix_origin(df):\n",
    "    who_label = 'hr_In general terms, who did you get the substance from?'\n",
    "    source_label = 'hr_Was the sample bought, given or found?'\n",
    "    \n",
    "    substance_from_map = get_rename_map('SubstanceFromMap')\n",
    "    substance_from_map['rather not say'] = np.nan\n",
    "    df[who_label].replace(to_replace=substance_from_map, inplace=True)\n",
    "    \n",
    "    # 'hr_Was the sample bought, given or found?'\n",
    "    #'hr_Where did you obtain this substance from?'\n",
    "    # Whereever this field is blank copy across values from 'hr_Was the sample bought, given or found?'\n",
    "    mask = ~df[who_label].isin(['', np.nan, None])\n",
    "    df[who_label] = df[who_label].where(mask, df[source_label])\n",
    "    return df\n",
    "\n",
    "    \n",
    "df_final = add_pid(df_final)\n",
    "df_final = columns_strip_and_to_lower_case(df_final)\n",
    "df_final = fix_ethnicities(df_final)\n",
    "df_final = fix_origin(df_final)\n",
    "df_final = clean_numeric_columns(df_final)\n",
    "\n",
    "# Final column tweakingReorder columns\n",
    "df_final['Version'] = VERSION  \n",
    "columns = df_final.columns.values.tolist()\n",
    "upfront = ['Festival', 'UID', 'PID']\n",
    "for u in upfront:\n",
    "    columns.remove(u)\n",
    "columns = upfront + columns\n",
    "df_final = df_final[columns]\n",
    "\n",
    "print(\"Final fixing at %s\" % now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(df_final))\n",
    "# pd.crosstab(df_final['hr_Ethnicity'], df_final['hr_Gender'], margins=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all non-HR data for Fiona\n",
    "out_dir = '/Users/jmht/Dropbox/TheLoop/Testing/Results/2018'\n",
    "# out_dir = '.'\n",
    "df_hr = df_final[df_final['PID'].notnull()]\n",
    "filename = 'Loop2018Data_HR_%2.1f.xls' % VERSION\n",
    "filepath = os.path.join(out_dir, filename)\n",
    "writer = pd.ExcelWriter(filepath)\n",
    "df_hr.to_excel(writer, 'MergedData', index=False)\n",
    "writer.save()\n",
    "print(now() + \" Wrote version %2.1f to file: %s\" % (VERSION, filepath))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Need to define in main or we can't pickle the data objects\n",
    "class Duplicates(object):\n",
    "    def __init__(self, dfs):\n",
    "        self.dfs = dfs\n",
    "        dtypes = ['catalog', 'ftir', 'reagent', 'mla', 'hr']\n",
    "        for t in dtypes:\n",
    "            setattr(self, t, None)\n",
    "        for t in dtypes:\n",
    "            self.find_duplicates(t)\n",
    "        \n",
    "    def find_duplicates(self, dtype):\n",
    "        dataframe = getattr(self.dfs, dtype)\n",
    "        if dataframe is None:\n",
    "            return\n",
    "        duplicates = dataframe['SampleNumber'].duplicated()\n",
    "        if duplicates.any():\n",
    "            duplicates = list(dataframe.loc[duplicates, 'SampleNumber'].values)\n",
    "            print(\"### %d duplicated %s SampleNumbers %s ###\" % (len(duplicates), dtype, duplicates))\n",
    "#             dataframe[datafra,e['SampleNumber'].duplicated(keep=False)].to_csv('{}_duplicates.csv'.format(dtype))\n",
    "        else:\n",
    "            duplicates = None\n",
    "        setattr(self, dtype, duplicates)\n",
    "        \n",
    "    def has_hr_duplicates(self):\n",
    "        if self.hr:\n",
    "            outs = 'Please fix HR duplicates'\n",
    "            print(outs)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class Orphans(object):\n",
    "    def __init__(self, dataframes):\n",
    "        self.catalog = None\n",
    "        self.ftir = None\n",
    "        self.reagent = None\n",
    "        self.mla = None\n",
    "        self.hr = None\n",
    "        self.ftir_missing = None\n",
    "        \n",
    "        self.find_unique(dataframes)\n",
    "        \n",
    "    def find_unique(self, dataframes):\n",
    "        # Check there are no SampleNumbers in any of the other spreadsheets that aren't in the cataolog sheet\n",
    "        catalog_unique = set(dataframes.catalog['SampleNumber'].unique())\n",
    "        \n",
    "        ftir_unique = set(dataframes.ftir['SampleNumber'].unique())\n",
    "        self.ftir = ftir_unique.difference(catalog_unique)\n",
    "\n",
    "        reagent_unique = set(dataframes.reagent['SampleNumber'].unique())\n",
    "        self.reagent = reagent_unique.difference(catalog_unique)\n",
    "\n",
    "        self.hr = None\n",
    "        hr_unique = None\n",
    "        if dataframes.hr is not None:\n",
    "            hr_unique = set(dataframes.hr['SampleNumber'].unique())\n",
    "            # HR need to be both in catalog and ftir\n",
    "            self.hr = hr_unique.difference(ftir_unique.union(catalog_unique))\n",
    "\n",
    "        mla_unique = set(dataframes.mla['SampleNumber'].unique()).difference(catalog_unique)\n",
    "        self.mla = mla_unique.difference(catalog_unique)\n",
    "\n",
    "        # Check for any that are only in the catalog\n",
    "        unique = [u for u in [ftir_unique, reagent_unique, hr_unique, mla_unique] if u is not None]\n",
    "        outside_catalog = set.union(*unique)\n",
    "        self.catalog = catalog_unique.difference(outside_catalog)\n",
    "\n",
    "        # Check for any that aren't in FTIR and don't have anything in reagent test\n",
    "        self.ftir_missing = catalog_unique.difference(ftir_unique).difference(reagent_unique).difference(self.catalog)\n",
    "\n",
    "            \n",
    "    def print_orphans(self):\n",
    "        if self.catalog:\n",
    "            print(\"Orphaned Catalog SampleNumbers: %s\" % sorted(self.catalog))\n",
    "        if self.ftir:\n",
    "            print(\"Orphaned FTIR SampleNumbers: %s\" % sorted(self.ftir))\n",
    "        if self.mla:\n",
    "            print(\"Orphaned MLA SampleNumbers: %s\" % sorted(self.mla))\n",
    "        if self.hr:\n",
    "            print(\"Orphaned HR SampleNumbers: %s\" % sorted(self.hr))\n",
    "        if self.ftir_missing:\n",
    "            print(\"Samples not in FTIR or Reagent: %s\" % sorted(self.ftir_missing))          \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Summary Cell\n",
    "catalog_orphan = 0\n",
    "ftir_orphan = 0\n",
    "hr_orphan = 0\n",
    "catalog_duplicates = 0\n",
    "ftir_duplicates = 0\n",
    "hr_duplicates = 0\n",
    "hr = 0\n",
    "for festival, fdfs in data.items():\n",
    "    print(\"\\n{} Festival: {}{} \".format(\"=\"* 15, festival, \"=\"*15))\n",
    "    orphans = Orphans(fdfs)\n",
    "    orphans.print_orphans()\n",
    "    catalog_orphan += len(orphans.catalog)\n",
    "    ftir_orphan += len(orphans.ftir)\n",
    "    if orphans.hr is not None:\n",
    "        hr_orphan += len(orphans.hr)\n",
    "    catalog_duplicates += sum(list(map(lambda x: x.startswith('D'), fdfs.catalog['SampleNumber'].values)))\n",
    "    ftir_duplicates += sum(list(map(lambda x: x.startswith('D'), fdfs.ftir['SampleNumber'].values)))\n",
    "    if fdfs.hr is not None:\n",
    "        hr_duplicates += sum(list(map(lambda x: x.startswith('D'), fdfs.hr['SampleNumber'].values)))\n",
    "    if fdfs.hr is not None:\n",
    "        hr += len(fdfs.hr)\n",
    "\n",
    "print(\"=\" * 30)\n",
    "print(\"SUMMARY\")\n",
    "print(\"Catalog duplicates \", catalog_duplicates)\n",
    "print(\"FTIR duplicates \", ftir_duplicates)\n",
    "print(\"HR duplicates \", hr_duplicates)\n",
    "print(\"Catalog orphan \", catalog_orphan)\n",
    "print(\"FTIR orphan \", ftir_orphan)\n",
    "print(\"HR orphan \", hr_orphan)\n",
    "\n",
    "print(\"Total number HR interventions \", hr)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def find_duplicate_matches(duplicates, df1, df2, df1_name='DataFrame1', df2_name='DataFrame2'):\n",
    "    hr = False\n",
    "    if df1_name.lower()[:2] == 'hr':\n",
    "        hr = True\n",
    "    duplicate_matches = {}\n",
    "    min_stage_delay = 60 * 1\n",
    "    max_stage_delay = 60 * 60\n",
    "    for sample_number in duplicates:\n",
    "        duplicate_matches[sample_number] = {}\n",
    "        for df1_idx, df1_row in df1.loc[df1['SampleNumber'] == sample_number].iterrows():\n",
    "            for df2_idx, df2_row in df2.loc[df2['SampleNumber'] == sample_number].iterrows():\n",
    "                df1_data = df1_row.loc[['SoldAs', 'AlreadyTried']].values.tolist()\n",
    "                if not hr:\n",
    "                    df1_data.append(df1_row.SampleForm)\n",
    "                df1_time = df1_row.Timestamp\n",
    "                df2_data = df2_row.loc[['SoldAs', 'AlreadyTried']].values.tolist()\n",
    "                if not hr:\n",
    "                    df2_data.append(df2_row.SampleForm)\n",
    "                df2_time = df2_row.Timestamp\n",
    "                if df2_time >= df1_time:\n",
    "                    delta_t = (df2_time - df1_time).seconds\n",
    "                else:\n",
    "                    delta_t = (df1_time - df2_time).seconds\n",
    "                if df1_data == df2_data and min_stage_delay < delta_t <= max_stage_delay:\n",
    "                    print(\"Duplicate %s SampleNumber %s (line: %d) MATCHES %s sample (line: %d)\" % \\\n",
    "                          (df1_name, sample_number, df1_idx + 1, df2_name, df2_idx + 1))\n",
    "                    duplicate_matches[sample_number][df1_idx] = True\n",
    "                else:\n",
    "                    print(\"Duplicate %s SampleNumber %s (line: %d) DIFFERENT %s sample (line: %d)\\n%s %s\\n%s %s\" % \\\n",
    "                          (df1_name, sample_number, df1_idx + 1, df2_name, df2_idx + 1,\n",
    "                           df1_data, df1_time,\n",
    "                           df2_data, df2_time))\n",
    "                    duplicate_matches[sample_number][df1_idx] = False\n",
    "    return duplicate_matches\n",
    "\n",
    "def match_orphans_to_duplicates(df1_orphans, duplicate_matches, df1, df2):\n",
    "    for orphan_sample_number in df1_orphans:\n",
    "        df1_data = df1.loc[df1['SampleNumber'] == orphan_sample_number, ['SampleForm', 'SoldAs', 'AlreadyTried', 'Timestamp']]\n",
    "        df1_data = df1_data.values.tolist()[0]\n",
    "        df1_time = df1_data.pop()\n",
    "        for sample_number, indexd in duplicate_matches.items():\n",
    "            for k, v in indexd.items():\n",
    "                if not v:\n",
    "                    df2_data = dfs.catalog.iloc[k][['SampleForm', 'SoldAs', 'AlreadyTried', 'Timestamp']].values.tolist()\n",
    "                    df2_time = df2_data.pop()\n",
    "                    if df2_time >= df1_time:\n",
    "                        delta_t = (df2_time - df1_time).seconds\n",
    "                    else:\n",
    "                        delta_t = (df1_time - df2_time).seconds\n",
    "                    if df2_data == df1_data and min_stage_delay < delta_t <= max_stage_delay:\n",
    "                        print(\"Orphan {} could be match for duplicate {} (line: {})\\n{} {}\\n{} {}\".format(orphan_sample_number,\n",
    "                                                                                                  sample_number, k+1,\n",
    "                                                                                                  df2_data, df2_time,\n",
    "                                                                                                  df1_data, df1_time))\n",
    "\n",
    "\n",
    "#duplicate_matches = find_duplicate_matches(duplicates.catalog, dfs.catalog, dfs.ftir, df1_name='Catalog', df2_name='FTIR')\n",
    "duplicate_matches = find_duplicate_matches(duplicates.ftir, dfs.ftir, dfs.catalog, df1_name='FTIR', df2_name='Catalog')\n",
    "#duplicate_matches = find_duplicate_matches(duplicates.hr, dfs.hr, dfs.catalog, df1_name='HR', df2_name='Catalog')\n",
    "\n",
    "#match_orphans_to_duplicates(ftir_orphan, duplicate_matches, dfs.ftir, dfs.catalog)\n",
    "# match_orphans_to_duplicates(catalog_orphan, duplicate_matches, dfs.catalog, dfs.ftir)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Check orphans against the FTIR sheet using just their numbers\n",
    "def match_orphans_with_sample_integer(orphans, orphan_df, ref_df):\n",
    "#     min_stage_delay = 60 * 1\n",
    "#     max_stage_delay = 60 * 60\n",
    "    def to_int(sn):\n",
    "        if type(sn) is str:\n",
    "            try:\n",
    "                sn = int(sn[-4:])\n",
    "            except ValueError:\n",
    "                print(\"Bad SampleNumber %s\" % sn)\n",
    "                sn = np.nan\n",
    "        return sn\n",
    "    orphan_df['SampleInteger'] = orphan_df['SampleNumber'].apply(to_int)\n",
    "    ref_df['SampleInteger'] = ref_df['SampleNumber'].apply(to_int)\n",
    "    orphan_ints = map(to_int, orphans)\n",
    "    skipform = True\n",
    "    for orphan_sample_number, oint in zip(orphans, orphan_ints):\n",
    "        for orphan_idx, orphan_row in orphan_df.loc[orphan_df['SampleNumber'] == orphan_sample_number].iterrows():\n",
    "            for ref_idx, ref_row in ref_df.loc[ref_df['SampleInteger'] == oint].iterrows():\n",
    "                orphan_data = orphan_row.loc[['SampleForm', 'SoldAs', 'AlreadyTried']].values.tolist()\n",
    "                orphan_time = orphan_row.Timestamp\n",
    "                ref_sample_number = ref_row.SampleNumber\n",
    "                ref_data = ref_row.loc[['SampleForm', 'SoldAs', 'AlreadyTried']].values.tolist()\n",
    "                ref_time = ref_row.Timestamp\n",
    "                delta_t = (ref_time - orphan_time).seconds\n",
    "#                 if orphan_data == ref_data and min_stage_delay < delta_t <= max_stage_delay:\n",
    "                if skipform:\n",
    "                    orphan_data.pop(0)\n",
    "                    ref_data.pop(0)\n",
    "                if orphan_data == ref_data:\n",
    "                    print(\"HR orphan %s (line: %d) could be match for FTIR SampleNumber %s (line: %d)\\n%s %s\\n%s %s\" % \\\n",
    "                          (orphan_sample_number, orphan_idx + 1, ref_sample_number, ref_idx + 1, orphan_data, orphan_time, ref_data, ref_time))\n",
    "\n",
    "# match_orphans_with_sample_integer(catalog_orphan, dfs.catalog, dfs.ftir)\n",
    "match_orphans_with_sample_integer(hr_orphan, dfs.hr, dfs.catalog)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Check orphans against other orphans just using data\n",
    "def match_orphans_vs_orphans(orphan1_list, orphan1_df, orphan2_list, orphan2_df, hr=False):\n",
    "    min_stage_delay = 60 * 1\n",
    "    max_stage_delay = 60 * 60\n",
    "    for orphan1 in orphan1_list:\n",
    "        orphan1_row = orphan1_df.loc[orphan1_df['SampleNumber'] == orphan1].iloc[0]\n",
    "        for orphan2 in orphan2_list:\n",
    "            orphan2_row = orphan2_df.loc[orphan2_df['SampleNumber'] == orphan2].iloc[0]\n",
    "            orphan1_data = orphan1_row.loc[['SampleForm', 'SoldAs', 'AlreadyTried']].values.tolist()\n",
    "            orphan1_time = orphan1_row.Timestamp\n",
    "            orphan2_data = orphan2_row.loc[['SampleForm', 'SoldAs', 'AlreadyTried']].values.tolist()\n",
    "            orphan2_time = orphan2_row.Timestamp\n",
    "            delta_t = (orphan2_time - orphan1_time).seconds\n",
    "            if orphan1_data == orphan2_data and min_stage_delay <= delta_t <= max_stage_delay:\n",
    "                print(\"orphan1 %s could be match for orphan2 %s\\n%s %s\\n%s %s\" % \\\n",
    "                      (orphan1, orphan2, orphan1_data, orphan1_time, orphan1_data, orphan1_time))\n",
    "\n",
    "# match_orphans_with_sample_integer(catalog_orphan, dfs.catalog, dfs.ftir)\n",
    "match_orphans_vs_orphans(catalog_orphan, dfs.catalog, ftir_orphan, dfs.ftir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
